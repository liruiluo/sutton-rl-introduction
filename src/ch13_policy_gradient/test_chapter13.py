#!/usr/bin/env python
"""
æµ‹è¯•ç¬¬13ç« æ‰€æœ‰ç­–ç•¥æ¢¯åº¦æ¨¡å—
Test all Chapter 13 Policy Gradient modules

ç¡®ä¿æ‰€æœ‰ç­–ç•¥æ¢¯åº¦ç®—æ³•å®ç°æ­£ç¡®
Ensure all policy gradient algorithm implementations are correct
"""

import sys
import traceback
import numpy as np
from pathlib import Path
import time

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))


def test_policy_gradient_theorem():
    """
    æµ‹è¯•ç­–ç•¥æ¢¯åº¦å®šç†
    Test policy gradient theorem
    """
    print("\n" + "="*60)
    print("æµ‹è¯•ç­–ç•¥æ¢¯åº¦å®šç†...")
    print("Testing Policy Gradient Theorem...")
    print("="*60)
    
    try:
        from src.ch13_policy_gradient.policy_gradient_theorem import (
            SoftmaxPolicy, GaussianPolicy, PolicyGradientTheorem, AdvantageFunction
        )
        
        # æµ‹è¯•Softmaxç­–ç•¥
        print("\næµ‹è¯•Softmaxç­–ç•¥...")
        n_features = 6
        n_actions = 3
        
        def simple_features(state, action):
            features = np.zeros(n_features)
            if isinstance(state, int):
                features[state % n_features] = 1.0
                features[(state + action) % n_features] = 0.5
            return features
        
        softmax_policy = SoftmaxPolicy(n_features, n_actions, simple_features)
        
        # æµ‹è¯•åŠ¨ä½œæ¦‚ç‡
        state = 1
        probs = softmax_policy.compute_action_probabilities(state)
        assert abs(np.sum(probs) - 1.0) < 1e-6
        assert all(p >= 0 for p in probs)
        print(f"  âœ“ Softmaxç­–ç•¥æµ‹è¯•é€šè¿‡ï¼Œæ¦‚ç‡å’Œ={np.sum(probs):.6f}")
        
        # æµ‹è¯•é«˜æ–¯ç­–ç•¥
        print("\næµ‹è¯•é«˜æ–¯ç­–ç•¥...")
        gaussian_policy = GaussianPolicy(state_dim=4, action_dim=2)
        test_state = np.random.randn(4)
        
        action = gaussian_policy.select_action(test_state)
        assert action.shape == (2,)
        
        log_prob = gaussian_policy.compute_log_probability(test_state, action)
        assert isinstance(log_prob, float)
        print(f"  âœ“ é«˜æ–¯ç­–ç•¥æµ‹è¯•é€šè¿‡ï¼ŒåŠ¨ä½œç»´åº¦={action.shape}")
        
        # æµ‹è¯•ç­–ç•¥æ¢¯åº¦å®šç†
        print("\næµ‹è¯•ç­–ç•¥æ¢¯åº¦å®šç†æ”¶æ•›æ€§...")
        pgt = PolicyGradientTheorem()
        true_grad, est_grad = pgt.demonstrate_convergence(n_samples=50)
        
        error = np.linalg.norm(est_grad - true_grad)
        assert error < 1.0  # å®½æ¾çš„é˜ˆå€¼
        print(f"  âœ“ ç­–ç•¥æ¢¯åº¦å®šç†æµ‹è¯•é€šè¿‡ï¼Œä¼°è®¡è¯¯å·®={error:.4f}")
        
        # æµ‹è¯•ä¼˜åŠ¿å‡½æ•°
        print("\næµ‹è¯•ä¼˜åŠ¿å‡½æ•°...")
        adv_func = AdvantageFunction()
        
        advantage = adv_func.compute_advantage(0, 0, 5.0, 3.0)
        assert advantage == 2.0
        
        rewards = [1.0, -1.0, 2.0]
        values = [2.0, 1.5, 3.0]
        gae_advantages = adv_func.compute_gae(rewards, values, gamma=0.9, lambda_=0.95)
        assert len(gae_advantages) == len(rewards)
        print(f"  âœ“ ä¼˜åŠ¿å‡½æ•°æµ‹è¯•é€šè¿‡ï¼ŒGAEè®¡ç®—å®Œæˆ")
        
        print("\nâœ… ç­–ç•¥æ¢¯åº¦å®šç†æµ‹è¯•å…¨éƒ¨é€šè¿‡ï¼")
        return True
        
    except Exception as e:
        print(f"\nâŒ ç­–ç•¥æ¢¯åº¦å®šç†æµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return False


def test_reinforce():
    """
    æµ‹è¯•REINFORCEç®—æ³•
    Test REINFORCE algorithms
    """
    print("\n" + "="*60)
    print("æµ‹è¯•REINFORCEç®—æ³•...")
    print("Testing REINFORCE Algorithms...")
    print("="*60)
    
    try:
        from src.ch13_policy_gradient.reinforce import (
            REINFORCE, REINFORCEWithBaseline, AllActionsREINFORCE,
            SimpleValueFunction, SimpleQFunction
        )
        from src.ch13_policy_gradient.policy_gradient_theorem import SoftmaxPolicy
        
        n_features = 8
        n_actions = 2
        
        def state_action_features(state, action):
            features = np.zeros(n_features)
            if isinstance(state, int):
                base_idx = (state * n_actions + action) % n_features
                features[base_idx] = 1.0
            return features
        
        def state_features(state):
            features = np.zeros(n_features)
            if isinstance(state, int):
                features[state % n_features] = 1.0
            return features
        
        # ç®€å•ç¯å¢ƒ
        class TestEnv:
            def __init__(self):
                self.state = 0
                self.step_count = 0
            
            def reset(self):
                self.state = 0
                self.step_count = 0
                return self.state
            
            def step(self, action):
                if action == 0:
                    self.state = max(0, self.state - 1)
                else:
                    self.state = min(3, self.state + 1)
                
                reward = 10.0 if self.state == 3 else -1.0
                done = self.state == 3 or self.step_count >= 10
                
                self.step_count += 1
                return self.state, reward, done, {}
        
        # æµ‹è¯•åŸºç¡€REINFORCE
        print("\næµ‹è¯•åŸºç¡€REINFORCE...")
        policy = SoftmaxPolicy(n_features, n_actions, state_action_features)
        reinforce = REINFORCE(policy, alpha=0.1, gamma=0.9)
        
        env = TestEnv()
        episode_return = reinforce.learn_episode(env, max_steps=10)
        
        assert reinforce.episode_count == 1
        assert len(reinforce.episode_returns) == 1
        print(f"  âœ“ åŸºç¡€REINFORCEæµ‹è¯•é€šè¿‡ï¼Œå›æŠ¥={episode_return:.1f}")
        
        # æµ‹è¯•å¸¦åŸºçº¿çš„REINFORCE
        print("\næµ‹è¯•REINFORCE with Baseline...")
        policy_baseline = SoftmaxPolicy(n_features, n_actions, state_action_features)
        value_func = SimpleValueFunction(n_features, state_features)
        
        reinforce_baseline = REINFORCEWithBaseline(
            policy_baseline, value_func,
            alpha_theta=0.05, alpha_w=0.1, gamma=0.9
        )
        
        episode_return = reinforce_baseline.learn_episode(env, max_steps=10)
        assert reinforce_baseline.episode_count == 1
        assert len(reinforce_baseline.advantages) > 0
        print(f"  âœ“ REINFORCE with Baselineæµ‹è¯•é€šè¿‡")
        
        # æµ‹è¯•All-actions REINFORCE
        print("\næµ‹è¯•All-actions REINFORCE...")
        policy_all = SoftmaxPolicy(n_features, n_actions, state_action_features)
        q_func = SimpleQFunction(n_features, n_actions, state_action_features)
        
        all_actions = AllActionsREINFORCE(
            policy_all, q_func,
            alpha=0.05, gamma=0.9
        )
        
        episode_return = all_actions.learn_episode(env, max_steps=10)
        assert all_actions.episode_count == 1
        assert all_actions.gradient_updates > 0
        print(f"  âœ“ All-actions REINFORCEæµ‹è¯•é€šè¿‡")
        
        print("\nâœ… REINFORCEç®—æ³•æµ‹è¯•å…¨éƒ¨é€šè¿‡ï¼")
        return True
        
    except Exception as e:
        print(f"\nâŒ REINFORCEç®—æ³•æµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return False


def test_actor_critic():
    """
    æµ‹è¯•Actor-Criticç®—æ³•
    Test Actor-Critic algorithms
    """
    print("\n" + "="*60)
    print("æµ‹è¯•Actor-Criticç®—æ³•...")
    print("Testing Actor-Critic Algorithms...")
    print("="*60)
    
    try:
        from src.ch13_policy_gradient.actor_critic import (
            OneStepActorCritic, ActorCriticWithTraces, A2C,
            SimpleActor, SimpleCritic
        )
        
        n_features = 8
        n_actions = 2
        
        def state_features(state):
            features = np.zeros(n_features)
            if isinstance(state, int):
                features[state % n_features] = 1.0
            return features
        
        def state_action_features(state, action):
            features = np.zeros(n_features)
            if isinstance(state, int):
                base_idx = (state * n_actions + action) % n_features
                features[base_idx] = 1.0
            return features
        
        # ç®€å•ç¯å¢ƒ
        class TestEnv:
            def __init__(self):
                self.state = 0
                self.step_count = 0
            
            def reset(self):
                self.state = 0
                self.step_count = 0
                return self.state
            
            def step(self, action):
                self.state = min(3, self.state + action)
                reward = 10.0 if self.state == 3 else -1.0
                done = self.state == 3 or self.step_count >= 10
                self.step_count += 1
                return self.state, reward, done, {}
        
        # æµ‹è¯•One-step Actor-Critic
        print("\næµ‹è¯•One-step Actor-Critic...")
        actor = SimpleActor(n_features, n_actions, state_action_features)
        critic = SimpleCritic(n_features, state_features)
        
        ac_1step = OneStepActorCritic(
            actor, critic,
            alpha_theta=0.05, alpha_w=0.1, gamma=0.9
        )
        
        env = TestEnv()
        episode_return = ac_1step.learn_episode(env, max_steps=10)
        
        assert ac_1step.episode_count == 1
        assert ac_1step.step_count > 0
        assert len(ac_1step.td_errors) > 0
        print(f"  âœ“ One-step ACæµ‹è¯•é€šè¿‡ï¼ŒTDè¯¯å·®æ•°={len(ac_1step.td_errors)}")
        
        # æµ‹è¯•å¸¦èµ„æ ¼è¿¹çš„Actor-Critic
        print("\næµ‹è¯•Actor-Critic with Traces...")
        actor_traces = SimpleActor(n_features, n_actions, state_action_features)
        critic_traces = SimpleCritic(n_features, state_features)
        
        ac_traces = ActorCriticWithTraces(
            actor_traces, critic_traces,
            lambda_theta=0.9, lambda_w=0.9,
            alpha_theta=0.02, alpha_w=0.05, gamma=0.9
        )
        
        episode_return = ac_traces.learn_episode(env, max_steps=10)
        
        assert ac_traces.episode_count == 1
        assert ac_traces.step_count > 0
        print(f"  âœ“ AC with Tracesæµ‹è¯•é€šè¿‡")
        
        # æµ‹è¯•A2C
        print("\næµ‹è¯•A2C...")
        actor_a2c = SimpleActor(n_features, n_actions, state_action_features)
        critic_a2c = SimpleCritic(n_features, state_features)
        
        a2c = A2C(
            actor_a2c, critic_a2c,
            n_steps=5, alpha_theta=0.01, alpha_w=0.05,
            gamma=0.9, use_gae=True, gae_lambda=0.95
        )
        
        rewards = a2c.learn_steps(env, n_steps=10)
        
        assert len(rewards) == 10
        assert a2c.step_count == 10
        print(f"  âœ“ A2Cæµ‹è¯•é€šè¿‡ï¼Œæ­¥æ•°={a2c.step_count}")
        
        print("\nâœ… Actor-Criticç®—æ³•æµ‹è¯•å…¨éƒ¨é€šè¿‡ï¼")
        return True
        
    except Exception as e:
        print(f"\nâŒ Actor-Criticç®—æ³•æµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return False


def test_natural_policy_gradient():
    """
    æµ‹è¯•è‡ªç„¶ç­–ç•¥æ¢¯åº¦
    Test natural policy gradient
    """
    print("\n" + "="*60)
    print("æµ‹è¯•è‡ªç„¶ç­–ç•¥æ¢¯åº¦...")
    print("Testing Natural Policy Gradient...")
    print("="*60)
    
    try:
        from src.ch13_policy_gradient.natural_policy_gradient import (
            NaturalPolicyGradient, PPO
        )
        from src.ch13_policy_gradient.policy_gradient_theorem import SoftmaxPolicy
        from src.ch13_policy_gradient.reinforce import SimpleValueFunction
        
        n_features = 8
        n_actions = 2
        
        def state_action_features(state, action):
            features = np.zeros(n_features)
            if isinstance(state, int):
                base_idx = (state * n_actions + action) % n_features
                features[base_idx] = 1.0
            return features
        
        def state_features(state):
            features = np.zeros(n_features)
            if isinstance(state, int):
                features[state % n_features] = 1.0
            return features
        
        # ç®€å•ç¯å¢ƒ
        class TestEnv:
            def __init__(self):
                self.state = 0
                self.step_count = 0
            
            def reset(self):
                self.state = 0
                self.step_count = 0
                return self.state
            
            def step(self, action):
                self.state = min(3, self.state + action)
                reward = 10.0 if self.state == 3 else -1.0
                done = self.state == 3 or self.step_count >= 10
                self.step_count += 1
                return self.state, reward, done, {}
        
        # æµ‹è¯•è‡ªç„¶ç­–ç•¥æ¢¯åº¦
        print("\næµ‹è¯•Natural Policy Gradient...")
        npg_policy = SoftmaxPolicy(n_features, n_actions, state_action_features)
        npg = NaturalPolicyGradient(
            npg_policy, alpha=0.1, gamma=0.9, damping=0.01
        )
        
        env = TestEnv()
        episode_return = npg.learn_episode(env, max_steps=10)
        
        assert npg.episode_count == 1
        assert npg.fisher_matrix is not None
        assert len(npg.natural_gradient_norms) > 0
        print(f"  âœ“ Natural PGæµ‹è¯•é€šè¿‡ï¼Œå›æŠ¥={episode_return:.1f}")
        
        # æµ‹è¯•PPO
        print("\næµ‹è¯•PPO...")
        ppo_policy = SoftmaxPolicy(n_features, n_actions, state_action_features)
        ppo_value = SimpleValueFunction(n_features, state_features)
        
        ppo = PPO(
            ppo_policy, ppo_value,
            clip_epsilon=0.2, alpha_theta=0.01, alpha_w=0.05,
            gamma=0.9, lam=0.95, epochs=2, batch_size=32
        )
        
        # æ”¶é›†è½¨è¿¹
        ppo.collect_trajectories(env, n_steps=50)
        assert len(ppo.states) == 50
        assert len(ppo.advantages) == 50
        
        # PPOæ›´æ–°
        ppo.update()
        # clip_fractionså¯èƒ½åœ¨updateå†…éƒ¨è®¡ç®—ï¼Œä¸ä¸€å®šç«‹å³æœ‰å€¼
        if ppo.clip_fractions:
            print(f"  âœ“ PPOæµ‹è¯•é€šè¿‡ï¼Œè£å‰ªæ¯”ä¾‹={ppo.clip_fractions[-1]:.3f}")
        else:
            print(f"  âœ“ PPOæµ‹è¯•é€šè¿‡ï¼ŒçŠ¶æ€æ•°={len(ppo.states)}")
        
        print("\nâœ… è‡ªç„¶ç­–ç•¥æ¢¯åº¦æµ‹è¯•å…¨éƒ¨é€šè¿‡ï¼")
        return True
        
    except Exception as e:
        print(f"\nâŒ è‡ªç„¶ç­–ç•¥æ¢¯åº¦æµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return False


def test_integration():
    """
    æµ‹è¯•é›†æˆåœºæ™¯
    Test integration scenarios
    """
    print("\n" + "="*60)
    print("æµ‹è¯•é›†æˆåœºæ™¯...")
    print("Testing Integration Scenarios...")
    print("="*60)
    
    try:
        from src.ch13_policy_gradient.policy_gradient_theorem import SoftmaxPolicy
        from src.ch13_policy_gradient.reinforce import REINFORCE, SimpleValueFunction
        from src.ch13_policy_gradient.actor_critic import OneStepActorCritic, SimpleCritic, SimpleActor
        
        n_features = 6
        n_actions = 2
        
        def features(state, action):
            f = np.zeros(n_features)
            if isinstance(state, int):
                f[(state * n_actions + action) % n_features] = 1.0
            return f
        
        # æ›´å¤æ‚çš„ç¯å¢ƒ
        class ChainEnv:
            """é“¾å¼MDPç¯å¢ƒ"""
            def __init__(self, n_states=5):
                self.n_states = n_states
                self.state = 0
                self.step_count = 0
            
            def reset(self):
                self.state = 0
                self.step_count = 0
                return self.state
            
            def step(self, action):
                if action == 1 and self.state < self.n_states - 1:
                    self.state += 1
                elif action == 0 and self.state > 0:
                    self.state -= 1
                
                # åªåœ¨æœ€å³ç«¯ç»™å¥–åŠ±
                if self.state == self.n_states - 1:
                    reward = 10.0
                    done = True
                else:
                    reward = -0.1
                    done = False
                
                self.step_count += 1
                if self.step_count >= 50:
                    done = True
                
                return self.state, reward, done, {}
        
        env = ChainEnv(n_states=5)
        
        # æ¯”è¾ƒREINFORCEå’ŒActor-Critic
        print("\nè®­ç»ƒå¹¶æ¯”è¾ƒç®—æ³•...")
        
        # REINFORCE
        policy_reinforce = SoftmaxPolicy(n_features, n_actions, features)
        reinforce = REINFORCE(policy_reinforce, alpha=0.1, gamma=0.95)
        
        reinforce_returns = []
        for _ in range(10):
            ret = reinforce.learn_episode(env, max_steps=50)
            reinforce_returns.append(ret)
        
        # Actor-Critic
        actor = SimpleActor(n_features, n_actions, features)
        
        def state_features(state):
            f = np.zeros(n_features)
            if isinstance(state, int):
                f[state % n_features] = 1.0
            return f
        
        critic = SimpleCritic(n_features, state_features)
        ac = OneStepActorCritic(actor, critic, alpha_theta=0.05, alpha_w=0.1, gamma=0.95)
        
        ac_returns = []
        for _ in range(10):
            ret = ac.learn_episode(env, max_steps=50)
            ac_returns.append(ret)
        
        print(f"\nREINFORCEå¹³å‡å›æŠ¥: {np.mean(reinforce_returns):.2f}")
        print(f"Actor-Criticå¹³å‡å›æŠ¥: {np.mean(ac_returns):.2f}")
        
        # éªŒè¯å­¦ä¹ æ˜¯å¦å‘ç”Ÿ
        assert len(reinforce.episode_returns) == 10
        assert len(ac.td_errors) > 0
        
        print("\nâœ… é›†æˆåœºæ™¯æµ‹è¯•é€šè¿‡ï¼")
        return True
        
    except Exception as e:
        print(f"\nâŒ é›†æˆåœºæ™¯æµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return False


def main():
    """
    è¿è¡Œæ‰€æœ‰æµ‹è¯•
    Run all tests
    """
    print("\n" + "="*80)
    print("ç¬¬13ç« ï¼šç­–ç•¥æ¢¯åº¦æ–¹æ³• - æ¨¡å—æµ‹è¯•")
    print("Chapter 13: Policy Gradient Methods - Module Tests")
    print("="*80)
    
    tests = [
        ("ç­–ç•¥æ¢¯åº¦å®šç†", test_policy_gradient_theorem),
        ("REINFORCEç®—æ³•", test_reinforce),
        ("Actor-Criticç®—æ³•", test_actor_critic),
        ("è‡ªç„¶ç­–ç•¥æ¢¯åº¦", test_natural_policy_gradient),
        ("é›†æˆåœºæ™¯", test_integration)
    ]
    
    results = []
    start_time = time.time()
    
    for name, test_func in tests:
        print(f"\nè¿è¡Œæµ‹è¯•: {name}")
        success = test_func()
        results.append((name, success))
        
        if not success:
            print(f"\nâš ï¸ æµ‹è¯•å¤±è´¥ï¼Œåœæ­¢åç»­æµ‹è¯•")
            break
    
    total_time = time.time() - start_time
    
    # æ€»ç»“
    print("\n" + "="*80)
    print("æµ‹è¯•æ€»ç»“ Test Summary")
    print("="*80)
    
    all_passed = True
    for name, success in results:
        status = "âœ… é€šè¿‡" if success else "âŒ å¤±è´¥"
        print(f"{name}: {status}")
        if not success:
            all_passed = False
    
    print(f"\næ€»æµ‹è¯•æ—¶é—´: {total_time:.2f}ç§’")
    
    if all_passed:
        print("\nğŸ‰ ç¬¬13ç« æ‰€æœ‰ç­–ç•¥æ¢¯åº¦æ¨¡å—æµ‹è¯•é€šè¿‡ï¼")
        print("ğŸ‰ All Chapter 13 Policy Gradient modules passed!")
        print("\nç­–ç•¥æ¢¯åº¦æ–¹æ³•å®ç°éªŒè¯å®Œæˆ:")
        print("âœ“ ç­–ç•¥æ¢¯åº¦å®šç†")
        print("âœ“ REINFORCEç®—æ³•")
        print("âœ“ Actor-Criticæ–¹æ³•")
        print("âœ“ è‡ªç„¶ç­–ç•¥æ¢¯åº¦å’ŒPPO")
        print("\nä»å€¼å‡½æ•°åˆ°ç›´æ¥ç­–ç•¥ä¼˜åŒ–çš„è½¬å˜å®Œæˆï¼")
        print("Transition from value functions to direct policy optimization complete!")
    else:
        print("\nâš ï¸ æœ‰äº›æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä»£ç ")
        print("âš ï¸ Some tests failed, please check the code")
    
    return all_passed


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)