"""
================================================================================
ç¬¬1.8èŠ‚ï¼šæ¢¯åº¦èµŒåšæœºç®—æ³• - åŸºäºåå¥½çš„å­¦ä¹ 
Section 1.8: Gradient Bandit Algorithm - Preference-Based Learning
================================================================================

æ¢¯åº¦èµŒåšæœºç®—æ³•ä¸ä¼°è®¡åŠ¨ä½œä»·å€¼ï¼Œè€Œæ˜¯å­¦ä¹ åŠ¨ä½œåå¥½
Gradient bandit doesn't estimate action values, but learns action preferences

æ ¸å¿ƒæ€æƒ³ Core Idea:
ä½¿ç”¨softmaxç­–ç•¥ï¼Œé€šè¿‡éšæœºæ¢¯åº¦ä¸Šå‡ä¼˜åŒ–æœŸæœ›å¥–åŠ±
Use softmax policy and optimize expected reward via stochastic gradient ascent

è¿™ä¸ªç®—æ³•å±•ç¤ºäº†ç­–ç•¥æ¢¯åº¦æ–¹æ³•çš„é›å½¢ï¼
This algorithm shows the prototype of policy gradient methods!
"""

import numpy as np
from typing import Optional, Dict, Any, List
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
import logging

from .bandit_introduction import BaseBanditAgent, MultiArmedBandit

# è®¾ç½®æ—¥å¿—
logger = logging.getLogger(__name__)


# ================================================================================
# ç¬¬1.8.1èŠ‚ï¼šæ¢¯åº¦èµŒåšæœºåŸç†
# Section 1.8.1: Gradient Bandit Principle
# ================================================================================

class GradientBanditPrinciple:
    """
    æ¢¯åº¦èµŒåšæœºç®—æ³•åŸç†
    Gradient Bandit Algorithm Principle
    
    è¿™æ˜¯ç­–ç•¥æ¢¯åº¦æ–¹æ³•çš„æœ€ç®€å•å½¢å¼
    This is the simplest form of policy gradient methods
    """
    
    @staticmethod
    def explain_principle():
        """
        è¯¦è§£æ¢¯åº¦èµŒåšæœºåŸç†
        Detailed Explanation of Gradient Bandit Principle
        """
        print("\n" + "="*80)
        print("æ¢¯åº¦èµŒåšæœºç®—æ³•åŸç†")
        print("Gradient Bandit Algorithm Principle")
        print("="*80)
        
        print("""
        1. æ ¸å¿ƒæ¦‚å¿µï¼šåå¥½è€Œéä»·å€¼
        Core Concept: Preferences, Not Values
        ----------------------------------------
        
        ä¸åŒäºä¹‹å‰çš„ç®—æ³•ï¼Œæ¢¯åº¦èµŒåšæœºï¼š
        Unlike previous algorithms, gradient bandit:
        
        â€¢ ä¸ä¼°è®¡åŠ¨ä½œä»·å€¼ Q(a)
          Doesn't estimate action values Q(a)
        â€¢ ç»´æŠ¤åŠ¨ä½œåå¥½ H(a) âˆˆ â„
          Maintains action preferences H(a) âˆˆ â„
        â€¢ ä½¿ç”¨softmaxå°†åå¥½è½¬æ¢ä¸ºæ¦‚ç‡
          Uses softmax to convert preferences to probabilities
        
        åŠ¨ä½œé€‰æ‹©æ¦‚ç‡ Action selection probability:
        
        Ï€_t(a) = P(A_t = a) = exp(H_t(a)) / Î£_b exp(H_t(b))
        
        è¿™å°±æ˜¯softmaxç­–ç•¥ï¼
        This is the softmax policy!
        
        2. æ¢¯åº¦ä¸Šå‡ä¼˜åŒ–
        Gradient Ascent Optimization
        -----------------------------
        
        ç›®æ ‡ï¼šæœ€å¤§åŒ–æœŸæœ›å¥–åŠ±
        Goal: Maximize expected reward
        
        J = E[R_t] = Î£_a Ï€_t(a)Â·q*(a)
        
        ä½¿ç”¨éšæœºæ¢¯åº¦ä¸Šå‡ï¼š
        Using stochastic gradient ascent:
        
        H_{t+1}(a) = H_t(a) + Î±Â·âˆ‚E[R_t]/âˆ‚H_t(a)
        
        æ¢¯åº¦æ¨å¯¼ï¼ˆREINFORCEç®—æ³•çš„é›å½¢ï¼‰ï¼š
        Gradient derivation (prototype of REINFORCE):
        
        âˆ‚E[R_t]/âˆ‚H_t(a) = E[(R_t - baseline)Â·(ğŸ™_{A_t=a} - Ï€_t(a))]
        
        å…¶ä¸­ Where:
        - R_t: æ—¶åˆ»tçš„å¥–åŠ± Reward at time t
        - baseline: åŸºçº¿ï¼ˆé€šå¸¸æ˜¯å¹³å‡å¥–åŠ±ï¼‰Baseline (usually average reward)
        - ğŸ™_{A_t=a}: æŒ‡ç¤ºå‡½æ•° Indicator function
        - Ï€_t(a): åŠ¨ä½œaçš„æ¦‚ç‡ Probability of action a
        
        3. æ›´æ–°è§„åˆ™
        Update Rule
        -----------
        
        å¯¹äºé€‰ä¸­çš„åŠ¨ä½œ A_tï¼š
        For selected action A_t:
        
        H_{t+1}(A_t) = H_t(A_t) + Î±(R_t - RÌ„_t)(1 - Ï€_t(A_t))
        
        å¯¹äºå…¶ä»–åŠ¨ä½œ a â‰  A_tï¼š
        For other actions a â‰  A_t:
        
        H_{t+1}(a) = H_t(a) - Î±(R_t - RÌ„_t)Ï€_t(a)
        
        å…¶ä¸­RÌ„_tæ˜¯åŸºçº¿ï¼ˆå¹³å‡å¥–åŠ±ï¼‰
        Where RÌ„_t is the baseline (average reward)
        
        4. ç›´è§‚ç†è§£
        Intuitive Understanding
        -----------------------
        
        â€¢ å¦‚æœå¥–åŠ± > åŸºçº¿ï¼š
          If reward > baseline:
          - å¢åŠ é€‰ä¸­åŠ¨ä½œçš„åå¥½
            Increase preference for selected action
          - å‡å°‘å…¶ä»–åŠ¨ä½œçš„åå¥½
            Decrease preference for other actions
            
        â€¢ å¦‚æœå¥–åŠ± < åŸºçº¿ï¼š
          If reward < baseline:
          - å‡å°‘é€‰ä¸­åŠ¨ä½œçš„åå¥½
            Decrease preference for selected action
          - å¢åŠ å…¶ä»–åŠ¨ä½œçš„åå¥½
            Increase preference for other actions
        
        5. ä¸ºä»€ä¹ˆä½¿ç”¨åŸºçº¿ï¼Ÿ
        Why Use a Baseline?
        -------------------
        
        åŸºçº¿å‡å°‘æ–¹å·®ï¼ŒåŠ é€Ÿå­¦ä¹ ï¼š
        Baseline reduces variance and speeds up learning:
        
        Var[gradient] with baseline < Var[gradient] without baseline
        
        æœ€ä¼˜åŸºçº¿ï¼šE[R_tÂ²]/E[R_t]
        Optimal baseline: E[R_tÂ²]/E[R_t]
        
        å®è·µä¸­ä½¿ç”¨ç§»åŠ¨å¹³å‡ï¼š
        In practice, use moving average:
        RÌ„_{t+1} = RÌ„_t + Î²(R_t - RÌ„_t)
        """)
        
        # å¯è§†åŒ–
        fig, axes = plt.subplots(1, 3, figsize=(15, 4))
        
        # å·¦å›¾ï¼šåå¥½åˆ°æ¦‚ç‡çš„è½¬æ¢ï¼ˆsoftmaxï¼‰
        ax1 = axes[0]
        H_values = np.linspace(-3, 3, 100)
        for temp in [0.5, 1.0, 2.0]:
            # æ¨¡æ‹Ÿ3ä¸ªåŠ¨ä½œçš„æƒ…å†µ
            H = np.array([H_values, np.zeros_like(H_values), -np.ones_like(H_values)])
            exp_H = np.exp(H / temp)
            probs = exp_H[0] / np.sum(exp_H, axis=0)
            ax1.plot(H_values, probs, label=f'Ï„={temp}', alpha=0.8)
        
        ax1.set_xlabel('Preference H(a) / åå¥½')
        ax1.set_ylabel('Probability Ï€(a) / æ¦‚ç‡')
        ax1.set_title('Softmax: Preference to Probability / Softmaxï¼šåå¥½åˆ°æ¦‚ç‡')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        ax1.set_ylim([0, 1])
        
        # ä¸­å›¾ï¼šæ¢¯åº¦æ›´æ–°æ–¹å‘
        ax2 = axes[1]
        rewards = np.linspace(-2, 2, 100)
        baseline = 0
        gradient_selected = (rewards - baseline) * (1 - 0.3)  # Ï€(a)=0.3
        gradient_others = -(rewards - baseline) * 0.3
        
        ax2.plot(rewards, gradient_selected, 'b-', label='Selected Action', linewidth=2)
        ax2.plot(rewards, gradient_others, 'r-', label='Other Actions', linewidth=2)
        ax2.axhline(y=0, color='black', linestyle='-', linewidth=0.5)
        ax2.axvline(x=baseline, color='green', linestyle='--', label='Baseline')
        ax2.fill_between(rewards[rewards > baseline], 0, gradient_selected[rewards > baseline],
                         alpha=0.3, color='blue', label='Increase preference')
        ax2.fill_between(rewards[rewards < baseline], 0, gradient_selected[rewards < baseline],
                         alpha=0.3, color='red', label='Decrease preference')
        
        ax2.set_xlabel('Reward R_t / å¥–åŠ±')
        ax2.set_ylabel('Gradient âˆ‡H / æ¢¯åº¦')
        ax2.set_title('Gradient Direction / æ¢¯åº¦æ–¹å‘')
        ax2.legend(loc='best', fontsize=8)
        ax2.grid(True, alpha=0.3)
        
        # å³å›¾ï¼šåŸºçº¿çš„ä½œç”¨
        ax3 = axes[2]
        n_samples = 1000
        rewards_no_baseline = np.random.normal(1, 2, n_samples)
        rewards_with_baseline = rewards_no_baseline - np.mean(rewards_no_baseline)
        
        # è®¡ç®—æ¢¯åº¦çš„æ–¹å·®
        grad_no_baseline = rewards_no_baseline * (1 - 0.3)
        grad_with_baseline = rewards_with_baseline * (1 - 0.3)
        
        data = [grad_no_baseline, grad_with_baseline]
        labels = ['No Baseline', 'With Baseline']
        
        bp = ax3.boxplot(data, labels=labels, patch_artist=True)
        colors = ['lightcoral', 'lightgreen']
        for patch, color in zip(bp['boxes'], colors):
            patch.set_facecolor(color)
            patch.set_alpha(0.7)
        
        ax3.set_ylabel('Gradient Value / æ¢¯åº¦å€¼')
        ax3.set_title('Baseline Reduces Variance / åŸºçº¿å‡å°‘æ–¹å·®')
        ax3.grid(True, alpha=0.3, axis='y')
        
        # æ·»åŠ æ–¹å·®æ–‡æœ¬
        var_no_baseline = np.var(grad_no_baseline)
        var_with_baseline = np.var(grad_with_baseline)
        ax3.text(0.5, 0.95, f'Var(no baseline) = {var_no_baseline:.2f}\n'
                           f'Var(with baseline) = {var_with_baseline:.2f}\n'
                           f'Reduction = {(1-var_with_baseline/var_no_baseline)*100:.1f}%',
                transform=ax3.transAxes, ha='center', va='top',
                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
        
        plt.tight_layout()
        return fig


# ================================================================================
# ç¬¬1.8.2èŠ‚ï¼šæ¢¯åº¦èµŒåšæœºå®ç°
# Section 1.8.2: Gradient Bandit Implementation
# ================================================================================

class GradientBanditAgent(BaseBanditAgent):
    """
    æ¢¯åº¦èµŒåšæœºæ™ºèƒ½ä½“
    Gradient Bandit Agent
    
    ä½¿ç”¨softmaxç­–ç•¥å’Œæ¢¯åº¦ä¸Šå‡
    Uses softmax policy and gradient ascent
    """
    
    def __init__(self, k: int = None,
                 n_arms: int = None,
                 alpha: float = 0.1,
                 use_baseline: bool = True,
                 baseline_alpha: float = 0.1,
                 temperature: float = 1.0,
                 **kwargs):
        """
        åˆå§‹åŒ–æ¢¯åº¦èµŒåšæœº
        Initialize gradient bandit
        
        Args:
            k: åŠ¨ä½œæ•°é‡ Number of actions
            alpha: å­¦ä¹ ç‡ Learning rate
            use_baseline: æ˜¯å¦ä½¿ç”¨åŸºçº¿ Whether to use baseline
            baseline_alpha: åŸºçº¿æ›´æ–°ç‡ Baseline update rate
            temperature: Softmaxæ¸©åº¦å‚æ•° Softmax temperature
            **kwargs: ä¼ é€’ç»™çˆ¶ç±»çš„å‚æ•°
        
        æ·±å…¥ç†è§£ Deep Understanding:
        
        1. åå¥½åˆå§‹åŒ–ï¼š
           Preference initialization:
           - H(a) = 0: æ‰€æœ‰åŠ¨ä½œæ¦‚ç‡ç›¸ç­‰ All actions equally likely
           - H(a) ~ N(0,Ïƒ): æ‰“ç ´å¯¹ç§°æ€§ Break symmetry
        
        2. æ¸©åº¦å‚æ•°ï¼š
           Temperature parameter:
           - Ï„ < 1: æ›´ç¡®å®šçš„é€‰æ‹© More deterministic
           - Ï„ = 1: æ ‡å‡†softmax Standard softmax
           - Ï„ > 1: æ›´éšæœºçš„é€‰æ‹© More random
        
        3. åŸºçº¿é€‰æ‹©ï¼š
           Baseline choice:
           - 0: ç®€å•ä½†æ¬¡ä¼˜ Simple but suboptimal
           - å¹³å‡å¥–åŠ±: å¸¸ç”¨é€‰æ‹© Common choice
           - åŠ æƒå¹³å‡: æ›´ç²¾ç¡® More accurate
        """
        # å¤„ç†n_armså‚æ•°ï¼ˆå‘åå…¼å®¹ï¼‰
        if n_arms is not None:
            k = n_arms
        if k is None:
            raise ValueError("å¿…é¡»æä¾›kæˆ–n_armså‚æ•°")
            
        # æ³¨æ„ï¼šæ¢¯åº¦èµŒåšæœºä¸ä½¿ç”¨Qå€¼ï¼Œæ‰€ä»¥ä¼ é€’ç‰¹æ®Šå‚æ•°ç»™çˆ¶ç±»
        super().__init__(k, initial_value=0.0, **kwargs)
        
        self.alpha = alpha
        self.use_baseline = use_baseline
        self.baseline_alpha = baseline_alpha
        self.temperature = temperature
        
        # åŠ¨ä½œåå¥½ï¼ˆä¸æ˜¯ä»·å€¼ï¼ï¼‰
        # Action preferences (not values!)
        self.H = np.zeros(k)
        
        # åŸºçº¿ï¼ˆå¹³å‡å¥–åŠ±ï¼‰
        # Baseline (average reward)
        self.baseline = 0.0
        
        # åŠ¨ä½œæ¦‚ç‡
        # Action probabilities
        self.pi = np.ones(k) / k
        
        # ç»Ÿè®¡
        self.total_steps = 0
        
        logger.info(f"åˆå§‹åŒ–æ¢¯åº¦èµŒåšæœº: k={k}, Î±={alpha}, "
                   f"baseline={use_baseline}, Ï„={temperature}")
    
    def _compute_softmax(self) -> np.ndarray:
        """
        è®¡ç®—softmaxæ¦‚ç‡
        Compute softmax probabilities
        
        ä½¿ç”¨æ•°å€¼ç¨³å®šçš„å®ç°
        Use numerically stable implementation
        
        Returns:
            åŠ¨ä½œæ¦‚ç‡åˆ†å¸ƒ Action probability distribution
        """
        # æ•°å€¼ç¨³å®šæ€§ï¼šå‡å»æœ€å¤§å€¼
        # Numerical stability: subtract maximum
        H_stable = self.H - np.max(self.H)
        
        # åº”ç”¨æ¸©åº¦å‚æ•°
        # Apply temperature parameter
        exp_H = np.exp(H_stable / self.temperature)
        
        # å½’ä¸€åŒ–å¾—åˆ°æ¦‚ç‡
        # Normalize to get probabilities
        self.pi = exp_H / np.sum(exp_H)
        
        return self.pi
    
    def select_action(self) -> int:
        """
        ä½¿ç”¨softmaxç­–ç•¥é€‰æ‹©åŠ¨ä½œ
        Select action using softmax policy
        
        Returns:
            é€‰æ‹©çš„åŠ¨ä½œ Selected action
        """
        # è®¡ç®—åŠ¨ä½œæ¦‚ç‡
        # Compute action probabilities
        probabilities = self._compute_softmax()
        
        # æŒ‰æ¦‚ç‡é€‰æ‹©åŠ¨ä½œ
        # Select action according to probabilities
        action = np.random.choice(self.k, p=probabilities)
        
        self.total_steps += 1
        
        logger.debug(f"Step {self.total_steps}: "
                    f"Selected action {action} with prob {probabilities[action]:.3f}")
        
        return action
    
    def update(self, action: int, reward: float):
        """
        ä½¿ç”¨æ¢¯åº¦ä¸Šå‡æ›´æ–°åå¥½
        Update preferences using gradient ascent
        
        è¿™æ˜¯REINFORCEç®—æ³•çš„ç®€åŒ–ç‰ˆæœ¬ï¼
        This is a simplified version of REINFORCE!
        
        Args:
            action: æ‰§è¡Œçš„åŠ¨ä½œ Action taken
            reward: è·å¾—çš„å¥–åŠ± Reward received
        """
        # æ›´æ–°åŸºçº¿ï¼ˆå¦‚æœä½¿ç”¨ï¼‰
        # Update baseline (if used)
        if self.use_baseline:
            # æŒ‡æ•°ç§»åŠ¨å¹³å‡
            # Exponential moving average
            self.baseline += self.baseline_alpha * (reward - self.baseline)
            advantage = reward - self.baseline
        else:
            advantage = reward
        
        # æ¢¯åº¦ä¸Šå‡æ›´æ–°
        # Gradient ascent update
        
        # å¯¹äºé€‰ä¸­çš„åŠ¨ä½œ
        # For selected action
        self.H[action] += self.alpha * advantage * (1 - self.pi[action])
        
        # å¯¹äºå…¶ä»–åŠ¨ä½œ
        # For other actions
        for a in range(self.k):
            if a != action:
                self.H[a] -= self.alpha * advantage * self.pi[a]
        
        # è®°å½•ï¼ˆçˆ¶ç±»æ–¹æ³•ï¼‰
        # Record (parent class method)
        self.action_history.append(action)
        self.reward_history.append(reward)
        
        # æ³¨æ„ï¼šæ¢¯åº¦èµŒåšæœºä¸æ›´æ–°Qå€¼
        # Note: Gradient bandit doesn't update Q values
        self.N[action] += 1
        
        logger.debug(f"Updated H[{action}]: advantage={advantage:.3f}, "
                    f"new H={self.H[action]:.3f}")
    
    def reset(self):
        """
        é‡ç½®æ™ºèƒ½ä½“
        Reset agent
        """
        super().reset()
        self.H = np.zeros(self.k)
        self.baseline = 0.0
        self.pi = np.ones(self.k) / self.k
        self.total_steps = 0
    
    def get_statistics(self) -> Dict[str, Any]:
        """
        è·å–ç»Ÿè®¡ä¿¡æ¯
        Get statistics
        """
        stats = super().get_statistics()
        stats.update({
            'preferences': self.H.copy(),
            'probabilities': self.pi.copy(),
            'baseline': self.baseline,
            'temperature': self.temperature,
            'entropy': -np.sum(self.pi * np.log(self.pi + 1e-10))  # ç­–ç•¥ç†µ
        })
        return stats


# ================================================================================
# ç¬¬1.8.3èŠ‚ï¼šæ”¹è¿›çš„æ¢¯åº¦èµŒåšæœºå˜ä½“
# Section 1.8.3: Improved Gradient Bandit Variants
# ================================================================================

class NaturalGradientBandit(GradientBanditAgent):
    """
    è‡ªç„¶æ¢¯åº¦èµŒåšæœº
    Natural Gradient Bandit
    
    ä½¿ç”¨Fisherä¿¡æ¯çŸ©é˜µä¿®æ­£æ¢¯åº¦æ–¹å‘
    Use Fisher information matrix to correct gradient direction
    """
    
    def __init__(self, k: int, **kwargs):
        """åˆå§‹åŒ–è‡ªç„¶æ¢¯åº¦èµŒåšæœº"""
        super().__init__(k, **kwargs)
        
        # Fisherä¿¡æ¯çŸ©é˜µçš„ä¼°è®¡
        self.fisher_matrix = np.eye(k) * 0.01  # åˆå§‹åŒ–ä¸ºå°çš„å¯¹è§’çŸ©é˜µ
        self.fisher_alpha = 0.01  # FisherçŸ©é˜µæ›´æ–°ç‡
    
    def update(self, action: int, reward: float):
        """
        è‡ªç„¶æ¢¯åº¦æ›´æ–°
        Natural gradient update
        
        è‡ªç„¶æ¢¯åº¦ = Fisher^{-1} Ã— æ™®é€šæ¢¯åº¦
        Natural gradient = Fisher^{-1} Ã— ordinary gradient
        """
        # è®¡ç®—æ™®é€šæ¢¯åº¦
        if self.use_baseline:
            self.baseline += self.baseline_alpha * (reward - self.baseline)
            advantage = reward - self.baseline
        else:
            advantage = reward
        
        # æ„å»ºæ¢¯åº¦å‘é‡
        gradient = np.zeros(self.k)
        gradient[action] = advantage * (1 - self.pi[action])
        for a in range(self.k):
            if a != action:
                gradient[a] = -advantage * self.pi[a]
        
        # æ›´æ–°Fisherä¿¡æ¯çŸ©é˜µï¼ˆè¿‘ä¼¼ï¼‰
        # å¯¹äºsoftmaxç­–ç•¥ï¼šF = diag(Ï€) - Ï€Ï€^T
        self.fisher_matrix = (1 - self.fisher_alpha) * self.fisher_matrix + \
                           self.fisher_alpha * (np.diag(self.pi) - np.outer(self.pi, self.pi))
        
        # æ·»åŠ æ­£åˆ™åŒ–é¿å…å¥‡å¼‚
        self.fisher_matrix += np.eye(self.k) * 0.01
        
        # è®¡ç®—è‡ªç„¶æ¢¯åº¦
        try:
            natural_gradient = np.linalg.solve(self.fisher_matrix, gradient)
        except np.linalg.LinAlgError:
            # å¦‚æœçŸ©é˜µå¥‡å¼‚ï¼Œä½¿ç”¨æ™®é€šæ¢¯åº¦
            natural_gradient = gradient
        
        # æ›´æ–°åå¥½
        self.H += self.alpha * natural_gradient
        
        # æ›´æ–°å…¶ä»–ç»Ÿè®¡
        self.action_history.append(action)
        self.reward_history.append(reward)
        self.N[action] += 1


class AdaptiveGradientBandit(GradientBanditAgent):
    """
    è‡ªé€‚åº”æ¢¯åº¦èµŒåšæœº
    Adaptive Gradient Bandit
    
    è‡ªåŠ¨è°ƒæ•´å­¦ä¹ ç‡å’Œæ¸©åº¦å‚æ•°
    Automatically adjust learning rate and temperature
    """
    
    def __init__(self, k: int, **kwargs):
        """åˆå§‹åŒ–è‡ªé€‚åº”æ¢¯åº¦èµŒåšæœº"""
        super().__init__(k, **kwargs)
        
        # è‡ªé€‚åº”å‚æ•°
        self.alpha_initial = self.alpha
        self.temperature_initial = self.temperature
        
        # æ€§èƒ½è·Ÿè¸ª
        self.performance_window = []
        self.window_size = 100
    
    def update(self, action: int, reward: float):
        """
        è‡ªé€‚åº”æ›´æ–°
        Adaptive update
        """
        # è·Ÿè¸ªæ€§èƒ½
        self.performance_window.append(reward)
        if len(self.performance_window) > self.window_size:
            self.performance_window.pop(0)
        
        # æ ¹æ®æ€§èƒ½è°ƒæ•´å‚æ•°
        if len(self.performance_window) >= self.window_size:
            # è®¡ç®—æ€§èƒ½è¶‹åŠ¿
            first_half = np.mean(self.performance_window[:self.window_size//2])
            second_half = np.mean(self.performance_window[self.window_size//2:])
            improvement = second_half - first_half
            
            # è°ƒæ•´å­¦ä¹ ç‡
            if improvement > 0:
                # æ€§èƒ½æå‡ï¼Œå¯ä»¥å¢åŠ å­¦ä¹ ç‡
                self.alpha = min(1.0, self.alpha * 1.01)
            else:
                # æ€§èƒ½ä¸‹é™ï¼Œå‡å°å­¦ä¹ ç‡
                self.alpha = max(0.001, self.alpha * 0.99)
            
            # è°ƒæ•´æ¸©åº¦ï¼ˆæ¢ç´¢ç¨‹åº¦ï¼‰
            # è®¡ç®—ç­–ç•¥ç†µ
            entropy = -np.sum(self.pi * np.log(self.pi + 1e-10))
            target_entropy = np.log(self.k) * 0.5  # ç›®æ ‡ç†µä¸ºæœ€å¤§ç†µçš„ä¸€åŠ
            
            if entropy < target_entropy:
                # ç†µå¤ªå°ï¼Œå¢åŠ æ¸©åº¦ï¼ˆæ›´å¤šæ¢ç´¢ï¼‰
                self.temperature = min(5.0, self.temperature * 1.01)
            else:
                # ç†µå¤ªå¤§ï¼Œå‡å°æ¸©åº¦ï¼ˆæ›´å¤šåˆ©ç”¨ï¼‰
                self.temperature = max(0.1, self.temperature * 0.99)
        
        # æ‰§è¡Œæ ‡å‡†æ›´æ–°
        super().update(action, reward)


class EntropyRegularizedGradientBandit(GradientBanditAgent):
    """
    ç†µæ­£åˆ™åŒ–æ¢¯åº¦èµŒåšæœº
    Entropy-Regularized Gradient Bandit
    
    åœ¨ç›®æ ‡ä¸­åŠ å…¥ç†µæ­£åˆ™é¡¹ï¼Œé¼“åŠ±æ¢ç´¢
    Add entropy regularization to encourage exploration
    
    è¿™æ˜¯è½¯æ¼”å‘˜-è¯„è®ºå®¶(SAC)ç®—æ³•çš„é›å½¢ï¼
    This is a prototype of Soft Actor-Critic (SAC)!
    """
    
    def __init__(self, k: int,
                 entropy_coef: float = 0.01,
                 **kwargs):
        """
        åˆå§‹åŒ–ç†µæ­£åˆ™åŒ–æ¢¯åº¦èµŒåšæœº
        
        Args:
            entropy_coef: ç†µç³»æ•° Entropy coefficient
        """
        super().__init__(k, **kwargs)
        self.entropy_coef = entropy_coef
    
    def update(self, action: int, reward: float):
        """
        å¸¦ç†µæ­£åˆ™çš„æ›´æ–°
        Update with entropy regularization
        
        ç›®æ ‡ï¼šJ = E[R] + Î²Â·H(Ï€)
        Objective: J = E[R] + Î²Â·H(Ï€)
        
        å…¶ä¸­H(Ï€)æ˜¯ç­–ç•¥ç†µ
        Where H(Ï€) is policy entropy
        """
        # æ›´æ–°åŸºçº¿
        if self.use_baseline:
            self.baseline += self.baseline_alpha * (reward - self.baseline)
            advantage = reward - self.baseline
        else:
            advantage = reward
        
        # æ ‡å‡†æ¢¯åº¦
        gradient = np.zeros(self.k)
        gradient[action] = advantage * (1 - self.pi[action])
        for a in range(self.k):
            if a != action:
                gradient[a] = -advantage * self.pi[a]
        
        # ç†µæ¢¯åº¦ï¼šâˆ‡H(Ï€) = -âˆ‡Î£_a Ï€(a)log Ï€(a)
        # Entropy gradient
        entropy_gradient = np.zeros(self.k)
        for a in range(self.k):
            if self.pi[a] > 0:
                entropy_gradient[a] = -self.pi[a] * (np.log(self.pi[a]) + 1) * (
                    (1 if a == action else 0) - self.pi[a]
                )
        
        # ç»„åˆæ¢¯åº¦
        # Combined gradient
        total_gradient = gradient + self.entropy_coef * entropy_gradient
        
        # æ›´æ–°åå¥½
        self.H += self.alpha * total_gradient
        
        # è®°å½•
        self.action_history.append(action)
        self.reward_history.append(reward)
        self.N[action] += 1


# ================================================================================
# ç¬¬1.8.4èŠ‚ï¼šæ¢¯åº¦èµŒåšæœºç†è®ºåˆ†æ
# Section 1.8.4: Gradient Bandit Theoretical Analysis
# ================================================================================

class GradientBanditAnalysis:
    """
    æ¢¯åº¦èµŒåšæœºç†è®ºåˆ†æ
    Gradient Bandit Theoretical Analysis
    """
    
    @staticmethod
    def convergence_analysis():
        """
        æ”¶æ•›æ€§åˆ†æ
        Convergence Analysis
        """
        print("\n" + "="*80)
        print("æ¢¯åº¦èµŒåšæœºæ”¶æ•›æ€§åˆ†æ")
        print("Gradient Bandit Convergence Analysis")
        print("="*80)
        
        print("""
        1. æ¢¯åº¦çš„æ— åæ€§
        Unbiasedness of Gradient
        ------------------------
        
        å®šç†ï¼šæ¢¯åº¦èµŒåšæœºçš„æ›´æ–°æ˜¯æœŸæœ›å¥–åŠ±çš„æ— åæ¢¯åº¦ä¼°è®¡
        Theorem: Gradient bandit update is unbiased gradient estimate
        
        è¯æ˜ Proof:
        E[âˆ‚J/âˆ‚H(a)] = E[(R_t - b)(ğŸ™_{A_t=a} - Ï€_t(a))]
                     = Î£_a' Ï€_t(a')[q*(a') - b][ğŸ™_{a'=a} - Ï€_t(a)]
                     = Ï€_t(a)[q*(a) - b][1 - Ï€_t(a)] - Ï€_t(a)Î£_{a'â‰ a} Ï€_t(a')[q*(a') - b]
                     = Ï€_t(a)[q*(a) - E_Ï€[q*]]  (å½“b = E_Ï€[q*]æ—¶)
                     = âˆ‚E_Ï€[q*]/âˆ‚H(a)
        
        è¿™è¯æ˜äº†ç®—æ³•ç¡®å®åœ¨ä¼˜åŒ–æœŸæœ›å¥–åŠ±ï¼
        This proves the algorithm is optimizing expected reward!
        
        2. æ”¶æ•›æ¡ä»¶
        Convergence Conditions
        ----------------------
        
        Robbins-Monroæ¡ä»¶ï¼š
        Î£_t Î±_t = âˆ ä¸” Î£_t Î±_tÂ² < âˆ
        
        ä¾‹å¦‚ For example:
        - Î±_t = c/t: æ»¡è¶³ï¼Œä¿è¯æ”¶æ•› Satisfies, guarantees convergence
        - Î±_t = c: ä¸æ»¡è¶³ç¬¬äºŒä¸ªæ¡ä»¶ Doesn't satisfy second condition
        
        3. æ”¶æ•›é€Ÿåº¦
        Convergence Rate
        ----------------
        
        åœ¨é€‚å½“æ¡ä»¶ä¸‹ï¼š
        Under appropriate conditions:
        
        E[||Ï€_t - Ï€*||Â²] = O(1/t^Î²)
        
        å…¶ä¸­Î² âˆˆ (0.5, 1]å–å†³äºé—®é¢˜ç»“æ„
        Where Î² âˆˆ (0.5, 1] depends on problem structure
        
        4. åŸºçº¿çš„ä½œç”¨
        Role of Baseline
        ----------------
        
        æ–¹å·®å‡å°‘ Variance Reduction:
        
        Var[gradient with baseline] / Var[gradient without] â‰ˆ 1 - ÏÂ²
        
        å…¶ä¸­Ïæ˜¯å¥–åŠ±ä¸æœ€ä¼˜åŸºçº¿çš„ç›¸å…³ç³»æ•°
        Where Ï is correlation between reward and optimal baseline
        
        æœ€ä¼˜åŸºçº¿ Optimal Baseline:
        b* = E[R_tÂ² Â· Ï€_t(A_t)] / E[Ï€_t(A_t)] = E[R_tÂ²] (å½“ç­–ç•¥å‡åŒ€æ—¶)
        
        5. ä¸ç­–ç•¥æ¢¯åº¦çš„è”ç³»
        Connection to Policy Gradient
        ------------------------------
        
        æ¢¯åº¦èµŒåšæœºæ˜¯REINFORCEç®—æ³•çš„ç‰¹ä¾‹ï¼š
        Gradient bandit is special case of REINFORCE:
        
        âˆ‡J(Î¸) = E_Ï€[(R - b)âˆ‡log Ï€(a|Î¸)]
        
        å¯¹äºsoftmaxç­–ç•¥ï¼š
        For softmax policy:
        âˆ‡log Ï€(a) = âˆ‡H(a) - E_Ï€[âˆ‡H]
                  = e_a - Ï€
        
        è¿™æ˜¯ç­–ç•¥æ¢¯åº¦æ–¹æ³•çš„èµ·ç‚¹ï¼
        This is the starting point of policy gradient methods!
        """)
    
    @staticmethod
    def demonstrate_convergence():
        """
        æ¼”ç¤ºæ”¶æ•›è¿‡ç¨‹
        Demonstrate Convergence Process
        """
        print("\næ¼”ç¤ºï¼šæ¢¯åº¦èµŒåšæœºæ”¶æ•›")
        print("Demo: Gradient Bandit Convergence")
        print("-" * 60)
        
        # åˆ›å»ºç®€å•çš„2è‡‚èµŒåšæœº
        k = 2
        true_values = np.array([0.3, 0.7])  # çœŸå®ä»·å€¼
        
        # ä¸åŒé…ç½®çš„æ¢¯åº¦èµŒåšæœº
        configs = [
            ('With Baseline', True, 0.1),
            ('No Baseline', False, 0.1),
            ('Large Î±', True, 0.5),
            ('Small Î±', True, 0.01),
        ]
        
        n_steps = 2000
        n_runs = 100
        
        results = {}
        
        for name, use_baseline, alpha in configs:
            all_probs = []
            
            for run in range(n_runs):
                agent = GradientBanditAgent(k=k, alpha=alpha, use_baseline=use_baseline)
                probs_history = []
                
                for step in range(n_steps):
                    # é€‰æ‹©åŠ¨ä½œ
                    agent._compute_softmax()
                    action = np.random.choice(k, p=agent.pi)
                    
                    # è·å¾—å¥–åŠ±ï¼ˆä¼¯åŠªåˆ©ï¼‰
                    reward = float(np.random.random() < true_values[action])
                    
                    # æ›´æ–°
                    agent.update(action, reward)
                    
                    # è®°å½•æ¦‚ç‡
                    probs_history.append(agent.pi[1])  # è®°å½•é€‰æ‹©æ›´å¥½è‡‚çš„æ¦‚ç‡
                
                all_probs.append(probs_history)
            
            results[name] = np.mean(all_probs, axis=0)
        
        # ç»˜åˆ¶ç»“æœ
        fig, axes = plt.subplots(1, 2, figsize=(12, 5))
        
        # å·¦å›¾ï¼šæ”¶æ•›è¿‡ç¨‹
        ax1 = axes[0]
        for name, probs in results.items():
            ax1.plot(probs, label=name, alpha=0.8)
        
        ax1.axhline(y=0.7, color='red', linestyle='--', alpha=0.5, 
                   label='Optimal Ï€(a*)')
        ax1.set_xlabel('Steps / æ­¥æ•°')
        ax1.set_ylabel('P(choosing better arm) / é€‰æ‹©æ›´å¥½è‡‚çš„æ¦‚ç‡')
        ax1.set_title('Convergence to Optimal Policy / æ”¶æ•›åˆ°æœ€ä¼˜ç­–ç•¥')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        ax1.set_ylim([0, 1])
        
        # å³å›¾ï¼šå­¦ä¹ é€Ÿåº¦æ¯”è¾ƒ
        ax2 = axes[1]
        convergence_steps = {}
        threshold = 0.65  # è®¤ä¸ºæ”¶æ•›çš„é˜ˆå€¼
        
        for name, probs in results.items():
            # æ‰¾åˆ°é¦–æ¬¡è¶…è¿‡é˜ˆå€¼çš„æ­¥æ•°
            converged = np.where(probs > threshold)[0]
            if len(converged) > 0:
                convergence_steps[name] = converged[0]
            else:
                convergence_steps[name] = n_steps
        
        names = list(convergence_steps.keys())
        steps = list(convergence_steps.values())
        colors = plt.cm.Set2(np.linspace(0, 1, len(names)))
        
        bars = ax2.bar(range(len(names)), steps, color=colors, alpha=0.7)
        ax2.set_xticks(range(len(names)))
        ax2.set_xticklabels(names, rotation=45, ha='right')
        ax2.set_ylabel('Steps to Converge / æ”¶æ•›æ­¥æ•°')
        ax2.set_title(f'Convergence Speed (threshold={threshold}) / æ”¶æ•›é€Ÿåº¦')
        ax2.grid(True, alpha=0.3, axis='y')
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, step in zip(bars, steps):
            ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height(),
                    f'{step}', ha='center', va='bottom')
        
        plt.tight_layout()
        
        print("\nå…³é”®å‘ç° Key Findings:")
        print("1. åŸºçº¿æ˜¾è‘—åŠ é€Ÿæ”¶æ•›")
        print("2. å­¦ä¹ ç‡å½±å“æ”¶æ•›é€Ÿåº¦å’Œç¨³å®šæ€§")
        print("3. å¤§å­¦ä¹ ç‡å¿«ä½†ä¸ç¨³å®š")
        print("4. å°å­¦ä¹ ç‡ç¨³å®šä½†æ…¢")
        
        return fig


# ================================================================================
# ç¬¬1.8.5èŠ‚ï¼šç»¼åˆæ¯”è¾ƒå®éªŒ
# Section 1.8.5: Comprehensive Comparison Experiment
# ================================================================================

def compare_all_algorithms():
    """
    æ¯”è¾ƒæ‰€æœ‰èµŒåšæœºç®—æ³•
    Compare all bandit algorithms
    """
    print("\n" + "="*80)
    print("ç»¼åˆç®—æ³•æ¯”è¾ƒ")
    print("Comprehensive Algorithm Comparison")
    print("="*80)
    
    # å¯¼å…¥å…¶ä»–ç®—æ³•
    from .epsilon_greedy import EpsilonGreedyAgent
    from .ucb_algorithm import UCBAgent
    
    # é…ç½®
    k = 10
    n_runs = 100
    n_steps = 1000
    
    # æ‰€æœ‰ç®—æ³•
    algorithms = [
        ('Îµ-Greedy (Îµ=0.1)', EpsilonGreedyAgent(k=k, epsilon=0.1)),
        ('UCB (c=2)', UCBAgent(k=k, c=2.0)),
        ('Gradient (baseline)', GradientBanditAgent(k=k, alpha=0.1, use_baseline=True)),
        ('Gradient (no baseline)', GradientBanditAgent(k=k, alpha=0.1, use_baseline=False)),
        ('Entropy-Regularized', EntropyRegularizedGradientBandit(k=k, alpha=0.1)),
    ]
    
    # è¿è¡Œå®éªŒ
    results = {name: {'rewards': [], 'optimal': [], 'regrets': []} 
              for name, _ in algorithms}
    
    print("è¿è¡Œç»¼åˆå®éªŒ...")
    for run in tqdm(range(n_runs), desc="Runs"):
        env = MultiArmedBandit(k=k, seed=run)
        
        for name, agent in algorithms:
            agent.reset()
            episode_data = agent.run_episode(env, n_steps)
            
            results[name]['rewards'].append(episode_data['rewards'])
            results[name]['optimal'].append(episode_data['optimal_actions'])
            results[name]['regrets'].append(episode_data['regrets'])
            
            env.reset()
    
    # ç»˜åˆ¶ç»“æœ
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    
    # 1. å¹³å‡å¥–åŠ±
    ax1 = axes[0, 0]
    for name in results:
        mean_rewards = np.mean(results[name]['rewards'], axis=0)
        ax1.plot(mean_rewards, label=name, alpha=0.8)
    ax1.set_xlabel('Steps / æ­¥æ•°')
    ax1.set_ylabel('Average Reward / å¹³å‡å¥–åŠ±')
    ax1.set_title('Average Reward Comparison / å¹³å‡å¥–åŠ±æ¯”è¾ƒ')
    ax1.legend(fontsize=8)
    ax1.grid(True, alpha=0.3)
    
    # 2. æœ€ä¼˜åŠ¨ä½œæ¯”ä¾‹
    ax2 = axes[0, 1]
    for name in results:
        optimal_rate = np.mean(results[name]['optimal'], axis=0) * 100
        ax2.plot(optimal_rate, label=name, alpha=0.8)
    ax2.set_xlabel('Steps / æ­¥æ•°')
    ax2.set_ylabel('Optimal Action % / æœ€ä¼˜åŠ¨ä½œç™¾åˆ†æ¯”')
    ax2.set_title('Optimal Action Selection / æœ€ä¼˜åŠ¨ä½œé€‰æ‹©')
    ax2.legend(fontsize=8)
    ax2.grid(True, alpha=0.3)
    ax2.set_ylim([0, 100])
    
    # 3. ç´¯ç§¯é—æ†¾ï¼ˆå¯¹æ•°å°ºåº¦ï¼‰
    ax3 = axes[1, 0]
    for name in results:
        mean_regrets = np.mean(results[name]['regrets'], axis=0)
        ax3.plot(mean_regrets, label=name, alpha=0.8)
    ax3.set_xlabel('Steps (log scale) / æ­¥æ•°ï¼ˆå¯¹æ•°å°ºåº¦ï¼‰')
    ax3.set_ylabel('Cumulative Regret / ç´¯ç§¯é—æ†¾')
    ax3.set_title('Cumulative Regret (Log Scale) / ç´¯ç§¯é—æ†¾ï¼ˆå¯¹æ•°å°ºåº¦ï¼‰')
    ax3.set_xscale('log')
    ax3.legend(fontsize=8)
    ax3.grid(True, alpha=0.3)
    
    # 4. æœ€ç»ˆæ€§èƒ½æ€»ç»“
    ax4 = axes[1, 1]
    
    # è®¡ç®—æœ€ç»ˆæŒ‡æ ‡
    final_metrics = {}
    for name in results:
        final_reward = np.mean([np.mean(r[-100:]) for r in results[name]['rewards']])
        final_optimal = np.mean([np.mean(o[-100:]) for o in results[name]['optimal']]) * 100
        final_regret = np.mean([r[-1] for r in results[name]['regrets']])
        final_metrics[name] = {
            'reward': final_reward,
            'optimal': final_optimal,
            'regret': final_regret
        }
    
    # åˆ›å»ºé›·è¾¾å›¾
    categories = ['Reward\n(normalized)', 'Optimal %\n(normalized)', 'Low Regret\n(normalized)']
    
    # å½’ä¸€åŒ–æŒ‡æ ‡ï¼ˆ0-1ï¼‰
    max_reward = max(m['reward'] for m in final_metrics.values())
    min_reward = min(m['reward'] for m in final_metrics.values())
    max_regret = max(m['regret'] for m in final_metrics.values())
    min_regret = min(m['regret'] for m in final_metrics.values())
    
    angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()
    angles += angles[:1]  # é—­åˆ
    
    ax4 = plt.subplot(2, 2, 4, projection='polar')
    
    for i, (name, metrics) in enumerate(final_metrics.items()):
        # å½’ä¸€åŒ–å€¼
        norm_reward = (metrics['reward'] - min_reward) / (max_reward - min_reward) if max_reward > min_reward else 0.5
        norm_optimal = metrics['optimal'] / 100
        norm_regret = 1 - (metrics['regret'] - min_regret) / (max_regret - min_regret) if max_regret > min_regret else 0.5
        
        values = [norm_reward, norm_optimal, norm_regret]
        values += values[:1]  # é—­åˆ
        
        ax4.plot(angles, values, 'o-', linewidth=2, label=name, alpha=0.7)
        ax4.fill(angles, values, alpha=0.1)
    
    ax4.set_xticks(angles[:-1])
    ax4.set_xticklabels(categories)
    ax4.set_ylim(0, 1)
    ax4.set_title('Overall Performance / æ•´ä½“æ€§èƒ½', y=1.08)
    ax4.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1), fontsize=8)
    ax4.grid(True)
    
    plt.tight_layout()
    
    # æ‰“å°æ€»ç»“
    print("\nç®—æ³•æ€§èƒ½æ€»ç»“ Algorithm Performance Summary:")
    print("=" * 80)
    print(f"{'Algorithm':25s} {'Final Reward':>15s} {'Optimal %':>15s} {'Total Regret':>15s}")
    print("-" * 80)
    for name, metrics in final_metrics.items():
        print(f"{name:25s} {metrics['reward']:>15.3f} {metrics['optimal']:>14.1f}% {metrics['regret']:>15.1f}")
    
    print("\nå…³é”®ç»“è®º Key Conclusions:")
    print("-" * 60)
    print("1. UCB: æœ€ä½³ç†è®ºä¿è¯ï¼Œç¨³å®šæ€§èƒ½")
    print("2. Îµ-Greedy: ç®€å•æœ‰æ•ˆï¼Œæ˜“äºå®ç°")
    print("3. Gradient Bandit: é€‚åˆéœ€è¦éšæœºç­–ç•¥çš„åœºæ™¯")
    print("4. åŸºçº¿å¯¹æ¢¯åº¦æ–¹æ³•è‡³å…³é‡è¦")
    print("5. ç†µæ­£åˆ™åŒ–æœ‰åŠ©äºæŒç»­æ¢ç´¢")
    
    return fig


# ================================================================================
# ä¸»å‡½æ•°
# Main Function
# ================================================================================

def main():
    """
    è¿è¡Œæ¢¯åº¦èµŒåšæœºå®Œæ•´æ¼”ç¤º
    Run complete gradient bandit demo
    """
    print("\n" + "="*80)
    print("ç¬¬1.8èŠ‚ï¼šæ¢¯åº¦èµŒåšæœºç®—æ³•")
    print("Section 1.8: Gradient Bandit Algorithm")
    print("="*80)
    
    # 1. åŸç†è§£é‡Š
    fig1 = GradientBanditPrinciple.explain_principle()
    
    # 2. ç†è®ºåˆ†æ
    GradientBanditAnalysis.convergence_analysis()
    fig2 = GradientBanditAnalysis.demonstrate_convergence()
    
    # 3. ç»¼åˆæ¯”è¾ƒ
    fig3 = compare_all_algorithms()
    
    print("\n" + "="*80)
    print("æ¢¯åº¦èµŒåšæœºæ¼”ç¤ºå®Œæˆï¼")
    print("Gradient Bandit Demo Complete!")
    print("\nè¿™æ ‡å¿—ç€ç¬¬1ç« çš„ç»“æŸï¼")
    print("This marks the end of Chapter 1!")
    print("="*80)
    
    plt.show()
    
    return [fig1, fig2, fig3]


if __name__ == "__main__":
    main()