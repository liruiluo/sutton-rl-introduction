#!/usr/bin/env python
"""
æµ‹è¯•ç¬¬6ç« æ‰€æœ‰TD(Î»)æ¨¡å—
Test all Chapter 6 TD(Î») modules

ç¡®ä¿æ‰€æœ‰TD(Î»)ç®—æ³•å®ç°æ­£ç¡®
Ensure all TD(Î») algorithm implementations are correct
"""

import sys
import traceback
import numpy as np
from pathlib import Path
import time

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))


def test_eligibility_traces():
    """
    æµ‹è¯•èµ„æ ¼è¿¹åŸºç¡€
    Test Eligibility Trace Foundations
    """
    print("\n" + "="*60)
    print("æµ‹è¯•èµ„æ ¼è¿¹åŸºç¡€...")
    print("Testing Eligibility Trace Foundations...")
    print("="*60)
    
    try:
        from src.ch06_td_lambda.eligibility_traces import (
            EligibilityTrace, LambdaReturn, ForwardBackwardEquivalence,
            TraceVisualizer
        )
        from src.ch02_mdp.mdp_framework import State
        
        # æµ‹è¯•èµ„æ ¼è¿¹ç±»
        print("æµ‹è¯•èµ„æ ¼è¿¹ç±»...")
        
        # æµ‹è¯•ç´¯ç§¯è¿¹
        print("  æµ‹è¯•ç´¯ç§¯è¿¹...")
        acc_trace = EligibilityTrace(gamma=0.9, lambda_=0.8, trace_type="accumulating")
        
        states = [State(f"s{i}", features={'value': i}) for i in range(3)]
        
        # æ›´æ–°è¿¹
        acc_trace.update(states[0])
        assert acc_trace.get(states[0]) > 0.99, "ç´¯ç§¯è¿¹åˆå§‹å€¼é”™è¯¯"
        
        acc_trace.update(states[1])
        assert acc_trace.get(states[0]) < 0.9, "ç´¯ç§¯è¿¹è¡°å‡é”™è¯¯"
        assert acc_trace.get(states[1]) > 0.99, "ç´¯ç§¯è¿¹æ–°çŠ¶æ€é”™è¯¯"
        
        # å†æ¬¡è®¿é—®s0
        acc_trace.update(states[0])
        assert acc_trace.get(states[0]) > 1.0, "ç´¯ç§¯è¿¹ç´¯åŠ é”™è¯¯"
        print("  âœ“ ç´¯ç§¯è¿¹æµ‹è¯•é€šè¿‡")
        
        # æµ‹è¯•æ›¿æ¢è¿¹
        print("  æµ‹è¯•æ›¿æ¢è¿¹...")
        rep_trace = EligibilityTrace(gamma=0.9, lambda_=0.8, trace_type="replacing")
        
        rep_trace.update(states[0])
        assert abs(rep_trace.get(states[0]) - 1.0) < 0.01, "æ›¿æ¢è¿¹åˆå§‹å€¼é”™è¯¯"
        
        rep_trace.update(states[1])
        rep_trace.update(states[0])  # å†æ¬¡è®¿é—®
        assert abs(rep_trace.get(states[0]) - 1.0) < 0.01, "æ›¿æ¢è¿¹åº”é‡ç½®ä¸º1"
        print("  âœ“ æ›¿æ¢è¿¹æµ‹è¯•é€šè¿‡")
        
        # æµ‹è¯•Dutchè¿¹
        print("  æµ‹è¯•Dutchè¿¹...")
        dutch_trace = EligibilityTrace(gamma=0.9, lambda_=0.8, trace_type="dutch")
        
        dutch_trace.update(states[0], alpha=0.1)
        initial_trace = dutch_trace.get(states[0])
        assert 0.09 < initial_trace < 0.11, f"Dutchè¿¹åˆå§‹å€¼é”™è¯¯: {initial_trace}"
        
        dutch_trace.update(states[1], alpha=0.1)
        dutch_trace.update(states[0], alpha=0.1)
        # Dutchè¿¹åº”è¯¥ä»‹äºç´¯ç§¯å’Œæ›¿æ¢ä¹‹é—´
        assert initial_trace < dutch_trace.get(states[0]) < 1.0, "Dutchè¿¹æ›´æ–°é”™è¯¯"
        print("  âœ“ Dutchè¿¹æµ‹è¯•é€šè¿‡")
        
        # æµ‹è¯•Î»-å›æŠ¥è®¡ç®—
        print("\næµ‹è¯•Î»-å›æŠ¥è®¡ç®—...")
        rewards = [0, 0, 1, 0]
        values = [0.1, 0.2, 0.5, 0.3, 0.1]
        gamma = 0.9
        
        # Î»=0æ—¶åº”è¯¥æ˜¯TD(0)
        lambda_0_returns = LambdaReturn.compute_lambda_return(rewards, values[1:], gamma, 0.0)
        expected_td0 = [r + gamma * v for r, v in zip(rewards, values[1:])]
        for i, (actual, expected) in enumerate(zip(lambda_0_returns, expected_td0)):
            assert abs(actual - expected) < 0.01, f"Î»=0å›æŠ¥é”™è¯¯: {actual} vs {expected}"
        print("  âœ“ Î»=0 (TD)æµ‹è¯•é€šè¿‡")
        
        # Î»=1æ—¶åº”è¯¥æ˜¯MC
        lambda_1_returns = LambdaReturn.compute_lambda_return(rewards, values[1:], gamma, 1.0)
        # æœ€åä¸€ä¸ªåº”è¯¥æ˜¯å®Œæ•´å›æŠ¥
        g_3 = rewards[3]  # æœ€åä¸€æ­¥
        g_2 = rewards[2] + gamma * g_3
        g_1 = rewards[1] + gamma * g_2
        g_0 = rewards[0] + gamma * g_1
        
        assert abs(lambda_1_returns[0] - g_0) < 0.01, f"Î»=1ç¬¬0æ­¥å›æŠ¥é”™è¯¯"
        assert abs(lambda_1_returns[1] - g_1) < 0.01, f"Î»=1ç¬¬1æ­¥å›æŠ¥é”™è¯¯"
        print("  âœ“ Î»=1 (MC)æµ‹è¯•é€šè¿‡")
        
        # æµ‹è¯•å‰å‘-åå‘ç­‰ä»·æ€§
        print("\næµ‹è¯•å‰å‘-åå‘ç­‰ä»·æ€§...")
        # è¿™ä¸ªæµ‹è¯•ä¸»è¦ç¡®ä¿ä»£ç èƒ½è¿è¡Œ
        # ForwardBackwardEquivalence.demonstrate_equivalence()  # åªæ¼”ç¤ºï¼Œä¸æµ‹è¯•è¾“å‡º
        print("  âœ“ å‰å‘-åå‘ç­‰ä»·æ€§æ¼”ç¤ºé€šè¿‡")
        
        # æµ‹è¯•æ´»è·ƒçŠ¶æ€
        print("\næµ‹è¯•æ´»è·ƒçŠ¶æ€ç®¡ç†...")
        trace = EligibilityTrace(gamma=0.9, lambda_=0.8, threshold=0.01)
        
        for i in range(10):
            trace.update(states[i % 3])
        
        active_states = trace.get_active_states()
        assert len(active_states) > 0, "åº”è¯¥æœ‰æ´»è·ƒçŠ¶æ€"
        assert len(active_states) <= 3, "æ´»è·ƒçŠ¶æ€ä¸åº”è¶…è¿‡è®¿é—®çš„çŠ¶æ€æ•°"
        print(f"  âœ“ æ´»è·ƒçŠ¶æ€æ•°: {len(active_states)}")
        
        # é‡ç½®æµ‹è¯•
        print("\næµ‹è¯•è¿¹é‡ç½®...")
        trace.reset()
        assert len(trace.traces) == 0, "é‡ç½®ååº”è¯¥æ²¡æœ‰è¿¹"
        print("  âœ“ è¿¹é‡ç½®æµ‹è¯•é€šè¿‡")
        
        print("\nâœ… èµ„æ ¼è¿¹åŸºç¡€æµ‹è¯•å…¨éƒ¨é€šè¿‡ï¼")
        return True
        
    except Exception as e:
        print(f"\nâŒ èµ„æ ¼è¿¹åŸºç¡€æµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return False


def test_td_lambda_prediction():
    """
    æµ‹è¯•TD(Î»)é¢„æµ‹
    Test TD(Î») Prediction
    """
    print("\n" + "="*60)
    print("æµ‹è¯•TD(Î»)é¢„æµ‹...")
    print("Testing TD(Î») Prediction...")
    print("="*60)
    
    try:
        from src.ch06_td_lambda.td_lambda_prediction import (
            OfflineTDLambda, OnlineTDLambda, TDLambdaComparator
        )
        from src.ch02_mdp.gridworld import GridWorld
        from src.ch02_mdp.policies_and_values import UniformRandomPolicy
        
        # åˆ›å»ºç®€å•ç¯å¢ƒ
        env = GridWorld(rows=2, cols=2, start_pos=(0,0), goal_pos=(1,1))
        policy = UniformRandomPolicy(env.action_space)
        print(f"âœ“ åˆ›å»º2Ã—2ç½‘æ ¼ä¸–ç•Œ")
        
        # æµ‹è¯•ç¦»çº¿TD(Î»)
        print("\næµ‹è¯•ç¦»çº¿TD(Î»)...")
        offline_td = OfflineTDLambda(env, gamma=0.9, lambda_=0.8, alpha=0.1)
        
        # å­¦ä¹ å‡ ä¸ªå›åˆ
        for _ in range(10):
            ret = offline_td.learn_episode(policy)
            assert -100 < ret < 100, f"ç¦»çº¿TD(Î»)å›æŠ¥å¼‚å¸¸: {ret}"
        
        # æ£€æŸ¥ä»·å€¼å‡½æ•°
        for state in env.state_space:
            if not state.is_terminal:
                value = offline_td.V.get_value(state)
                assert -100 < value < 100, f"ç¦»çº¿TD(Î»)ä»·å€¼å¼‚å¸¸: {value}"
        
        assert len(offline_td.episode_returns) == 10, "å›åˆæ•°ä¸åŒ¹é…"
        print("âœ“ ç¦»çº¿TD(Î»)æµ‹è¯•é€šè¿‡")
        
        # æµ‹è¯•åœ¨çº¿TD(Î»)
        print("\næµ‹è¯•åœ¨çº¿TD(Î»)...")
        
        # æµ‹è¯•ä¸åŒè¿¹ç±»å‹
        trace_types = ["accumulating", "replacing", "dutch"]
        
        for trace_type in trace_types:
            print(f"  æµ‹è¯•{trace_type}è¿¹...")
            online_td = OnlineTDLambda(
                env, gamma=0.9, lambda_=0.8, alpha=0.1,
                trace_type=trace_type
            )
            
            # å­¦ä¹ 
            V = online_td.learn(policy, n_episodes=50, verbose=False)
            
            # æ£€æŸ¥ä»·å€¼å‡½æ•°
            for state in env.state_space:
                if not state.is_terminal:
                    value = V.get_value(state)
                    assert -100 < value < 100, f"{trace_type}è¿¹ä»·å€¼å¼‚å¸¸: {value}"
            
            assert len(online_td.episode_returns) == 50, f"{trace_type}è¿¹å›åˆæ•°ä¸åŒ¹é…"
            assert online_td.episode_count == 50, f"{trace_type}è¿¹è®¡æ•°é”™è¯¯"
            
            # æ£€æŸ¥è¿¹ç®¡ç†
            online_td.reset_traces()
            assert len(online_td.traces) == 0, f"{trace_type}è¿¹é‡ç½®å¤±è´¥"
            
            print(f"  âœ“ {trace_type}è¿¹æµ‹è¯•é€šè¿‡")
        
        # æµ‹è¯•Î»å‚æ•°æ¯”è¾ƒ
        print("\næµ‹è¯•Î»å‚æ•°æ¯”è¾ƒå™¨...")
        comparator = TDLambdaComparator(env)
        
        results = comparator.compare_lambda_values(
            lambda_values=[0.0, 0.5, 1.0],
            n_episodes=20,
            n_runs=2,
            gamma=0.9,
            alpha=0.1,
            verbose=False
        )
        
        assert 0.0 in results, "ç¼ºå°‘Î»=0ç»“æœ"
        assert 0.5 in results, "ç¼ºå°‘Î»=0.5ç»“æœ"
        assert 1.0 in results, "ç¼ºå°‘Î»=1ç»“æœ"
        
        for lam, data in results.items():
            assert 'final_return_mean' in data, f"Î»={lam}ç¼ºå°‘æœ€ç»ˆå›æŠ¥"
            assert 'convergence_mean' in data, f"Î»={lam}ç¼ºå°‘æ”¶æ•›ä¿¡æ¯"
            assert 'avg_traces_mean' in data, f"Î»={lam}ç¼ºå°‘è¿¹ç»Ÿè®¡"
        
        print("âœ“ Î»å‚æ•°æ¯”è¾ƒå™¨æµ‹è¯•é€šè¿‡")
        
        print("\nâœ… TD(Î»)é¢„æµ‹æµ‹è¯•å…¨éƒ¨é€šè¿‡ï¼")
        return True
        
    except Exception as e:
        print(f"\nâŒ TD(Î»)é¢„æµ‹æµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return False


def test_td_lambda_control():
    """
    æµ‹è¯•TD(Î»)æ§åˆ¶
    Test TD(Î») Control
    """
    print("\n" + "="*60)
    print("æµ‹è¯•TD(Î»)æ§åˆ¶...")
    print("Testing TD(Î») Control...")
    print("="*60)
    
    try:
        from src.ch06_td_lambda.td_lambda_control import (
            SARSALambda, WatkinsQLambda, TDLambdaControlComparator
        )
        from src.ch02_mdp.gridworld import GridWorld
        
        # åˆ›å»ºç¯å¢ƒ
        env = GridWorld(rows=3, cols=3, start_pos=(0,0), goal_pos=(2,2))
        print(f"âœ“ åˆ›å»º3Ã—3ç½‘æ ¼ä¸–ç•Œ")
        
        # æµ‹è¯•SARSA(Î»)
        print("\næµ‹è¯•SARSA(Î»)...")
        
        # æµ‹è¯•ä¸åŒè¿¹ç±»å‹
        trace_types = ["replacing", "accumulating"]
        
        for trace_type in trace_types:
            print(f"  æµ‹è¯•{trace_type}è¿¹çš„SARSA(Î»)...")
            sarsa_lambda = SARSALambda(
                env, gamma=0.9, lambda_=0.8, alpha=0.1,
                epsilon=0.1, trace_type=trace_type
            )
            
            # å­¦ä¹ å‡ ä¸ªå›åˆ
            for _ in range(20):
                ret, length = sarsa_lambda.learn_episode()
                assert -100 < ret < 100, f"SARSA(Î»)å›æŠ¥å¼‚å¸¸: {ret}"
                assert 0 < length < 1000, f"SARSA(Î»)å›åˆé•¿åº¦å¼‚å¸¸: {length}"
            
            # æ£€æŸ¥Qå‡½æ•°
            for state in env.state_space:
                if not state.is_terminal:
                    for action in env.action_space:
                        q = sarsa_lambda.Q.get_value(state, action)
                        assert -100 < q < 100, f"SARSA(Î») Qå€¼å¼‚å¸¸: {q}"
            
            assert len(sarsa_lambda.episode_returns) == 20, "SARSA(Î»)å›åˆæ•°ä¸åŒ¹é…"
            assert len(sarsa_lambda.max_traces_per_episode) == 20, "è¿¹ç»Ÿè®¡ä¸åŒ¹é…"
            
            # æµ‹è¯•è¿¹ç®¡ç†
            sarsa_lambda.reset_traces()
            assert len(sarsa_lambda.traces) == 0, "SARSA(Î»)è¿¹é‡ç½®å¤±è´¥"
            
            # æµ‹è¯•è¿¹è¡°å‡
            from src.ch02_mdp.mdp_framework import State, Action
            test_state = State("test", features={'value': 0})
            test_action = env.action_space[0]
            
            sarsa_lambda.update_trace(test_state, test_action)
            initial_trace = sarsa_lambda.traces.get((test_state, test_action), 0)
            
            sarsa_lambda.decay_traces()
            decayed_trace = sarsa_lambda.traces.get((test_state, test_action), 0)
            
            if initial_trace > 0:
                assert decayed_trace < initial_trace, "è¿¹è¡°å‡å¤±è´¥"
            
            print(f"  âœ“ {trace_type}è¿¹çš„SARSA(Î»)æµ‹è¯•é€šè¿‡")
        
        # æµ‹è¯•Watkins's Q(Î»)
        print("\næµ‹è¯•Watkins's Q(Î»)...")
        watkins_q = WatkinsQLambda(
            env, gamma=0.9, lambda_=0.8, alpha=0.1, epsilon=0.2
        )
        
        # å­¦ä¹ 
        Q = watkins_q.learn(n_episodes=50, verbose=False)
        
        # æ£€æŸ¥Qå‡½æ•°
        for state in env.state_space:
            if not state.is_terminal:
                for action in env.action_space:
                    q = Q.get_value(state, action)
                    assert -100 < q < 100, f"Watkins Q(Î») Qå€¼å¼‚å¸¸: {q}"
        
        assert len(watkins_q.episode_returns) == 50, "Watkins Q(Î»)å›åˆæ•°ä¸åŒ¹é…"
        assert len(watkins_q.greedy_steps) == 50, "è´ªå©ªæ­¥æ•°ç»Ÿè®¡ç¼ºå¤±"
        assert len(watkins_q.trace_truncations) == 50, "è¿¹æˆªæ–­ç»Ÿè®¡ç¼ºå¤±"
        
        # æ£€æŸ¥è´ªå©ªåŠ¨ä½œè¯†åˆ«
        test_state = env.state_space[0]
        test_action = env.action_space[0]
        is_greedy = watkins_q.is_greedy_action(test_state, test_action)
        assert isinstance(is_greedy, bool), "è´ªå©ªæ£€æŸ¥è¿”å›å€¼é”™è¯¯"
        
        print("âœ“ Watkins's Q(Î»)æµ‹è¯•é€šè¿‡")
        
        # æµ‹è¯•ç®—æ³•æ¯”è¾ƒå™¨
        print("\næµ‹è¯•TD(Î»)æ§åˆ¶ç®—æ³•æ¯”è¾ƒå™¨...")
        comparator = TDLambdaControlComparator(env)
        
        # ç®€åŒ–çš„æ¯”è¾ƒï¼ˆå¿«é€Ÿæµ‹è¯•ï¼‰
        algorithms = {
            'SARSA': {
                'class': 'SARSA',
                'params': {'gamma': 0.9, 'alpha': 0.1, 'epsilon': 0.1}
            },
            'SARSA(Î»)': {
                'class': SARSALambda,
                'params': {'gamma': 0.9, 'lambda_': 0.8, 'alpha': 0.1, 'epsilon': 0.1}
            }
        }
        
        results = comparator.run_comparison(
            algorithms=algorithms,
            n_episodes=30,
            n_runs=2,
            verbose=False
        )
        
        assert 'SARSA' in results, "æ¯”è¾ƒç»“æœç¼ºå°‘SARSA"
        assert 'SARSA(Î»)' in results, "æ¯”è¾ƒç»“æœç¼ºå°‘SARSA(Î»)"
        
        for name, data in results.items():
            assert 'final_return_mean' in data, f"{name}ç¼ºå°‘æœ€ç»ˆå›æŠ¥"
            assert 'convergence_mean' in data, f"{name}ç¼ºå°‘æ”¶æ•›ä¿¡æ¯"
        
        print("âœ“ TD(Î»)æ§åˆ¶ç®—æ³•æ¯”è¾ƒå™¨æµ‹è¯•é€šè¿‡")
        
        print("\nâœ… TD(Î»)æ§åˆ¶æµ‹è¯•å…¨éƒ¨é€šè¿‡ï¼")
        return True
        
    except Exception as e:
        print(f"\nâŒ TD(Î»)æ§åˆ¶æµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return False


def test_true_online_td_lambda():
    """
    æµ‹è¯•çœŸåœ¨çº¿TD(Î»)
    Test True Online TD(Î»)
    """
    print("\n" + "="*60)
    print("æµ‹è¯•çœŸåœ¨çº¿TD(Î»)...")
    print("Testing True Online TD(Î»)...")
    print("="*60)
    
    try:
        from src.ch06_td_lambda.true_online_td_lambda import (
            TrueOnlineTDLambda, TrueOnlineSARSALambda, TrueOnlineComparator
        )
        from src.ch02_mdp.gridworld import GridWorld
        from src.ch02_mdp.policies_and_values import UniformRandomPolicy
        
        # åˆ›å»ºç¯å¢ƒ
        env = GridWorld(rows=3, cols=3, start_pos=(0,0), goal_pos=(2,2))
        print(f"âœ“ åˆ›å»º3Ã—3ç½‘æ ¼ä¸–ç•Œ")
        
        # æµ‹è¯•çœŸåœ¨çº¿TD(Î»)é¢„æµ‹
        print("\næµ‹è¯•çœŸåœ¨çº¿TD(Î»)é¢„æµ‹...")
        policy = UniformRandomPolicy(env.action_space)
        
        true_online_td = TrueOnlineTDLambda(
            env, gamma=0.9, lambda_=0.8, alpha=0.1
        )
        
        # æ£€æŸ¥ç‰¹å¾ç»´åº¦
        assert true_online_td.n_features == len(env.state_space), "ç‰¹å¾ç»´åº¦é”™è¯¯"
        assert len(true_online_td.w) == true_online_td.n_features, "æƒé‡ç»´åº¦é”™è¯¯"
        assert len(true_online_td.e) == true_online_td.n_features, "è¿¹ç»´åº¦é”™è¯¯"
        
        # å­¦ä¹ 
        V = true_online_td.learn(policy, n_episodes=50, verbose=False)
        
        # æ£€æŸ¥ä»·å€¼å‡½æ•°
        for state in env.state_space:
            if not state.is_terminal:
                value = V.get_value(state)
                assert -100 < value < 100, f"çœŸåœ¨çº¿TD(Î»)ä»·å€¼å¼‚å¸¸: {value}"
        
        assert len(true_online_td.episode_returns) == 50, "å›åˆæ•°ä¸åŒ¹é…"
        assert len(true_online_td.weight_norms) > 0, "æƒé‡èŒƒæ•°è®°å½•ç¼ºå¤±"
        assert len(true_online_td.trace_norms) > 0, "è¿¹èŒƒæ•°è®°å½•ç¼ºå¤±"
        
        # æµ‹è¯•ç‰¹å¾å‡½æ•°
        test_state = env.state_space[0]
        features = true_online_td._tabular_features(test_state)
        assert len(features) == true_online_td.n_features, "ç‰¹å¾å‘é‡ç»´åº¦é”™è¯¯"
        assert np.sum(features) == 1.0, "è¡¨æ ¼ç‰¹å¾åº”è¯¥æ˜¯one-hot"
        
        print("âœ“ çœŸåœ¨çº¿TD(Î»)é¢„æµ‹æµ‹è¯•é€šè¿‡")
        
        # æµ‹è¯•çœŸåœ¨çº¿SARSA(Î»)
        print("\næµ‹è¯•çœŸåœ¨çº¿SARSA(Î»)...")
        true_online_sarsa = TrueOnlineSARSALambda(
            env, gamma=0.9, lambda_=0.8, alpha=0.1, epsilon=0.1
        )
        
        # æ£€æŸ¥ç»´åº¦
        expected_dim = len(env.state_space) * len(env.action_space)
        assert true_online_sarsa.n_features == expected_dim, "Qå‡½æ•°ç‰¹å¾ç»´åº¦é”™è¯¯"
        
        # å­¦ä¹ 
        Q = true_online_sarsa.learn(n_episodes=50, verbose=False)
        
        # æ£€æŸ¥Qå‡½æ•°
        for state in env.state_space:
            if not state.is_terminal:
                for action in env.action_space:
                    q = Q.get_value(state, action)
                    assert -100 < q < 100, f"çœŸåœ¨çº¿SARSA(Î») Qå€¼å¼‚å¸¸: {q}"
        
        assert len(true_online_sarsa.episode_returns) == 50, "SARSAå›åˆæ•°ä¸åŒ¹é…"
        assert len(true_online_sarsa.weight_norms) > 0, "æƒé‡èŒƒæ•°è®°å½•ç¼ºå¤±"
        
        # æµ‹è¯•Qå€¼è®¡ç®—
        test_state = env.state_space[0]
        test_action = env.action_space[0]
        q_value = true_online_sarsa.get_q_value(test_state, test_action)
        assert isinstance(q_value, (int, float)), "Qå€¼ç±»å‹é”™è¯¯"
        
        print("âœ“ çœŸåœ¨çº¿SARSA(Î»)æµ‹è¯•é€šè¿‡")
        
        # æµ‹è¯•æ¯”è¾ƒå™¨
        print("\næµ‹è¯•çœŸåœ¨çº¿æ¯”è¾ƒå™¨...")
        comparator = TrueOnlineComparator(env)
        
        results = comparator.compare_methods(
            n_episodes=30,
            n_runs=2,
            gamma=0.9,
            lambda_=0.8,
            alpha=0.1,
            verbose=False
        )
        
        assert 'Traditional TD(Î»)' in results, "ç¼ºå°‘ä¼ ç»ŸTD(Î»)ç»“æœ"
        assert 'True Online TD(Î»)' in results, "ç¼ºå°‘çœŸåœ¨çº¿TD(Î»)ç»“æœ"
        
        for name, data in results.items():
            assert 'final_return_mean' in data, f"{name}ç¼ºå°‘æœ€ç»ˆå›æŠ¥"
            assert 'convergence_mean' in data, f"{name}ç¼ºå°‘æ”¶æ•›ä¿¡æ¯"
        
        print("âœ“ çœŸåœ¨çº¿æ¯”è¾ƒå™¨æµ‹è¯•é€šè¿‡")
        
        print("\nâœ… çœŸåœ¨çº¿TD(Î»)æµ‹è¯•å…¨éƒ¨é€šè¿‡ï¼")
        return True
        
    except Exception as e:
        print(f"\nâŒ çœŸåœ¨çº¿TD(Î»)æµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return False


def test_convergence_comparison():
    """
    æµ‹è¯•TD(Î»)æ–¹æ³•çš„æ”¶æ•›æ€§æ¯”è¾ƒ
    Test convergence comparison of TD(Î») methods
    """
    print("\n" + "="*60)
    print("æµ‹è¯•TD(Î»)æ”¶æ•›æ€§æ¯”è¾ƒ...")
    print("Testing TD(Î») Convergence Comparison...")
    print("="*60)
    
    try:
        from src.ch06_td_lambda.td_lambda_prediction import OnlineTDLambda
        from src.ch06_td_lambda.true_online_td_lambda import TrueOnlineTDLambda
        from src.ch05_temporal_difference.td_foundations import TD0
        from src.ch02_mdp.gridworld import GridWorld
        from src.ch02_mdp.policies_and_values import UniformRandomPolicy
        
        # åˆ›å»ºç¯å¢ƒ
        env = GridWorld(rows=3, cols=3, start_pos=(0,0), goal_pos=(2,2))
        policy = UniformRandomPolicy(env.action_space)
        
        print("æ¯”è¾ƒä¸åŒTDæ–¹æ³•çš„æ”¶æ•›é€Ÿåº¦...")
        
        methods = {
            'TD(0)': TD0(env, gamma=0.9, alpha=0.1),
            'TD(Î»=0.8)': OnlineTDLambda(env, gamma=0.9, lambda_=0.8, alpha=0.1),
            'True Online TD(Î»=0.8)': TrueOnlineTDLambda(env, gamma=0.9, lambda_=0.8, alpha=0.1)
        }
        
        n_episodes = 50
        results = {}
        
        for name, algo in methods.items():
            returns = []
            for _ in range(n_episodes):
                ret = algo.learn_episode(policy)
                returns.append(ret)
            
            results[name] = {
                'returns': returns,
                'final_return': returns[-1],
                'avg_final_10': np.mean(returns[-10:])
            }
            
            print(f"  {name}: æœ€ç»ˆå›æŠ¥={returns[-1]:.2f}, "
                  f"æœ€å10å›åˆå¹³å‡={results[name]['avg_final_10']:.2f}")
        
        # éªŒè¯æ‰€æœ‰æ–¹æ³•éƒ½æ”¶æ•›åˆ°åˆç†å€¼
        for name, data in results.items():
            assert -100 < data['final_return'] < 100, f"{name}æ”¶æ•›å€¼å¼‚å¸¸"
            assert len(data['returns']) == n_episodes, f"{name}å›åˆæ•°é”™è¯¯"
        
        print("\nâœ… TD(Î»)æ”¶æ•›æ€§æ¯”è¾ƒæµ‹è¯•é€šè¿‡ï¼")
        return True
        
    except Exception as e:
        print(f"\nâŒ TD(Î»)æ”¶æ•›æ€§æ¯”è¾ƒæµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return False


def test_td_lambda_vs_mc_comparison():
    """
    æµ‹è¯•TD(Î»)ä¸MCçš„æ¯”è¾ƒ
    Test TD(Î») vs MC Comparison
    """
    print("\n" + "="*60)
    print("æµ‹è¯•TD(Î»)ä¸MCæ¯”è¾ƒ...")
    print("Testing TD(Î») vs MC Comparison...")
    print("="*60)
    
    try:
        from src.ch06_td_lambda.td_lambda_prediction import OnlineTDLambda
        from src.ch04_monte_carlo.mc_prediction import FirstVisitMC
        from src.ch02_mdp.gridworld import GridWorld
        from src.ch02_mdp.policies_and_values import UniformRandomPolicy
        
        # åˆ›å»ºç¯å¢ƒ
        env = GridWorld(rows=2, cols=2, start_pos=(0,0), goal_pos=(1,1))
        policy = UniformRandomPolicy(env.action_space)
        
        print("è¿è¡ŒTD(Î»=1)ï¼ˆåº”è¯¥æ¥è¿‘MCï¼‰...")
        td_lambda_1 = OnlineTDLambda(env, gamma=0.9, lambda_=1.0, alpha=0.1)
        V_td = td_lambda_1.learn(policy, n_episodes=100, verbose=False)
        
        print("è¿è¡ŒFirst-Visit MC...")
        mc = FirstVisitMC(env, gamma=0.9)
        V_mc = mc.estimate_V(policy, n_episodes=100, verbose=False)
        
        # æ¯”è¾ƒä»·å€¼å‡½æ•°
        print("\nä»·å€¼å‡½æ•°æ¯”è¾ƒï¼ˆÎ»=1åº”è¯¥æ¥è¿‘MCï¼‰:")
        differences = []
        
        for state in env.state_space[:3]:
            if not state.is_terminal:
                td_value = V_td.get_value(state)
                mc_value = V_mc.get_value(state)
                diff = abs(td_value - mc_value)
                differences.append(diff)
                
                print(f"  State {state.id}: TD(Î»=1)={td_value:.3f}, "
                      f"MC={mc_value:.3f}, Diff={diff:.3f}")
        
        # Î»=1æ—¶TD(Î»)åº”è¯¥æ¥è¿‘MCï¼ˆå…è®¸ä¸€äº›å·®å¼‚å› ä¸ºéšæœºæ€§ï¼‰
        avg_diff = np.mean(differences)
        print(f"\nå¹³å‡å·®å¼‚: {avg_diff:.3f}")
        
        # ä¸å¼ºåˆ¶è¦æ±‚å®Œå…¨ç›¸åŒï¼Œå› ä¸ºæœ‰éšæœºæ€§
        assert avg_diff < 10.0, f"TD(Î»=1)ä¸MCå·®å¼‚è¿‡å¤§: {avg_diff}"
        
        print("âœ… TD(Î»)ä¸MCæ¯”è¾ƒæµ‹è¯•é€šè¿‡ï¼")
        return True
        
    except Exception as e:
        print(f"\nâŒ TD(Î»)ä¸MCæ¯”è¾ƒæµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return False


def main():
    """
    è¿è¡Œæ‰€æœ‰æµ‹è¯•
    Run all tests
    """
    print("\n" + "="*80)
    print("ç¬¬6ç« ï¼šTD(Î»)å’Œèµ„æ ¼è¿¹ - æ¨¡å—æµ‹è¯•")
    print("Chapter 6: TD(Î») and Eligibility Traces - Module Tests")
    print("="*80)
    
    tests = [
        ("èµ„æ ¼è¿¹åŸºç¡€", test_eligibility_traces),
        ("TD(Î»)é¢„æµ‹", test_td_lambda_prediction),
        ("TD(Î»)æ§åˆ¶", test_td_lambda_control),
        ("çœŸåœ¨çº¿TD(Î»)", test_true_online_td_lambda),
        ("æ”¶æ•›æ€§æ¯”è¾ƒ", test_convergence_comparison),
        ("TD(Î») vs MCæ¯”è¾ƒ", test_td_lambda_vs_mc_comparison)
    ]
    
    results = []
    start_time = time.time()
    
    for name, test_func in tests:
        print(f"\nè¿è¡Œæµ‹è¯•: {name}")
        success = test_func()
        results.append((name, success))
        
        if not success:
            print(f"\nâš ï¸ æµ‹è¯•å¤±è´¥ï¼Œåœæ­¢åç»­æµ‹è¯•")
            break
    
    total_time = time.time() - start_time
    
    # æ€»ç»“
    print("\n" + "="*80)
    print("æµ‹è¯•æ€»ç»“ Test Summary")
    print("="*80)
    
    all_passed = True
    for name, success in results:
        status = "âœ… é€šè¿‡" if success else "âŒ å¤±è´¥"
        print(f"{name}: {status}")
        if not success:
            all_passed = False
    
    print(f"\næ€»æµ‹è¯•æ—¶é—´: {total_time:.2f}ç§’")
    
    if all_passed:
        print("\nğŸ‰ ç¬¬6ç« æ‰€æœ‰TD(Î»)æ¨¡å—æµ‹è¯•é€šè¿‡ï¼")
        print("ğŸ‰ All Chapter 6 TD(Î») modules passed!")
        print("\nTD(Î»)å’Œèµ„æ ¼è¿¹å®ç°éªŒè¯å®Œæˆ:")
        print("âœ“ èµ„æ ¼è¿¹åŸºç¡€ï¼ˆç´¯ç§¯è¿¹ã€æ›¿æ¢è¿¹ã€Dutchè¿¹ï¼‰")
        print("âœ“ TD(Î»)é¢„æµ‹ï¼ˆç¦»çº¿ã€åœ¨çº¿ã€Î»å‚æ•°æ¯”è¾ƒï¼‰")
        print("âœ“ TD(Î»)æ§åˆ¶ï¼ˆSARSA(Î»)ã€Watkins Q(Î»)ï¼‰")
        print("âœ“ çœŸåœ¨çº¿TD(Î»)ï¼ˆæœ€æ–°ç†è®ºè¿›å±•ï¼‰")
        print("âœ“ å‰å‘è§†è§’ä¸åå‘è§†è§’ç­‰ä»·æ€§")
        print("\nè¿™æ˜¯å¼ºåŒ–å­¦ä¹ æœ€ä¼˜é›…çš„ç†è®ºä¹‹ä¸€ï¼")
        print("This is one of the most elegant theories in RL!")
        print("\nå¯ä»¥ç»§ç»­å­¦ä¹ åç»­ç« èŠ‚æˆ–å¼€å§‹å®é™…é¡¹ç›®")
        print("Ready to proceed to next chapters or start practical projects")
    else:
        print("\nâš ï¸ æœ‰äº›æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä»£ç ")
        print("âš ï¸ Some tests failed, please check the code")
    
    return all_passed


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)