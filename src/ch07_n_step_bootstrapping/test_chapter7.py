#!/usr/bin/env python
"""
æµ‹è¯•ç¬¬7ç« æ‰€æœ‰næ­¥è‡ªä¸¾æ–¹æ³•æ¨¡å—
Test all Chapter 7 n-step Bootstrapping modules

ç¡®ä¿æ‰€æœ‰næ­¥ç®—æ³•å®ç°æ­£ç¡®
Ensure all n-step algorithm implementations are correct
"""

import sys
import traceback
import numpy as np
from pathlib import Path
import time

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))


def test_n_step_td():
    """
    æµ‹è¯•næ­¥TDé¢„æµ‹
    Test n-step TD Prediction
    """
    print("\n" + "="*60)
    print("æµ‹è¯•næ­¥TDé¢„æµ‹...")
    print("Testing n-step TD Prediction...")
    print("="*60)
    
    try:
        from src.ch07_n_step_bootstrapping.n_step_td import (
            NStepReturn, NStepBuffer, NStepTD, NStepTDComparator,
            NStepExperience
        )
        from src.ch02_mdp.mdp_framework import State
        from src.ch02_mdp.gridworld import GridWorld
        from src.ch02_mdp.policies_and_values import UniformRandomPolicy
        
        # æµ‹è¯•næ­¥å›æŠ¥è®¡ç®—
        print("æµ‹è¯•næ­¥å›æŠ¥è®¡ç®—...")
        rewards = [0, 0, 1, 0, 0]
        values = [0.1, 0.2, 0.5, 0.3, 0.1, 0]
        gamma = 0.9
        
        # æµ‹è¯•ä¸åŒnå€¼
        for n in [1, 2, 3]:
            g_n = NStepReturn.compute_n_step_return(rewards, values, n, gamma, t=0)
            print(f"  {n}æ­¥å›æŠ¥: {g_n:.3f}")
            assert isinstance(g_n, float), f"{n}æ­¥å›æŠ¥ç±»å‹é”™è¯¯"
        print("  âœ“ næ­¥å›æŠ¥è®¡ç®—æµ‹è¯•é€šè¿‡")
        
        # æµ‹è¯•næ­¥ç¼“å†²åŒº
        print("\næµ‹è¯•næ­¥ç¼“å†²åŒº...")
        buffer = NStepBuffer(n=3)
        
        # æ·»åŠ ç»éªŒ
        for i in range(5):
            state = State(f"s{i}", features={'value': i})
            exp = NStepExperience(
                state=state,
                action=None,
                reward=i * 0.1,
                next_state=State(f"s{i+1}", features={'value': i+1}),
                done=(i == 4),
                value=0.1 * (i + 1)
            )
            buffer.add(exp)
        
        assert buffer.is_ready(), "ç¼“å†²åŒºåº”è¯¥å‡†å¤‡å¥½"
        g_buffer = buffer.compute_n_step_return(gamma)
        assert isinstance(g_buffer, float), "ç¼“å†²åŒºå›æŠ¥ç±»å‹é”™è¯¯"
        print(f"  ç¼“å†²åŒºnæ­¥å›æŠ¥: {g_buffer:.3f}")
        print("  âœ“ næ­¥ç¼“å†²åŒºæµ‹è¯•é€šè¿‡")
        
        # æµ‹è¯•næ­¥TDç®—æ³•
        print("\næµ‹è¯•næ­¥TDç®—æ³•...")
        env = GridWorld(rows=3, cols=3, start_pos=(0,0), goal_pos=(2,2))
        policy = UniformRandomPolicy(env.action_space)
        
        for n in [1, 2, 4]:
            n_step_td = NStepTD(env, n=n, gamma=0.9, alpha=0.1)
            
            # å­¦ä¹ å‡ ä¸ªå›åˆ
            for _ in range(10):
                ret = n_step_td.learn_episode(policy)
                assert -100 < ret < 100, f"{n}æ­¥TDå›æŠ¥å¼‚å¸¸: {ret}"
            
            # æ£€æŸ¥ä»·å€¼å‡½æ•°
            for state in env.state_space[:3]:
                if not state.is_terminal:
                    value = n_step_td.V.get_value(state)
                    assert -100 < value < 100, f"{n}æ­¥TDä»·å€¼å¼‚å¸¸: {value}"
            
            assert len(n_step_td.episode_returns) == 10, f"{n}æ­¥TDå›åˆæ•°ä¸åŒ¹é…"
            print(f"  âœ“ {n}æ­¥TDæµ‹è¯•é€šè¿‡")
        
        # æµ‹è¯•æ¯”è¾ƒå™¨
        print("\næµ‹è¯•næ­¥TDæ¯”è¾ƒå™¨...")
        comparator = NStepTDComparator(env)
        results = comparator.compare_n_values(
            n_values=[1, 2, 4],
            n_episodes=20,
            n_runs=2,
            verbose=False
        )
        
        assert len(results) == 3, "æ¯”è¾ƒç»“æœæ•°é‡é”™è¯¯"
        for n in [1, 2, 4]:
            assert n in results, f"ç¼ºå°‘n={n}çš„ç»“æœ"
            assert 'final_return_mean' in results[n], f"n={n}ç¼ºå°‘æœ€ç»ˆå›æŠ¥"
            assert 'convergence_mean' in results[n], f"n={n}ç¼ºå°‘æ”¶æ•›ä¿¡æ¯"
        print("  âœ“ næ­¥TDæ¯”è¾ƒå™¨æµ‹è¯•é€šè¿‡")
        
        print("\nâœ… næ­¥TDé¢„æµ‹æµ‹è¯•å…¨éƒ¨é€šè¿‡ï¼")
        return True
        
    except Exception as e:
        print(f"\nâŒ næ­¥TDé¢„æµ‹æµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return False


def test_n_step_sarsa():
    """
    æµ‹è¯•næ­¥SARSAæ§åˆ¶
    Test n-step SARSA Control
    """
    print("\n" + "="*60)
    print("æµ‹è¯•næ­¥SARSAæ§åˆ¶...")
    print("Testing n-step SARSA Control...")
    print("="*60)
    
    try:
        from src.ch07_n_step_bootstrapping.n_step_sarsa import (
            NStepSARSA, NStepExpectedSARSA, NStepQSigma
        )
        from src.ch02_mdp.gridworld import GridWorld
        
        # åˆ›å»ºç¯å¢ƒ
        env = GridWorld(rows=3, cols=3, start_pos=(0,0), goal_pos=(2,2))
        print("âœ“ åˆ›å»º3Ã—3ç½‘æ ¼ä¸–ç•Œ")
        
        # æµ‹è¯•næ­¥SARSA
        print("\næµ‹è¯•næ­¥SARSA...")
        for n in [1, 2, 4]:
            sarsa = NStepSARSA(env, n=n, gamma=0.9, alpha=0.1, epsilon=0.1)
            
            # å­¦ä¹ å‡ ä¸ªå›åˆ
            for _ in range(20):
                ret, length = sarsa.learn_episode()
                assert -100 < ret < 100, f"{n}æ­¥SARSAå›æŠ¥å¼‚å¸¸: {ret}"
                assert 0 < length < 1000, f"{n}æ­¥SARSAé•¿åº¦å¼‚å¸¸: {length}"
            
            # æ£€æŸ¥Qå‡½æ•°
            for state in env.state_space[:3]:
                if not state.is_terminal:
                    for action in env.action_space:
                        q = sarsa.Q.get_value(state, action)
                        assert -100 < q < 100, f"{n}æ­¥SARSA Qå€¼å¼‚å¸¸: {q}"
            
            assert len(sarsa.episode_returns) == 20, f"{n}æ­¥SARSAå›åˆæ•°ä¸åŒ¹é…"
            print(f"  âœ“ {n}æ­¥SARSAæµ‹è¯•é€šè¿‡")
        
        # æµ‹è¯•næ­¥æœŸæœ›SARSA
        print("\næµ‹è¯•næ­¥æœŸæœ›SARSA...")
        expected_sarsa = NStepExpectedSARSA(env, n=4, gamma=0.9, alpha=0.1, epsilon=0.1)
        
        # æµ‹è¯•æœŸæœ›å€¼è®¡ç®—
        test_state = env.state_space[0]
        if not test_state.is_terminal:
            expected_value = expected_sarsa.compute_expected_value(test_state)
            assert isinstance(expected_value, float), "æœŸæœ›å€¼ç±»å‹é”™è¯¯"
            assert -100 < expected_value < 100, f"æœŸæœ›å€¼å¼‚å¸¸: {expected_value}"
        
        # å­¦ä¹ 
        for _ in range(20):
            ret, length = expected_sarsa.learn_episode()
            assert -100 < ret < 100, f"æœŸæœ›SARSAå›æŠ¥å¼‚å¸¸: {ret}"
            assert 0 < length < 1000, f"æœŸæœ›SARSAé•¿åº¦å¼‚å¸¸: {length}"
        
        assert len(expected_sarsa.episode_returns) == 20, "æœŸæœ›SARSAå›åˆæ•°ä¸åŒ¹é…"
        print("  âœ“ næ­¥æœŸæœ›SARSAæµ‹è¯•é€šè¿‡")
        
        # æµ‹è¯•næ­¥Q(Ïƒ)
        print("\næµ‹è¯•næ­¥Q(Ïƒ)...")
        q_sigma = NStepQSigma(env, n=4, gamma=0.9, alpha=0.1, sigma=0.5, epsilon=0.1)
        
        # æµ‹è¯•Ïƒå‚æ•°
        sigma_value = q_sigma.sigma_func(0)
        assert 0 <= sigma_value <= 1, f"Ïƒå€¼å¼‚å¸¸: {sigma_value}"
        
        # æµ‹è¯•næ­¥å›æŠ¥è®¡ç®—
        states = [env.state_space[i] for i in range(3)]
        actions = [env.action_space[0], env.action_space[1]]
        rewards = [0.0, 1.0]
        
        g_sigma = q_sigma.compute_n_step_return(states, actions, rewards, tau=0, T=2)
        assert isinstance(g_sigma, float), "Q(Ïƒ)å›æŠ¥ç±»å‹é”™è¯¯"
        print(f"  Q(Ïƒ)å›æŠ¥: {g_sigma:.3f}")
        print("  âœ“ næ­¥Q(Ïƒ)æµ‹è¯•é€šè¿‡")
        
        print("\nâœ… næ­¥SARSAæ§åˆ¶æµ‹è¯•å…¨éƒ¨é€šè¿‡ï¼")
        return True
        
    except Exception as e:
        print(f"\nâŒ næ­¥SARSAæ§åˆ¶æµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return False


def test_off_policy_n_step():
    """
    æµ‹è¯•Off-Policy næ­¥æ–¹æ³•
    Test Off-Policy n-step Methods
    """
    print("\n" + "="*60)
    print("æµ‹è¯•Off-Policy næ­¥æ–¹æ³•...")
    print("Testing Off-Policy n-step Methods...")
    print("="*60)
    
    try:
        from src.ch07_n_step_bootstrapping.off_policy_n_step import (
            ImportanceSamplingCorrection, OffPolicyNStepTD, OffPolicyNStepSARSA
        )
        from src.ch02_mdp.gridworld import GridWorld
        from src.ch02_mdp.policies_and_values import UniformRandomPolicy
        from src.ch04_monte_carlo.mc_control import EpsilonGreedyPolicy
        
        # æµ‹è¯•é‡è¦æ€§é‡‡æ ·ä¿®æ­£
        print("æµ‹è¯•é‡è¦æ€§é‡‡æ ·ä¿®æ­£...")
        
        # å•æ­¥æ¯”ç‡
        ratio = ImportanceSamplingCorrection.compute_importance_ratio(
            target_prob=0.8, behavior_prob=0.4, truncate=5.0
        )
        assert ratio == 2.0, f"å•æ­¥æ¯”ç‡é”™è¯¯: {ratio}"
        print(f"  å•æ­¥æ¯”ç‡: {ratio:.2f}")
        
        # ç´¯ç§¯æ¯”ç‡
        target_probs = [0.8, 0.7, 0.9]
        behavior_probs = [0.4, 0.5, 0.3]
        
        cumulative_ratio = ImportanceSamplingCorrection.compute_cumulative_ratio(
            target_probs, behavior_probs, truncate=10.0
        )
        expected_ratio = (0.8/0.4) * (0.7/0.5) * (0.9/0.3)
        assert abs(cumulative_ratio - expected_ratio) < 0.01, "ç´¯ç§¯æ¯”ç‡é”™è¯¯"
        print(f"  ç´¯ç§¯æ¯”ç‡: {cumulative_ratio:.2f}")
        
        # Per-decisionæ¯”ç‡
        per_decision = ImportanceSamplingCorrection.compute_per_decision_ratios(
            target_probs, behavior_probs, gamma=0.9, truncate=10.0
        )
        assert len(per_decision) == 3, "Per-decisionæ¯”ç‡æ•°é‡é”™è¯¯"
        print(f"  Per-decisionæ¯”ç‡: {[f'{r:.2f}' for r in per_decision]}")
        print("  âœ“ é‡è¦æ€§é‡‡æ ·ä¿®æ­£æµ‹è¯•é€šè¿‡")
        
        # æµ‹è¯•Off-Policy næ­¥TD
        print("\næµ‹è¯•Off-Policy næ­¥TD...")
        env = GridWorld(rows=3, cols=3, start_pos=(0,0), goal_pos=(2,2))
        
        # åˆ›å»ºè¡Œä¸ºå’Œç›®æ ‡ç­–ç•¥
        behavior_policy = UniformRandomPolicy(env.action_space)
        
        from src.ch02_mdp.policies_and_values import ActionValueFunction
        Q_target = ActionValueFunction(env.state_space, env.action_space, 0.0)
        target_policy = EpsilonGreedyPolicy(
            Q_target, epsilon=0.1, epsilon_decay=1.0,
            epsilon_min=0.1, action_space=env.action_space
        )
        
        off_policy_td = OffPolicyNStepTD(env, n=4, gamma=0.9, alpha=0.1)
        
        # å­¦ä¹ 
        for _ in range(10):
            ret = off_policy_td.learn_episode(behavior_policy, target_policy)
            assert -100 < ret < 100, f"Off-Policy TDå›æŠ¥å¼‚å¸¸: {ret}"
        
        # æ£€æŸ¥ä»·å€¼å‡½æ•°
        for state in env.state_space[:3]:
            if not state.is_terminal:
                value = off_policy_td.V.get_value(state)
                assert -100 < value < 100, f"Off-Policy TDä»·å€¼å¼‚å¸¸: {value}"
        
        assert len(off_policy_td.episode_returns) == 10, "Off-Policy TDå›åˆæ•°ä¸åŒ¹é…"
        assert len(off_policy_td.importance_ratios_history) > 0, "ç¼ºå°‘ISæ¯”ç‡è®°å½•"
        print("  âœ“ Off-Policy næ­¥TDæµ‹è¯•é€šè¿‡")
        
        # æµ‹è¯•Off-Policy næ­¥SARSA
        print("\næµ‹è¯•Off-Policy næ­¥SARSA...")
        off_policy_sarsa = OffPolicyNStepSARSA(
            env, n=4, gamma=0.9, alpha=0.1,
            epsilon_behavior=0.3, epsilon_target=0.1
        )
        
        # æµ‹è¯•åŠ¨ä½œæ¦‚ç‡è®¡ç®—
        test_state = env.state_space[0]
        test_action = env.action_space[0]
        prob = off_policy_sarsa.get_action_probability(
            off_policy_sarsa.behavior_policy, test_state, test_action
        )
        assert 0 <= prob <= 1, f"åŠ¨ä½œæ¦‚ç‡å¼‚å¸¸: {prob}"
        
        # å­¦ä¹ 
        for _ in range(20):
            ret, length = off_policy_sarsa.learn_episode()
            assert -100 < ret < 100, f"Off-Policy SARSAå›æŠ¥å¼‚å¸¸: {ret}"
            assert 0 < length < 1000, f"Off-Policy SARSAé•¿åº¦å¼‚å¸¸: {length}"
        
        assert len(off_policy_sarsa.episode_returns) == 20, "Off-Policy SARSAå›åˆæ•°ä¸åŒ¹é…"
        print("  âœ“ Off-Policy næ­¥SARSAæµ‹è¯•é€šè¿‡")
        
        print("\nâœ… Off-Policy næ­¥æ–¹æ³•æµ‹è¯•å…¨éƒ¨é€šè¿‡ï¼")
        return True
        
    except Exception as e:
        print(f"\nâŒ Off-Policy næ­¥æ–¹æ³•æµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return False


def test_tree_backup():
    """
    æµ‹è¯•Tree Backupç®—æ³•
    Test Tree Backup Algorithm
    """
    print("\n" + "="*60)
    print("æµ‹è¯•Tree Backupç®—æ³•...")
    print("Testing Tree Backup Algorithm...")
    print("="*60)
    
    try:
        from src.ch07_n_step_bootstrapping.tree_backup import (
            TreeBackupNode, NStepTreeBackup, TreeBackupVisualizer
        )
        from src.ch02_mdp.mdp_framework import State, Action
        from src.ch02_mdp.gridworld import GridWorld
        from src.ch02_mdp.policies_and_values import UniformRandomPolicy
        
        # æµ‹è¯•Tree BackupèŠ‚ç‚¹
        print("æµ‹è¯•Tree BackupèŠ‚ç‚¹...")
        state = State("test", features={'value': 0})
        node = TreeBackupNode(
            state=state,
            depth=0,
            is_leaf=False,
            expected_value=1.0
        )
        
        # æ·»åŠ åŠ¨ä½œå€¼
        action1 = Action("a1")
        action2 = Action("a2")
        node.action_values = {action1: 0.5, action2: 0.8}
        node.action_probabilities = {action1: 0.3, action2: 0.7}
        node.taken_action = action1
        
        backup_value = node.compute_backup_value(gamma=0.9)
        assert isinstance(backup_value, float), "Backupå€¼ç±»å‹é”™è¯¯"
        print(f"  èŠ‚ç‚¹backupå€¼: {backup_value:.3f}")
        print("  âœ“ Tree BackupèŠ‚ç‚¹æµ‹è¯•é€šè¿‡")
        
        # æµ‹è¯•næ­¥Tree Backupç®—æ³•
        print("\næµ‹è¯•næ­¥Tree Backupç®—æ³•...")
        env = GridWorld(rows=3, cols=3, start_pos=(0,0), goal_pos=(2,2))
        
        for n in [1, 2, 4]:
            tree_backup = NStepTreeBackup(env, n=n, gamma=0.9, alpha=0.1, epsilon=0.1)
            
            # æµ‹è¯•åŠ¨ä½œæ¦‚ç‡è®¡ç®—
            test_state = env.state_space[0]
            test_action = env.action_space[0]
            prob = tree_backup.get_action_probability(test_state, test_action)
            assert 0 <= prob <= 1, f"åŠ¨ä½œæ¦‚ç‡å¼‚å¸¸: {prob}"
            
            # æµ‹è¯•tree backupå›æŠ¥è®¡ç®—
            states = [env.state_space[i] for i in range(min(3, len(env.state_space)))]
            actions = [env.action_space[0], env.action_space[1]]
            rewards = [0.0, 1.0]
            
            g_tree = tree_backup.compute_tree_backup_return(
                states, actions, rewards, tau=0, T=2
            )
            assert isinstance(g_tree, float), "Tree backupå›æŠ¥ç±»å‹é”™è¯¯"
            
            # å­¦ä¹ 
            for _ in range(20):
                ret, length = tree_backup.learn_episode()
                assert -100 < ret < 100, f"{n}æ­¥Tree Backupå›æŠ¥å¼‚å¸¸: {ret}"
                assert 0 < length < 1000, f"{n}æ­¥Tree Backupé•¿åº¦å¼‚å¸¸: {length}"
            
            # æ£€æŸ¥Qå‡½æ•°
            for state in env.state_space[:3]:
                if not state.is_terminal:
                    for action in env.action_space:
                        q = tree_backup.Q.get_value(state, action)
                        assert -100 < q < 100, f"{n}æ­¥Tree Backup Qå€¼å¼‚å¸¸: {q}"
            
            assert len(tree_backup.episode_returns) == 20, f"{n}æ­¥Tree Backupå›åˆæ•°ä¸åŒ¹é…"
            print(f"  âœ“ {n}æ­¥Tree Backupæµ‹è¯•é€šè¿‡")
        
        # æµ‹è¯•off-policyå­¦ä¹ 
        print("\næµ‹è¯•Tree Backupçš„off-policyå­¦ä¹ ...")
        behavior_policy = UniformRandomPolicy(env.action_space)
        tree_backup_offpolicy = NStepTreeBackup(
            env, n=4, gamma=0.9, alpha=0.1, epsilon=0.05
        )
        
        # ä½¿ç”¨éšæœºç­–ç•¥å­¦ä¹ 
        for _ in range(20):
            ret, length = tree_backup_offpolicy.learn_episode(behavior_policy)
            assert -100 < ret < 100, "Off-policy Tree Backupå›æŠ¥å¼‚å¸¸"
            assert 0 < length < 1000, "Off-policy Tree Backupé•¿åº¦å¼‚å¸¸"
        
        assert len(tree_backup_offpolicy.episode_returns) == 20, "Off-policyå›åˆæ•°ä¸åŒ¹é…"
        print("  âœ“ Off-policy Tree Backupæµ‹è¯•é€šè¿‡")
        
        print("\nâœ… Tree Backupç®—æ³•æµ‹è¯•å…¨éƒ¨é€šè¿‡ï¼")
        return True
        
    except Exception as e:
        print(f"\nâŒ Tree Backupç®—æ³•æµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return False


def test_n_step_comparison():
    """
    æµ‹è¯•næ­¥æ–¹æ³•çš„æ¯”è¾ƒ
    Test comparison of n-step methods
    """
    print("\n" + "="*60)
    print("æµ‹è¯•næ­¥æ–¹æ³•æ¯”è¾ƒ...")
    print("Testing n-step Methods Comparison...")
    print("="*60)
    
    try:
        from src.ch07_n_step_bootstrapping.n_step_td import NStepTD
        from src.ch07_n_step_bootstrapping.n_step_sarsa import NStepSARSA, NStepExpectedSARSA
        from src.ch07_n_step_bootstrapping.tree_backup import NStepTreeBackup
        from src.ch02_mdp.gridworld import GridWorld
        from src.ch02_mdp.policies_and_values import UniformRandomPolicy
        
        # åˆ›å»ºç¯å¢ƒ
        env = GridWorld(rows=3, cols=3, start_pos=(0,0), goal_pos=(2,2))
        policy = UniformRandomPolicy(env.action_space)
        
        n = 4
        n_episodes = 50
        
        print(f"æ¯”è¾ƒä¸åŒçš„{n}æ­¥æ–¹æ³•...")
        
        methods = {
            'n-step TD': NStepTD(env, n=n, gamma=0.9, alpha=0.1),
            'n-step SARSA': NStepSARSA(env, n=n, gamma=0.9, alpha=0.1, epsilon=0.1),
            'n-step Expected SARSA': NStepExpectedSARSA(env, n=n, gamma=0.9, alpha=0.1, epsilon=0.1),
            'n-step Tree Backup': NStepTreeBackup(env, n=n, gamma=0.9, alpha=0.1, epsilon=0.1)
        }
        
        results = {}
        
        for name, algo in methods.items():
            returns = []
            
            if name == 'n-step TD':
                # TDé¢„æµ‹
                for _ in range(n_episodes):
                    ret = algo.learn_episode(policy)
                    returns.append(ret)
            else:
                # æ§åˆ¶ç®—æ³•
                for _ in range(n_episodes):
                    ret, _ = algo.learn_episode()
                    returns.append(ret)
            
            results[name] = {
                'final_return': returns[-1],
                'avg_return': np.mean(returns[-10:])
            }
            
            print(f"  {name}: æœ€ç»ˆå›æŠ¥={returns[-1]:.2f}, "
                  f"å¹³å‡å›æŠ¥={results[name]['avg_return']:.2f}")
        
        # éªŒè¯æ‰€æœ‰æ–¹æ³•éƒ½æ”¶æ•›åˆ°åˆç†å€¼
        for name, data in results.items():
            assert -100 < data['final_return'] < 100, f"{name}æ”¶æ•›å€¼å¼‚å¸¸"
        
        print("\nâœ… næ­¥æ–¹æ³•æ¯”è¾ƒæµ‹è¯•é€šè¿‡ï¼")
        return True
        
    except Exception as e:
        print(f"\nâŒ næ­¥æ–¹æ³•æ¯”è¾ƒæµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return False


def main():
    """
    è¿è¡Œæ‰€æœ‰æµ‹è¯•
    Run all tests
    """
    print("\n" + "="*80)
    print("ç¬¬7ç« ï¼šnæ­¥è‡ªä¸¾æ–¹æ³• - æ¨¡å—æµ‹è¯•")
    print("Chapter 7: n-step Bootstrapping - Module Tests")
    print("="*80)
    
    tests = [
        ("næ­¥TDé¢„æµ‹", test_n_step_td),
        ("næ­¥SARSAæ§åˆ¶", test_n_step_sarsa),
        ("Off-Policy næ­¥æ–¹æ³•", test_off_policy_n_step),
        ("Tree Backupç®—æ³•", test_tree_backup),
        ("næ­¥æ–¹æ³•æ¯”è¾ƒ", test_n_step_comparison)
    ]
    
    results = []
    start_time = time.time()
    
    for name, test_func in tests:
        print(f"\nè¿è¡Œæµ‹è¯•: {name}")
        success = test_func()
        results.append((name, success))
        
        if not success:
            print(f"\nâš ï¸ æµ‹è¯•å¤±è´¥ï¼Œåœæ­¢åç»­æµ‹è¯•")
            break
    
    total_time = time.time() - start_time
    
    # æ€»ç»“
    print("\n" + "="*80)
    print("æµ‹è¯•æ€»ç»“ Test Summary")
    print("="*80)
    
    all_passed = True
    for name, success in results:
        status = "âœ… é€šè¿‡" if success else "âŒ å¤±è´¥"
        print(f"{name}: {status}")
        if not success:
            all_passed = False
    
    print(f"\næ€»æµ‹è¯•æ—¶é—´: {total_time:.2f}ç§’")
    
    if all_passed:
        print("\nğŸ‰ ç¬¬7ç« æ‰€æœ‰næ­¥è‡ªä¸¾æ–¹æ³•æ¨¡å—æµ‹è¯•é€šè¿‡ï¼")
        print("ğŸ‰ All Chapter 7 n-step Bootstrapping modules passed!")
        print("\nnæ­¥æ–¹æ³•å®ç°éªŒè¯å®Œæˆ:")
        print("âœ“ næ­¥TDé¢„æµ‹")
        print("âœ“ næ­¥SARSAå’ŒæœŸæœ›SARSA")
        print("âœ“ næ­¥Q(Ïƒ)ç»Ÿä¸€ç®—æ³•")
        print("âœ“ Off-policy næ­¥æ–¹æ³•withé‡è¦æ€§é‡‡æ ·")
        print("âœ“ Tree Backupç®—æ³•ï¼ˆæ— éœ€ISï¼‰")
        print("\nnæ­¥æ–¹æ³•ä¼˜é›…åœ°ç»Ÿä¸€äº†TDå’ŒMCï¼")
        print("n-step methods elegantly unify TD and MC!")
        print("\nå¯ä»¥ç»§ç»­å­¦ä¹ ç¬¬8ç« æˆ–å¼€å§‹å®é™…é¡¹ç›®")
        print("Ready to proceed to Chapter 8 or start practical projects")
    else:
        print("\nâš ï¸ æœ‰äº›æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä»£ç ")
        print("âš ï¸ Some tests failed, please check the code")
    
    return all_passed


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)