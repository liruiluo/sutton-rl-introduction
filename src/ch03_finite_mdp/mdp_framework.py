"""
================================================================================
ç¬¬2.1èŠ‚ï¼šMDPæ¡†æ¶ - å¼ºåŒ–å­¦ä¹ çš„æ•°å­¦åŸºç¡€
Section 2.1: MDP Framework - Mathematical Foundation of RL
================================================================================

é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹(MDP)å°†ç¬¬1ç« çš„å•çŠ¶æ€é—®é¢˜æ‰©å±•åˆ°å¤šçŠ¶æ€åºåˆ—å†³ç­–é—®é¢˜
MDP extends Chapter 1's single-state problem to multi-state sequential decision problems

æ ¸å¿ƒå‡çº§ Core Upgrades:
1. çŠ¶æ€(State): ä»æ— çŠ¶æ€ â†’ ç¯å¢ƒçŠ¶æ€ç©ºé—´ S
2. è½¬ç§»(Transition): ä»ç«‹å³å¥–åŠ± â†’ çŠ¶æ€è½¬ç§»æ¦‚ç‡ P
3. ç­–ç•¥(Policy): ä»åŠ¨ä½œé€‰æ‹© â†’ çŠ¶æ€åˆ°åŠ¨ä½œçš„æ˜ å°„ Ï€
4. ä»·å€¼(Value): ä»Q(a) â†’ V(s)å’ŒQ(s,a)

è¿™ä¸€ç« æ˜¯æ•´ä¸ªå¼ºåŒ–å­¦ä¹ çš„ç†è®ºåŸºçŸ³ï¼
This chapter is the theoretical cornerstone of all RL!
"""

import numpy as np
from typing import Dict, List, Tuple, Optional, Union, Any, Callable
from dataclasses import dataclass, field
from enum import Enum
import logging
from abc import ABC, abstractmethod

# è®¾ç½®æ—¥å¿—
logger = logging.getLogger(__name__)


# ================================================================================
# ç¬¬2.1.1èŠ‚ï¼šMDPçš„å½¢å¼åŒ–å®šä¹‰
# Section 2.1.1: Formal Definition of MDP
# ================================================================================

@dataclass
class State:
    """
    çŠ¶æ€ - ç¯å¢ƒçš„å®Œæ•´æè¿°
    State - Complete description of the environment
    
    åœ¨MDPä¸­ï¼ŒçŠ¶æ€å¿…é¡»æ»¡è¶³é©¬å°”å¯å¤«æ€§è´¨ï¼š
    In MDP, state must satisfy Markov property:
    
    P[S_{t+1} | S_t] = P[S_{t+1} | S_1, S_2, ..., S_t]
    
    å³ï¼šæœªæ¥åªä¾èµ–äºç°åœ¨ï¼Œä¸ä¾èµ–äºè¿‡å»
    i.e., The future depends only on the present, not on the past
    
    ç”Ÿæ´»ç±»æ¯” Life Analogy:
    å°±åƒä¸‹æ£‹ï¼Œå½“å‰æ£‹ç›˜çŠ¶æ€åŒ…å«äº†æ‰€æœ‰å†³ç­–æ‰€éœ€ä¿¡æ¯ï¼Œ
    ä¸éœ€è¦çŸ¥é“ä¹‹å‰æ¯ä¸€æ­¥æ˜¯æ€ä¹ˆèµ°çš„
    Like playing chess, the current board state contains all information needed for decisions,
    no need to know how each previous move was made
    """
    
    # çŠ¶æ€æ ‡è¯†ç¬¦
    id: Union[int, str, Tuple]
    
    # çŠ¶æ€ç‰¹å¾ï¼ˆå¯é€‰ï¼Œç”¨äºå‡½æ•°è¿‘ä¼¼ï¼‰
    features: Optional[np.ndarray] = None
    
    # æ˜¯å¦ä¸ºç»ˆæ­¢çŠ¶æ€
    is_terminal: bool = False
    
    # é¢å¤–ä¿¡æ¯
    info: Dict[str, Any] = field(default_factory=dict)
    
    def __hash__(self):
        """ä½¿çŠ¶æ€å¯å“ˆå¸Œï¼Œç”¨ä½œå­—å…¸é”®"""
        if isinstance(self.id, (list, np.ndarray)):
            return hash(tuple(self.id))
        return hash(self.id)
    
    def __eq__(self, other):
        """çŠ¶æ€ç›¸ç­‰æ€§æ¯”è¾ƒ"""
        if not isinstance(other, State):
            return False
        return self.id == other.id


@dataclass
class Action:
    """
    åŠ¨ä½œ - æ™ºèƒ½ä½“å¯ä»¥æ‰§è¡Œçš„æ“ä½œ
    Action - Operations that agent can execute
    
    åŠ¨ä½œç©ºé—´å¯ä»¥æ˜¯ï¼š
    Action space can be:
    - ç¦»æ•£çš„ï¼šå¦‚{ä¸Š,ä¸‹,å·¦,å³}
      Discrete: e.g., {up, down, left, right}
    - è¿ç»­çš„ï¼šå¦‚æœºå™¨äººå…³èŠ‚è§’åº¦
      Continuous: e.g., robot joint angles
    
    æœ¬ç« ä¸»è¦å…³æ³¨ç¦»æ•£åŠ¨ä½œç©ºé—´
    This chapter focuses on discrete action spaces
    """
    
    # åŠ¨ä½œæ ‡è¯†ç¬¦
    id: Union[int, str, Tuple]
    
    # åŠ¨ä½œå‚æ•°ï¼ˆç”¨äºè¿ç»­åŠ¨ä½œï¼‰
    parameters: Optional[np.ndarray] = None
    
    # åŠ¨ä½œåç§°ï¼ˆä¾¿äºç†è§£ï¼‰
    name: str = ""
    
    # é¢å¤–ä¿¡æ¯
    info: Dict[str, Any] = field(default_factory=dict)
    
    def __hash__(self):
        """ä½¿åŠ¨ä½œå¯å“ˆå¸Œ"""
        if isinstance(self.id, (list, np.ndarray)):
            return hash(tuple(self.id))
        return hash(self.id)
    
    def __eq__(self, other):
        """åŠ¨ä½œç›¸ç­‰æ€§æ¯”è¾ƒ"""
        if not isinstance(other, Action):
            return False
        return self.id == other.id


class PolicyType(Enum):
    """
    ç­–ç•¥ç±»å‹
    Policy Types
    
    å¼ºåŒ–å­¦ä¹ ä¸­çš„ç­–ç•¥åˆ†ç±»
    Policy categories in RL
    """
    DETERMINISTIC = "deterministic"  # ç¡®å®šæ€§ç­–ç•¥ Ï€: S â†’ A
    STOCHASTIC = "stochastic"        # éšæœºç­–ç•¥ Ï€: S Ã— A â†’ [0,1]
    EPSILON_GREEDY = "epsilon_greedy"  # Îµ-è´ªå©ªç­–ç•¥
    SOFTMAX = "softmax"              # åŸºäºsoftmaxçš„ç­–ç•¥


# ================================================================================
# ç¬¬2.1.2èŠ‚ï¼šMDPçš„æ ¸å¿ƒç»„ä»¶
# Section 2.1.2: Core Components of MDP
# ================================================================================

class TransitionProbability:
    """
    çŠ¶æ€è½¬ç§»æ¦‚ç‡å‡½æ•° P
    State Transition Probability Function P
    
    æ•°å­¦å®šä¹‰ Mathematical Definition:
    P(s', r | s, a) = Pr{S_{t+1}=s', R_{t+1}=r | S_t=s, A_t=a}
    
    è¿™å®šä¹‰äº†MDPçš„åŠ¨æ€ç‰¹æ€§ï¼
    This defines the dynamics of MDP!
    
    æ€§è´¨ Properties:
    1. å½’ä¸€åŒ–ï¼šÎ£_{s',r} P(s',r|s,a) = 1, âˆ€s,a
       Normalization: Sum over all s',r equals 1
    
    2. é©¬å°”å¯å¤«æ€§ï¼šåªä¾èµ–å½“å‰çŠ¶æ€å’ŒåŠ¨ä½œ
       Markov property: Depends only on current state and action
    
    æ·±å…¥ç†è§£ Deep Understanding:
    è½¬ç§»æ¦‚ç‡å®Œå…¨æè¿°äº†ç¯å¢ƒçš„è¡Œä¸ºã€‚å¦‚æœçŸ¥é“Pï¼Œ
    å°±å¯ä»¥å®Œç¾é¢„æµ‹ç¯å¢ƒçš„ååº”ï¼ˆè™½ç„¶å¯èƒ½æ˜¯éšæœºçš„ï¼‰
    Transition probability fully describes environment behavior.
    If we know P, we can perfectly predict environment response (though possibly stochastic)
    """
    
    def __init__(self):
        """åˆå§‹åŒ–è½¬ç§»æ¦‚ç‡"""
        # å­˜å‚¨æ ¼å¼ï¼šP[s][a] = [(s', r, prob), ...]
        # Storage format: P[s][a] = [(s', r, prob), ...]
        self.P: Dict[State, Dict[Action, List[Tuple[State, float, float]]]] = {}
        
        logger.info("åˆå§‹åŒ–è½¬ç§»æ¦‚ç‡å‡½æ•°")
    
    def set_probability(self, s: State, a: Action, 
                       s_prime: State, r: float, prob: float):
        """
        è®¾ç½®è½¬ç§»æ¦‚ç‡
        Set transition probability
        
        Args:
            s: å½“å‰çŠ¶æ€ Current state
            a: æ‰§è¡ŒåŠ¨ä½œ Action taken
            s_prime: ä¸‹ä¸€çŠ¶æ€ Next state
            r: è·å¾—å¥–åŠ± Reward received
            prob: è½¬ç§»æ¦‚ç‡ Transition probability
        """
        if s not in self.P:
            self.P[s] = {}
        if a not in self.P[s]:
            self.P[s][a] = []
        
        self.P[s][a].append((s_prime, r, prob))
        
        logger.debug(f"è®¾ç½®P(s'={s_prime.id}, r={r} | s={s.id}, a={a.id}) = {prob}")
    
    def get_transitions(self, s: State, a: Action) -> List[Tuple[State, float, float]]:
        """
        è·å–ç»™å®š(s,a)çš„æ‰€æœ‰å¯èƒ½è½¬ç§»
        Get all possible transitions for given (s,a)
        
        Returns:
            [(ä¸‹ä¸€çŠ¶æ€, å¥–åŠ±, æ¦‚ç‡), ...]
            [(next_state, reward, probability), ...]
        """
        if s in self.P and a in self.P[s]:
            return self.P[s][a]
        return []
    
    def sample(self, s: State, a: Action) -> Tuple[State, float]:
        """
        æ ¹æ®è½¬ç§»æ¦‚ç‡é‡‡æ ·ä¸‹ä¸€çŠ¶æ€å’Œå¥–åŠ±
        Sample next state and reward according to transition probability
        
        è¿™æ¨¡æ‹Ÿäº†ç¯å¢ƒçš„éšæœºæ€§ï¼
        This simulates environment stochasticity!
        
        Returns:
            (ä¸‹ä¸€çŠ¶æ€, å¥–åŠ±)
            (next_state, reward)
        """
        transitions = self.get_transitions(s, a)
        if not transitions:
            raise ValueError(f"No transitions defined for state {s.id}, action {a.id}")
        
        # æå–æ¦‚ç‡åˆ†å¸ƒ
        probs = [p for _, _, p in transitions]
        probs = np.array(probs) / np.sum(probs)  # å½’ä¸€åŒ–
        
        # æŒ‰æ¦‚ç‡é‡‡æ ·
        idx = np.random.choice(len(transitions), p=probs)
        s_prime, r, _ = transitions[idx]
        
        return s_prime, r


class RewardFunction:
    """
    å¥–åŠ±å‡½æ•° R
    Reward Function R
    
    æ•°å­¦å®šä¹‰ Mathematical Definition:
    1. å››å‚æ•°å½¢å¼ï¼šr(s, a, s')
       Four-parameter form: r(s, a, s')
    
    2. æœŸæœ›å¥–åŠ±ï¼šr(s, a) = E[R_{t+1} | S_t=s, A_t=a]
       Expected reward: r(s, a) = E[R_{t+1} | S_t=s, A_t=a]
    
    å¥–åŠ±å‡è®¾ Reward Hypothesis:
    "æˆ‘ä»¬å¯ä»¥è®¤ä¸ºï¼Œæ‰€æœ‰çš„ç›®æ ‡å’Œç›®çš„éƒ½å¯ä»¥è¢«æè¿°ä¸º
    æœŸæœ›ç´¯ç§¯å¥–åŠ±æ€»å’Œçš„æœ€å¤§åŒ–"
    "That all of what we mean by goals and purposes can be well thought of as
    the maximization of the expected value of the cumulative sum of rewards"
    
    è¿™æ˜¯å¼ºåŒ–å­¦ä¹ çš„æ ¸å¿ƒå‡è®¾ï¼
    This is the core hypothesis of RL!
    """
    
    def __init__(self, reward_type: str = "deterministic"):
        """
        åˆå§‹åŒ–å¥–åŠ±å‡½æ•°
        
        Args:
            reward_type: "deterministic" æˆ– "stochastic"
        """
        self.reward_type = reward_type
        # å­˜å‚¨æ ¼å¼å–å†³äºç±»å‹
        self.R: Dict = {}
        
        logger.info(f"åˆå§‹åŒ–{reward_type}å¥–åŠ±å‡½æ•°")
    
    def set_reward(self, s: State, a: Action, 
                  s_prime: Optional[State] = None,
                  reward: Union[float, Callable] = 0.0):
        """
        è®¾ç½®å¥–åŠ±
        Set reward
        
        æ”¯æŒå¤šç§å¥–åŠ±å®šä¹‰æ–¹å¼ï¼š
        Supports multiple reward definition methods:
        1. r(s,a,s'): æœ€ä¸€èˆ¬å½¢å¼
        2. r(s,a): æœŸæœ›å¥–åŠ±
        3. r(s): çŠ¶æ€å¥–åŠ±
        """
        key = (s, a, s_prime) if s_prime else (s, a)
        self.R[key] = reward
    
    def get_reward(self, s: State, a: Action, 
                  s_prime: Optional[State] = None) -> float:
        """
        è·å–å¥–åŠ±
        Get reward
        
        Returns:
            å¥–åŠ±å€¼
            Reward value
        """
        # å°è¯•ä¸åŒçš„é”®ç»„åˆ
        keys = [
            (s, a, s_prime) if s_prime else None,
            (s, a),
            (s,)
        ]
        
        for key in keys:
            if key and key in self.R:
                reward = self.R[key]
                # å¦‚æœæ˜¯å‡½æ•°ï¼Œè°ƒç”¨å®ƒ
                if callable(reward):
                    return reward(s, a, s_prime)
                return reward
        
        # é»˜è®¤å¥–åŠ±
        return 0.0


# ================================================================================
# ç¬¬2.1.3èŠ‚ï¼šMDPç¯å¢ƒåŸºç±»
# Section 2.1.3: MDP Environment Base Class
# ================================================================================

class MDPEnvironment(ABC):
    """
    MDPç¯å¢ƒåŸºç±»
    MDP Environment Base Class
    
    è¿™æ˜¯æ™ºèƒ½ä½“äº¤äº’çš„ä¸–ç•Œï¼
    This is the world the agent interacts with!
    
    ç¯å¢ƒçš„èŒè´£ Environment Responsibilities:
    1. ç»´æŠ¤å½“å‰çŠ¶æ€
       Maintain current state
    2. æ¥æ”¶åŠ¨ä½œï¼Œè¿”å›å¥–åŠ±å’Œä¸‹ä¸€çŠ¶æ€
       Receive action, return reward and next state
    3. åˆ¤æ–­å›åˆæ˜¯å¦ç»“æŸ
       Determine if episode is done
    4. æä¾›çŠ¶æ€å’ŒåŠ¨ä½œç©ºé—´ä¿¡æ¯
       Provide state and action space information
    
    è®¾è®¡åŸåˆ™ Design Principles:
    - ç¯å¢ƒæ˜¯è¢«åŠ¨çš„ï¼Œåªå“åº”åŠ¨ä½œ
      Environment is passive, only responds to actions
    - ç¯å¢ƒä¸çŸ¥é“æ™ºèƒ½ä½“çš„å­˜åœ¨
      Environment doesn't know about agent's existence
    - ç¯å¢ƒçš„åŠ¨æ€ç”±På’ŒRå®Œå…¨å®šä¹‰
      Environment dynamics fully defined by P and R
    """
    
    def __init__(self, name: str = "MDP Environment"):
        """
        åˆå§‹åŒ–MDPç¯å¢ƒ
        Initialize MDP environment
        
        Args:
            name: ç¯å¢ƒåç§°
        """
        self.name = name
        
        # çŠ¶æ€å’ŒåŠ¨ä½œç©ºé—´
        self.state_space: List[State] = []
        self.action_space: List[Action] = []
        
        # è½¬ç§»æ¦‚ç‡å’Œå¥–åŠ±å‡½æ•°
        self.P = TransitionProbability()
        self.R = RewardFunction()
        
        # å½“å‰çŠ¶æ€
        self.current_state: Optional[State] = None
        
        # æŠ˜æ‰£å› å­ï¼ˆå…³é”®å‚æ•°ï¼ï¼‰
        self.gamma = 0.99
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.step_count = 0
        self.episode_count = 0
        
        logger.info(f"åˆå§‹åŒ–MDPç¯å¢ƒ: {name}")
    
    @abstractmethod
    def reset(self) -> State:
        """
        é‡ç½®ç¯å¢ƒåˆ°åˆå§‹çŠ¶æ€
        Reset environment to initial state
        
        æ¯ä¸ªå›åˆå¼€å§‹æ—¶è°ƒç”¨
        Called at the beginning of each episode
        
        Returns:
            åˆå§‹çŠ¶æ€
            Initial state
        """
        pass
    
    @abstractmethod
    def step(self, action: Action) -> Tuple[State, float, bool, Dict]:
        """
        æ‰§è¡ŒåŠ¨ä½œï¼Œç¯å¢ƒå‰è¿›ä¸€æ­¥
        Execute action, environment steps forward
        
        è¿™æ˜¯ç¯å¢ƒçš„æ ¸å¿ƒæ–¹æ³•ï¼
        This is the core method of environment!
        
        Args:
            action: è¦æ‰§è¡Œçš„åŠ¨ä½œ
                   Action to execute
        
        Returns:
            (ä¸‹ä¸€çŠ¶æ€, å¥–åŠ±, æ˜¯å¦ç»“æŸ, é¢å¤–ä¿¡æ¯)
            (next_state, reward, done, info)
        """
        pass
    
    def render(self, mode: str = 'human'):
        """
        æ¸²æŸ“ç¯å¢ƒå½“å‰çŠ¶æ€
        Render current environment state
        
        Args:
            mode: æ¸²æŸ“æ¨¡å¼
                 Rendering mode
        """
        print(f"Current State: {self.current_state.id if self.current_state else 'None'}")
        print(f"Step: {self.step_count}, Episode: {self.episode_count}")
    
    def get_state_space(self) -> List[State]:
        """è·å–çŠ¶æ€ç©ºé—´"""
        return self.state_space
    
    def get_action_space(self, state: Optional[State] = None) -> List[Action]:
        """
        è·å–åŠ¨ä½œç©ºé—´ï¼ˆå¯èƒ½ä¾èµ–äºçŠ¶æ€ï¼‰
        Get action space (may depend on state)
        """
        return self.action_space
    
    def is_terminal(self, state: State) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦ä¸ºç»ˆæ­¢çŠ¶æ€
        Check if state is terminal
        """
        return state.is_terminal
    
    def get_dynamics(self) -> Tuple[TransitionProbability, RewardFunction]:
        """
        è·å–ç¯å¢ƒåŠ¨æ€ï¼ˆç”¨äºåŸºäºæ¨¡å‹çš„æ–¹æ³•ï¼‰
        Get environment dynamics (for model-based methods)
        
        æ³¨æ„ï¼šå®é™…ç¯å¢ƒé€šå¸¸ä¸æä¾›è¿™ä¸ªï¼
        Note: Real environments usually don't provide this!
        """
        return self.P, self.R


# ================================================================================
# ç¬¬2.1.4èŠ‚ï¼šMDPæ™ºèƒ½ä½“åŸºç±»
# Section 2.1.4: MDP Agent Base Class
# ================================================================================

class MDPAgent(ABC):
    """
    MDPæ™ºèƒ½ä½“åŸºç±»
    MDP Agent Base Class
    
    æ™ºèƒ½ä½“æ˜¯å­¦ä¹ å’Œå†³ç­–çš„ä¸»ä½“ï¼
    Agent is the subject of learning and decision-making!
    
    ä¸ç¬¬1ç« çš„åŒºåˆ« Differences from Chapter 1:
    1. éœ€è¦å¤„ç†çŠ¶æ€åºåˆ—ï¼Œä¸åªæ˜¯å•ä¸ªåŠ¨ä½œ
       Need to handle state sequences, not just single actions
    2. éœ€è¦å­¦ä¹ ç­–ç•¥Ï€(a|s)ï¼Œä¸åªæ˜¯åŠ¨ä½œä»·å€¼Q(a)
       Need to learn policy Ï€(a|s), not just action values Q(a)
    3. éœ€è¦è€ƒè™‘æœªæ¥å¥–åŠ±ï¼ˆæŠ˜æ‰£ï¼‰ï¼Œä¸åªæ˜¯å³æ—¶å¥–åŠ±
       Need to consider future rewards (discounted), not just immediate rewards
    
    æ ¸å¿ƒç»„ä»¶ Core Components:
    - ç­–ç•¥(Policy): å¦‚ä½•é€‰æ‹©åŠ¨ä½œ
    - ä»·å€¼å‡½æ•°(Value Function): è¯„ä¼°çŠ¶æ€æˆ–åŠ¨ä½œçš„å¥½å
    - æ¨¡å‹(Model): å¯¹ç¯å¢ƒçš„ç†è§£ï¼ˆå¯é€‰ï¼‰
    """
    
    def __init__(self, name: str = "MDP Agent"):
        """
        åˆå§‹åŒ–MDPæ™ºèƒ½ä½“
        
        Args:
            name: æ™ºèƒ½ä½“åç§°
        """
        self.name = name
        
        # ç­–ç•¥ç±»å‹
        self.policy_type = PolicyType.STOCHASTIC
        
        # å­¦ä¹ ç‡
        self.alpha = 0.1
        
        # æŠ˜æ‰£å› å­
        self.gamma = 0.99
        
        # æ¢ç´¢å‚æ•°
        self.epsilon = 0.1
        
        # ç»éªŒç¼“å†²
        self.experience_buffer = []
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.total_reward = 0.0
        self.episode_rewards = []
        
        logger.info(f"åˆå§‹åŒ–MDPæ™ºèƒ½ä½“: {name}")
    
    @abstractmethod
    def select_action(self, state: State) -> Action:
        """
        æ ¹æ®å½“å‰çŠ¶æ€é€‰æ‹©åŠ¨ä½œ
        Select action based on current state
        
        è¿™æ˜¯ç­–ç•¥çš„ä½“ç°ï¼
        This embodies the policy!
        
        Args:
            state: å½“å‰çŠ¶æ€
                  Current state
        
        Returns:
            é€‰æ‹©çš„åŠ¨ä½œ
            Selected action
        """
        pass
    
    @abstractmethod
    def update(self, state: State, action: Action, 
              reward: float, next_state: State, done: bool):
        """
        æ ¹æ®ç»éªŒæ›´æ–°æ™ºèƒ½ä½“
        Update agent based on experience
        
        è¿™æ˜¯å­¦ä¹ çš„æ ¸å¿ƒï¼
        This is the core of learning!
        
        Args:
            state: å½“å‰çŠ¶æ€ Current state
            action: æ‰§è¡Œçš„åŠ¨ä½œ Action taken
            reward: è·å¾—çš„å¥–åŠ± Reward received
            next_state: ä¸‹ä¸€çŠ¶æ€ Next state
            done: æ˜¯å¦ç»“æŸ Whether episode is done
        """
        pass
    
    def reset(self):
        """
        é‡ç½®æ™ºèƒ½ä½“ï¼ˆæ–°å›åˆå¼€å§‹ï¼‰
        Reset agent (new episode starts)
        """
        if len(self.experience_buffer) > 0:
            episode_reward = sum(exp[2] for exp in self.experience_buffer)
            self.episode_rewards.append(episode_reward)
            logger.info(f"Episode finished. Total reward: {episode_reward}")
        
        self.experience_buffer = []
    
    def save_experience(self, state: State, action: Action,
                       reward: float, next_state: State, done: bool):
        """
        ä¿å­˜ç»éªŒ
        Save experience
        
        ç»éªŒå›æ”¾çš„åŸºç¡€ï¼
        Foundation for experience replay!
        """
        experience = (state, action, reward, next_state, done)
        self.experience_buffer.append(experience)
        self.total_reward += reward
    
    def get_statistics(self) -> Dict[str, Any]:
        """
        è·å–ç»Ÿè®¡ä¿¡æ¯
        Get statistics
        """
        return {
            'total_reward': self.total_reward,
            'episode_rewards': self.episode_rewards,
            'average_reward': np.mean(self.episode_rewards) if self.episode_rewards else 0,
            'episodes_completed': len(self.episode_rewards)
        }


# ================================================================================
# ç¬¬2.1.5èŠ‚ï¼šMDPé—®é¢˜çš„æ•°å­¦è¡¨è¿°
# Section 2.1.5: Mathematical Formulation of MDP Problem
# ================================================================================

class MDPMathematics:
    """
    MDPçš„æ•°å­¦ç†è®º
    Mathematical Theory of MDP
    
    è¿™éƒ¨åˆ†æå…¶é‡è¦ï¼Œæ˜¯æ‰€æœ‰RLç®—æ³•çš„ç†è®ºåŸºç¡€ï¼
    This part is extremely important, the theoretical foundation of all RL algorithms!
    """
    
    @staticmethod
    def explain_mdp_formulation():
        """
        è¯¦è§£MDPçš„æ•°å­¦è¡¨è¿°
        Detailed explanation of MDP mathematical formulation
        """
        print("\n" + "="*80)
        print("MDPçš„å®Œæ•´æ•°å­¦è¡¨è¿°")
        print("Complete Mathematical Formulation of MDP")
        print("="*80)
        
        print("""
        1. MDPçš„äº”å…ƒç»„å®šä¹‰ Five-tuple Definition
        ==========================================
        
        MDP = (S, A, P, R, Î³)
        
        å…¶ä¸­ Where:
        - S: çŠ¶æ€ç©ºé—´ï¼ˆæœ‰é™é›†åˆï¼‰
             State space (finite set)
        - A: åŠ¨ä½œç©ºé—´ï¼ˆæœ‰é™é›†åˆï¼‰
             Action space (finite set)  
        - P: S Ã— A Ã— S â†’ [0,1]ï¼ŒçŠ¶æ€è½¬ç§»æ¦‚ç‡
             State transition probability
        - R: S Ã— A Ã— S â†’ â„ï¼Œå¥–åŠ±å‡½æ•°
             Reward function
        - Î³ âˆˆ [0,1]: æŠ˜æ‰£å› å­
             Discount factor
        
        2. é©¬å°”å¯å¤«æ€§è´¨ Markov Property
        ==================================
        
        "æœªæ¥ç‹¬ç«‹äºè¿‡å»ï¼Œç»™å®šç°åœ¨"
        "The future is independent of the past given the present"
        
        æ•°å­¦è¡¨è¿°ï¼š
        P[S_{t+1} | S_t, A_t, S_{t-1}, A_{t-1}, ..., S_0, A_0] = P[S_{t+1} | S_t, A_t]
        
        è¿™ä¸ªæ€§è´¨æå¤§ç®€åŒ–äº†é—®é¢˜ï¼
        This property greatly simplifies the problem!
        
        3. è½¨è¿¹å’Œå›æŠ¥ Trajectory and Return
        =====================================
        
        è½¨è¿¹ Trajectory:
        Ï„ = S_0, A_0, R_1, S_1, A_1, R_2, ..., S_T
        
        å›æŠ¥ Return (å…³é”®æ¦‚å¿µï¼):
        G_t = R_{t+1} + Î³R_{t+2} + Î³Â²R_{t+3} + ... 
            = Î£_{k=0}^âˆ Î³^k R_{t+k+1}
        
        æŠ˜æ‰£å› å­Î³çš„ä½œç”¨ï¼š
        Role of discount factor Î³:
        - Î³ = 0: åªå…³å¿ƒå³æ—¶å¥–åŠ±ï¼ˆçŸ­è§†ï¼‰
                Only care about immediate reward (myopic)
        - Î³ = 1: æ‰€æœ‰å¥–åŠ±åŒç­‰é‡è¦ï¼ˆå¯èƒ½ä¸æ”¶æ•›ï¼‰
                All rewards equally important (may not converge)
        - 0 < Î³ < 1: å¹³è¡¡å³æ—¶å’Œæœªæ¥å¥–åŠ±
                    Balance immediate and future rewards
        
        4. ç­–ç•¥ Policy
        ===============
        
        ç¡®å®šæ€§ç­–ç•¥ Deterministic policy:
        Ï€: S â†’ A
        
        éšæœºç­–ç•¥ Stochastic policy:
        Ï€(a|s) = P[A_t = a | S_t = s]
        
        æ»¡è¶³ï¼šÎ£_a Ï€(a|s) = 1, âˆ€s âˆˆ S
        
        5. ä»·å€¼å‡½æ•° Value Functions (æ ¸å¿ƒï¼)
        =====================================
        
        çŠ¶æ€ä»·å€¼å‡½æ•° State-value function:
        v_Ï€(s) = E_Ï€[G_t | S_t = s]
               = E_Ï€[Î£_{k=0}^âˆ Î³^k R_{t+k+1} | S_t = s]
        
        åŠ¨ä½œä»·å€¼å‡½æ•° Action-value function:
        q_Ï€(s,a) = E_Ï€[G_t | S_t = s, A_t = a]
                 = E_Ï€[Î£_{k=0}^âˆ Î³^k R_{t+k+1} | S_t = s, A_t = a]
        
        å…³ç³» Relationship:
        v_Ï€(s) = Î£_a Ï€(a|s) q_Ï€(s,a)
        
        6. è´å°”æ›¼æ–¹ç¨‹ Bellman Equations (æœ€é‡è¦ï¼)
        ===========================================
        
        è´å°”æ›¼æœŸæœ›æ–¹ç¨‹ Bellman Expectation Equation:
        
        å¯¹äºv_Ï€ï¼š
        v_Ï€(s) = Î£_a Ï€(a|s) Î£_{s',r} p(s',r|s,a)[r + Î³v_Ï€(s')]
        
        å¯¹äºq_Ï€ï¼š
        q_Ï€(s,a) = Î£_{s',r} p(s',r|s,a)[r + Î³Î£_{a'} Ï€(a'|s')q_Ï€(s',a')]
        
        è¿™äº›æ–¹ç¨‹æ­ç¤ºäº†ä»·å€¼å‡½æ•°çš„é€’å½’ç»“æ„ï¼
        These equations reveal the recursive structure of value functions!
        
        7. æœ€ä¼˜æ€§ Optimality
        ====================
        
        æœ€ä¼˜çŠ¶æ€ä»·å€¼å‡½æ•°ï¼š
        v*(s) = max_Ï€ v_Ï€(s), âˆ€s âˆˆ S
        
        æœ€ä¼˜åŠ¨ä½œä»·å€¼å‡½æ•°ï¼š
        q*(s,a) = max_Ï€ q_Ï€(s,a), âˆ€s âˆˆ S, a âˆˆ A
        
        è´å°”æ›¼æœ€ä¼˜æ–¹ç¨‹ Bellman Optimality Equation:
        v*(s) = max_a Î£_{s',r} p(s',r|s,a)[r + Î³v*(s')]
        q*(s,a) = Î£_{s',r} p(s',r|s,a)[r + Î³max_{a'} q*(s',a')]
        
        æœ€ä¼˜ç­–ç•¥ï¼š
        Ï€*(a|s) = 1 if a = argmax_a q*(s,a)
                  0 otherwise
        
        8. è§£å†³MDPçš„æ–¹æ³• Methods to Solve MDP
        =======================================
        
        å·²çŸ¥æ¨¡å‹ï¼ˆPå’ŒRå·²çŸ¥ï¼‰ï¼š
        Known model (P and R known):
        - åŠ¨æ€è§„åˆ’ Dynamic Programming (Chapter 3)
        - çº¿æ€§è§„åˆ’ Linear Programming
        
        æœªçŸ¥æ¨¡å‹ï¼ˆéœ€è¦å­¦ä¹ ï¼‰ï¼š
        Unknown model (need to learn):
        - è’™ç‰¹å¡æ´›æ–¹æ³• Monte Carlo Methods (Chapter 4)
        - æ—¶åºå·®åˆ†å­¦ä¹  Temporal-Difference Learning (Chapter 5)
        - è§„åˆ’ä¸å­¦ä¹ ç»“åˆ Planning and Learning (Chapter 8)
        """)
    
    @staticmethod
    def demonstrate_value_iteration():
        """
        æ¼”ç¤ºä»·å€¼å‡½æ•°çš„è¿­ä»£è®¡ç®—
        Demonstrate iterative calculation of value function
        
        è¿™æ˜¯ç†è§£è´å°”æ›¼æ–¹ç¨‹çš„å…³é”®ï¼
        This is key to understanding Bellman equations!
        """
        print("\n" + "="*80)
        print("æ¼”ç¤ºï¼šä»·å€¼å‡½æ•°è¿­ä»£")
        print("Demo: Value Function Iteration")
        print("="*80)
        
        # åˆ›å»ºç®€å•çš„2çŠ¶æ€MDP
        print("\nç®€å•2çŠ¶æ€MDPç¤ºä¾‹ï¼š")
        print("Simple 2-state MDP example:")
        print("""
        çŠ¶æ€ States: S = {s1, s2}
        åŠ¨ä½œ Actions: A = {a1, a2}
        
        è½¬ç§»æ¦‚ç‡ Transition probabilities:
        P(s1|s1,a1) = 0.7, P(s2|s1,a1) = 0.3
        P(s1|s1,a2) = 0.4, P(s2|s1,a2) = 0.6
        P(s1|s2,a1) = 0.5, P(s2|s2,a1) = 0.5
        P(s1|s2,a2) = 0.2, P(s2|s2,a2) = 0.8
        
        å¥–åŠ± Rewards:
        R(s1,a1) = 1, R(s1,a2) = 0
        R(s2,a1) = 0, R(s2,a2) = 2
        
        ç­–ç•¥ Policy: Ï€(a1|s1) = 0.5, Ï€(a2|s1) = 0.5
                    Ï€(a1|s2) = 0.3, Ï€(a2|s2) = 0.7
        
        æŠ˜æ‰£å› å­ Discount: Î³ = 0.9
        """)
        
        # åˆå§‹åŒ–
        gamma = 0.9
        
        # è½¬ç§»æ¦‚ç‡
        P = {
            ('s1', 'a1'): [('s1', 0.7), ('s2', 0.3)],
            ('s1', 'a2'): [('s1', 0.4), ('s2', 0.6)],
            ('s2', 'a1'): [('s1', 0.5), ('s2', 0.5)],
            ('s2', 'a2'): [('s1', 0.2), ('s2', 0.8)]
        }
        
        # å¥–åŠ±
        R = {
            ('s1', 'a1'): 1,
            ('s1', 'a2'): 0,
            ('s2', 'a1'): 0,
            ('s2', 'a2'): 2
        }
        
        # ç­–ç•¥
        pi = {
            ('s1', 'a1'): 0.5,
            ('s1', 'a2'): 0.5,
            ('s2', 'a1'): 0.3,
            ('s2', 'a2'): 0.7
        }
        
        # ä»·å€¼å‡½æ•°åˆå§‹åŒ–
        V = {'s1': 0.0, 's2': 0.0}
        
        print("\nä»·å€¼è¿­ä»£è¿‡ç¨‹ï¼š")
        print("Value iteration process:")
        print("-" * 40)
        
        # è¿­ä»£è®¡ç®—
        for iteration in range(10):
            V_new = {}
            
            for s in ['s1', 's2']:
                v = 0
                for a in ['a1', 'a2']:
                    # è®¡ç®—q(s,a)
                    q = R[(s, a)]
                    for s_next, p_trans in P[(s, a)]:
                        q += gamma * p_trans * V[s_next]
                    
                    # åŠ æƒbyç­–ç•¥
                    v += pi[(s, a)] * q
                
                V_new[s] = v
            
            # æ‰“å°è¿›åº¦
            print(f"Iteration {iteration + 1}:")
            print(f"  V(s1) = {V_new['s1']:.4f}, V(s2) = {V_new['s2']:.4f}")
            print(f"  Change: {abs(V_new['s1'] - V['s1']) + abs(V_new['s2'] - V['s2']):.6f}")
            
            V = V_new
        
        print("\næ”¶æ•›çš„ä»·å€¼å‡½æ•°ï¼š")
        print("Converged value function:")
        print(f"v_Ï€(s1) = {V['s1']:.4f}")
        print(f"v_Ï€(s2) = {V['s2']:.4f}")
        
        print("""
        è§‚å¯Ÿ Observations:
        1. ä»·å€¼å‡½æ•°é€šè¿‡è¿­ä»£é€æ¸æ”¶æ•›
           Value function gradually converges through iteration
        2. æ¯æ¬¡è¿­ä»£åº”ç”¨è´å°”æ›¼æœŸæœ›æ–¹ç¨‹
           Each iteration applies Bellman expectation equation
        3. æ”¶æ•›é€Ÿåº¦å–å†³äºÎ³å’ŒMDPç»“æ„
           Convergence speed depends on Î³ and MDP structure
        """)


# ================================================================================
# ç¬¬2.1.6èŠ‚ï¼šMDPç¤ºä¾‹ - å›æ”¶æœºå™¨äºº
# Section 2.1.6: MDP Example - Recycling Robot
# ================================================================================

class RecyclingRobot(MDPEnvironment):
    """
    å›æ”¶æœºå™¨äººç¤ºä¾‹ï¼ˆSutton & Barto ä¹¦ä¸­ä¾‹å­ï¼‰
    Recycling Robot Example (from Sutton & Barto book)
    
    åœºæ™¯æè¿° Scenario Description:
    ä¸€ä¸ªç§»åŠ¨æœºå™¨äººçš„å·¥ä½œæ˜¯åœ¨åŠå…¬å®¤æ”¶é›†ç©ºç½å­ã€‚
    æœºå™¨äººæœ‰å……ç”µç«™ï¼Œç”µæ± ç”µé‡å†³å®šå…¶çŠ¶æ€ã€‚
    A mobile robot's job is to collect empty cans in an office.
    The robot has a charging station, battery level determines its state.
    
    è¿™ä¸ªä¾‹å­å±•ç¤ºäº†ï¼š
    This example demonstrates:
    1. è¿ç»­å†³ç­–é—®é¢˜
       Sequential decision problem
    2. é£é™©ä¸æ”¶ç›Šæƒè¡¡
       Risk-reward trade-off
    3. é•¿æœŸvsçŸ­æœŸè€ƒè™‘
       Long-term vs short-term considerations
    """
    
    def __init__(self):
        """åˆå§‹åŒ–å›æ”¶æœºå™¨äººç¯å¢ƒ"""
        super().__init__(name="Recycling Robot")
        
        # å®šä¹‰çŠ¶æ€ç©ºé—´
        # Define state space
        self.state_space = [
            State(id='high', info={'description': 'é«˜ç”µé‡'}),  # High battery
            State(id='low', info={'description': 'ä½ç”µé‡'})    # Low battery
        ]
        
        # å®šä¹‰åŠ¨ä½œç©ºé—´
        # Define action space
        self.action_space = [
            Action(id='search', name='æœç´¢åƒåœ¾'),  # Search for cans
            Action(id='wait', name='ç­‰å¾…'),        # Wait
            Action(id='recharge', name='å……ç”µ')     # Recharge
        ]
        
        # è®¾ç½®è½¬ç§»æ¦‚ç‡
        # Set transition probabilities
        self._setup_dynamics()
        
        # åˆå§‹çŠ¶æ€
        self.initial_state = self.state_space[0]  # Start with high battery
        
        logger.info("åˆå§‹åŒ–å›æ”¶æœºå™¨äººç¯å¢ƒå®Œæˆ")
    
    def _setup_dynamics(self):
        """
        è®¾ç½®ç¯å¢ƒåŠ¨æ€
        Setup environment dynamics
        
        è¿™å®šä¹‰äº†æœºå™¨äººä¸–ç•Œçš„è§„åˆ™ï¼
        This defines the rules of the robot's world!
        """
        high, low = self.state_space
        search, wait, recharge = self.action_space
        
        # é«˜ç”µé‡çŠ¶æ€çš„è½¬ç§»
        # Transitions from high battery state
        
        # æœç´¢ï¼šå¯èƒ½ä¿æŒé«˜ç”µé‡æˆ–é™åˆ°ä½ç”µé‡
        # Search: may stay high or drop to low
        self.P.set_probability(high, search, high, 2.0, 0.7)  # æˆåŠŸæ‰¾åˆ°ï¼Œä¿æŒé«˜ç”µé‡
        self.P.set_probability(high, search, low, 2.0, 0.3)   # æˆåŠŸæ‰¾åˆ°ï¼Œä½†ç”µé‡é™ä½
        
        # ç­‰å¾…ï¼šä¿æŒé«˜ç”µé‡
        # Wait: stay high
        self.P.set_probability(high, wait, high, 0.5, 1.0)
        
        # ä½ç”µé‡çŠ¶æ€çš„è½¬ç§»
        # Transitions from low battery state
        
        # æœç´¢ï¼šæœ‰é£é™©è€—å°½ç”µé‡
        # Search: risk of depleting battery
        self.P.set_probability(low, search, high, -3.0, 0.3)  # è€—å°½ç”µé‡ï¼Œè¢«æ•‘æ´
        self.P.set_probability(low, search, low, 1.0, 0.7)    # æ‰¾åˆ°åƒåœ¾ï¼Œä¿æŒä½ç”µé‡
        
        # ç­‰å¾…ï¼šä¿æŒä½ç”µé‡
        # Wait: stay low
        self.P.set_probability(low, wait, low, 0.5, 1.0)
        
        # å……ç”µï¼šå›åˆ°é«˜ç”µé‡
        # Recharge: back to high
        self.P.set_probability(low, recharge, high, 0.0, 1.0)
        
        # æ³¨æ„ï¼šé«˜ç”µé‡çŠ¶æ€ä¸èƒ½æ‰§è¡Œå……ç”µåŠ¨ä½œï¼ˆçº¦æŸï¼‰
        # Note: Cannot recharge in high battery state (constraint)
    
    def reset(self) -> State:
        """é‡ç½®åˆ°åˆå§‹çŠ¶æ€"""
        self.current_state = self.initial_state
        self.step_count = 0
        self.episode_count += 1
        
        logger.info(f"Episode {self.episode_count}: Robot reset to high battery")
        return self.current_state
    
    def step(self, action: Action) -> Tuple[State, float, bool, Dict]:
        """
        æ‰§è¡ŒåŠ¨ä½œ
        Execute action
        
        å±•ç¤ºäº†MDPçš„æ ¸å¿ƒå¾ªç¯ï¼
        Demonstrates the core MDP loop!
        """
        if self.current_state is None:
            raise ValueError("Environment not reset. Call reset() first.")
        
        # æ£€æŸ¥åŠ¨ä½œåˆæ³•æ€§
        # Check action validity
        if self.current_state.id == 'high' and action.id == 'recharge':
            raise ValueError("Cannot recharge when battery is high!")
        
        # ä»è½¬ç§»æ¦‚ç‡ä¸­é‡‡æ ·
        # Sample from transition probability
        next_state, reward = self.P.sample(self.current_state, action)
        
        # æ›´æ–°çŠ¶æ€
        self.current_state = next_state
        self.step_count += 1
        
        # è¿™ä¸ªä»»åŠ¡æ˜¯æŒç»­çš„ï¼Œæ²¡æœ‰ç»ˆæ­¢çŠ¶æ€
        # This is a continuing task, no terminal state
        done = False
        
        # é¢å¤–ä¿¡æ¯
        info = {
            'battery_level': self.current_state.id,
            'action_taken': action.name,
            'reward_received': reward
        }
        
        logger.debug(f"Step {self.step_count}: {action.name} -> "
                    f"Battery: {next_state.id}, Reward: {reward}")
        
        return next_state, reward, done, info
    
    def render(self, mode: str = 'human'):
        """å¯è§†åŒ–å½“å‰çŠ¶æ€"""
        if mode == 'human':
            battery_icon = "ğŸ”‹" if self.current_state.id == 'high' else "ğŸª«"
            print(f"\nå›æ”¶æœºå™¨äººçŠ¶æ€ Robot Status:")
            print(f"  ç”µé‡ Battery: {battery_icon} {self.current_state.id}")
            print(f"  æ­¥æ•° Steps: {self.step_count}")
            print(f"  å¯é€‰åŠ¨ä½œ Available actions:")
            
            for action in self.action_space:
                if not (self.current_state.id == 'high' and action.id == 'recharge'):
                    # æ˜¾ç¤ºæ¯ä¸ªåŠ¨ä½œçš„æœŸæœ›ç»“æœ
                    transitions = self.P.get_transitions(self.current_state, action)
                    exp_reward = sum(r * p for _, r, p in transitions)
                    print(f"    - {action.name}: æœŸæœ›å¥–åŠ± Expected reward = {exp_reward:.2f}")


# ================================================================================
# ç¤ºä¾‹è¿è¡Œ
# Example Run
# ================================================================================

def main():
    """
    è¿è¡ŒMDPæ¡†æ¶æ¼”ç¤º
    Run MDP framework demonstration
    """
    print("\n" + "="*80)
    print("ç¬¬2.1èŠ‚ï¼šMDPæ¡†æ¶")
    print("Section 2.1: MDP Framework")
    print("="*80)
    
    # 1. è§£é‡ŠMDPæ•°å­¦
    MDPMathematics.explain_mdp_formulation()
    
    # 2. æ¼”ç¤ºä»·å€¼è¿­ä»£
    MDPMathematics.demonstrate_value_iteration()
    
    # 3. è¿è¡Œå›æ”¶æœºå™¨äººç¤ºä¾‹
    print("\n" + "="*80)
    print("å›æ”¶æœºå™¨äººç¤ºä¾‹")
    print("Recycling Robot Example")
    print("="*80)
    
    # åˆ›å»ºç¯å¢ƒ
    env = RecyclingRobot()
    
    # ç®€å•æ¼”ç¤º
    print("\næ¼”ç¤ºéšæœºç­–ç•¥ï¼š")
    print("Demo random policy:")
    
    state = env.reset()
    env.render()
    
    for step in range(5):
        # è·å–åˆæ³•åŠ¨ä½œ
        if state.id == 'high':
            valid_actions = [a for a in env.action_space if a.id != 'recharge']
        else:
            valid_actions = env.action_space
        
        # éšæœºé€‰æ‹©
        action = np.random.choice(valid_actions)
        
        print(f"\nStep {step + 1}: é€‰æ‹©åŠ¨ä½œ Choose action: {action.name}")
        state, reward, done, info = env.step(action)
        print(f"  ç»“æœ Result: å¥–åŠ±={reward:.1f}, æ–°çŠ¶æ€={state.id}")
        
        env.render()
    
    print("\n" + "="*80)
    print("MDPæ¡†æ¶æ¼”ç¤ºå®Œæˆï¼")
    print("MDP Framework Demo Complete!")
    print("\nä¸‹ä¸€æ­¥ï¼šå®ç°æ™ºèƒ½ä½“-ç¯å¢ƒæ¥å£")
    print("Next: Implement Agent-Environment Interface")
    print("="*80)


if __name__ == "__main__":
    main()