"""
================================================================================
ç¬¬4.4èŠ‚ï¼šé‡è¦æ€§é‡‡æ · - Off-Policyå­¦ä¹ çš„æ•°å­¦åŸºç¡€
Section 4.4: Importance Sampling - Mathematical Foundation of Off-Policy Learning
================================================================================

é‡è¦æ€§é‡‡æ ·æ˜¯off-policyå­¦ä¹ çš„æ ¸å¿ƒæŠ€æœ¯ã€‚
Importance sampling is the core technique for off-policy learning.

æ ¸å¿ƒé—®é¢˜ï¼šå¦‚ä½•ç”¨ä¸€ä¸ªåˆ†å¸ƒçš„æ ·æœ¬ä¼°è®¡å¦ä¸€ä¸ªåˆ†å¸ƒçš„æœŸæœ›ï¼Ÿ
Core problem: How to estimate expectation under one distribution using samples from another?

æ•°å­¦åŸç†ï¼š
Mathematical principle:
E_Ï€[X] = E_b[Ï Ã— X]
å…¶ä¸­ Ï = Ï€(Â·)/b(Â·) æ˜¯é‡è¦æ€§é‡‡æ ·æ¯”ç‡
where Ï = Ï€(Â·)/b(Â·) is the importance sampling ratio

ä¸¤ç§ä¸»è¦å˜ä½“ï¼š
Two main variants:
1. æ™®é€šé‡è¦æ€§é‡‡æ ·ï¼ˆOrdinary ISï¼‰
   - æ— åä½†é«˜æ–¹å·®
     Unbiased but high variance
   - V^Ï€(s) = (1/n)Î£áµ¢ Ïáµ¢Gáµ¢

2. åŠ æƒé‡è¦æ€§é‡‡æ ·ï¼ˆWeighted ISï¼‰
   - æœ‰åä½†ä½æ–¹å·®
     Biased but lower variance
   - V^Ï€(s) = Î£áµ¢(Ïáµ¢Gáµ¢)/Î£áµ¢Ïáµ¢

æƒè¡¡ï¼š
Trade-offs:
- åå·® vs æ–¹å·®
  Bias vs Variance
- æ”¶æ•›é€Ÿåº¦ vs ç¨³å®šæ€§
  Convergence speed vs Stability
- ç†è®ºæ€§è´¨ vs å®è·µæ€§èƒ½
  Theoretical properties vs Practical performance

è¿™æ˜¯é€šå‘ç°ä»£off-policyæ–¹æ³•ï¼ˆå¦‚Q-learningï¼‰çš„æ¡¥æ¢ï¼
This is the bridge to modern off-policy methods like Q-learning!
"""

import numpy as np
from typing import List, Dict, Tuple, Optional, Any, Callable, Union
from dataclasses import dataclass, field
from collections import defaultdict, deque
from abc import ABC, abstractmethod
import logging
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import seaborn as sns
from scipy import stats
import time

# å¯¼å…¥åŸºç¡€ç»„ä»¶
import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent))

from src.ch03_finite_mdp.mdp_framework import State, Action, MDPEnvironment
from src.ch03_finite_mdp.policies_and_values import (
    Policy, StateValueFunction, ActionValueFunction,
    StochasticPolicy, DeterministicPolicy
)
from ch04_monte_carlo.mc_foundations import (
    Episode, Experience, Return, MCStatistics
)
from ch04_monte_carlo.mc_control import EpsilonGreedyPolicy

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


# ================================================================================
# ç¬¬4.4.1èŠ‚ï¼šé‡è¦æ€§é‡‡æ ·åŸºç¡€
# Section 4.4.1: Importance Sampling Fundamentals
# ================================================================================

class ImportanceSamplingTheory:
    """
    é‡è¦æ€§é‡‡æ ·ç†è®º
    Importance Sampling Theory
    
    å±•ç¤ºé‡è¦æ€§é‡‡æ ·çš„æ•°å­¦åŸç†å’Œæ€§è´¨
    Demonstrate mathematical principles and properties of importance sampling
    
    æ ¸å¿ƒæ€æƒ³ï¼šæœŸæœ›çš„å˜æ¢
    Core idea: Transformation of expectation
    
    é—®é¢˜è®¾å®šï¼š
    Problem setup:
    - æƒ³è¦ï¼šE_Ï€[f(X)] åœ¨ç›®æ ‡åˆ†å¸ƒÏ€ä¸‹çš„æœŸæœ›
      Want: E_Ï€[f(X)] expectation under target distribution Ï€
    - æ‹¥æœ‰ï¼šæ¥è‡ªè¡Œä¸ºåˆ†å¸ƒbçš„æ ·æœ¬
      Have: Samples from behavior distribution b
    
    è§£å†³æ–¹æ¡ˆï¼š
    Solution:
    E_Ï€[f(X)] = âˆ« f(x)Ï€(x)dx
             = âˆ« f(x)[Ï€(x)/b(x)]b(x)dx
             = E_b[f(X) Ã— Ï€(X)/b(X)]
             = E_b[f(X) Ã— Ï(X)]
    
    å…¶ä¸­ Ï(X) = Ï€(X)/b(X) æ˜¯é‡è¦æ€§æƒé‡
    where Ï(X) = Ï€(X)/b(X) is importance weight
    
    å…³é”®è¦æ±‚ï¼š
    Key requirements:
    1. è¦†ç›–æ¡ä»¶ï¼šb(x) > 0 whenever Ï€(x) > 0
       Coverage: b(x) > 0 whenever Ï€(x) > 0
    2. å·²çŸ¥æ¯”ç‡ï¼šéœ€è¦çŸ¥é“Ï€(x)/b(x)
       Known ratio: Need to know Ï€(x)/b(x)
    
    åœ¨RLä¸­çš„åº”ç”¨ï¼š
    Application in RL:
    - Xæ˜¯è½¨è¿¹Ï„
      X is trajectory Ï„
    - f(X)æ˜¯å›æŠ¥G(Ï„)
      f(X) is return G(Ï„)
    - Ï€æ˜¯ç›®æ ‡ç­–ç•¥
      Ï€ is target policy
    - bæ˜¯è¡Œä¸ºç­–ç•¥
      b is behavior policy
    """
    
    @staticmethod
    def demonstrate_basic_principle():
        """
        æ¼”ç¤ºé‡è¦æ€§é‡‡æ ·åŸºæœ¬åŸç†
        Demonstrate basic principle of importance sampling
        
        ç”¨ç®€å•ä¾‹å­å±•ç¤ºå¦‚ä½•ç”¨ä¸€ä¸ªåˆ†å¸ƒä¼°è®¡å¦ä¸€ä¸ªåˆ†å¸ƒ
        Show how to estimate one distribution using another with simple example
        """
        print("\n" + "="*80)
        print("é‡è¦æ€§é‡‡æ ·åŸºæœ¬åŸç†æ¼”ç¤º")
        print("Importance Sampling Basic Principle Demo")
        print("="*80)
        
        # ç®€å•ä¾‹å­ï¼šä¼°è®¡æ­£æ€åˆ†å¸ƒçš„æœŸæœ›ï¼Œç”¨å¦ä¸€ä¸ªæ­£æ€åˆ†å¸ƒé‡‡æ ·
        # Simple example: Estimate mean of normal, sample from another normal
        
        # ç›®æ ‡åˆ†å¸ƒï¼šN(5, 1)
        # Target distribution: N(5, 1)
        target_mean, target_std = 5.0, 1.0
        
        # è¡Œä¸ºåˆ†å¸ƒï¼šN(3, 2)
        # Behavior distribution: N(3, 2)
        behavior_mean, behavior_std = 3.0, 2.0
        
        print(f"\nç›®æ ‡åˆ†å¸ƒ Target: N({target_mean}, {target_std}Â²)")
        print(f"è¡Œä¸ºåˆ†å¸ƒ Behavior: N({behavior_mean}, {behavior_std}Â²)")
        print(f"çœŸå®æœŸæœ› True expectation: {target_mean}")
        
        # ä»è¡Œä¸ºåˆ†å¸ƒé‡‡æ ·
        # Sample from behavior distribution
        n_samples = 10000
        np.random.seed(42)
        samples = np.random.normal(behavior_mean, behavior_std, n_samples)
        
        # æ–¹æ³•1ï¼šç›´æ¥å¹³å‡ï¼ˆé”™è¯¯ï¼ï¼‰
        # Method 1: Direct average (wrong!)
        naive_estimate = np.mean(samples)
        print(f"\nç›´æ¥å¹³å‡ Direct average: {naive_estimate:.3f}")
        print(f"  é”™è¯¯ï¼è¿™æ˜¯è¡Œä¸ºåˆ†å¸ƒçš„æœŸæœ›")
        print(f"  Wrong! This is behavior distribution's mean")
        
        # æ–¹æ³•2ï¼šé‡è¦æ€§é‡‡æ ·ï¼ˆæ­£ç¡®ï¼ï¼‰
        # Method 2: Importance sampling (correct!)
        
        # è®¡ç®—é‡è¦æ€§æƒé‡
        # Compute importance weights
        target_pdf = stats.norm.pdf(samples, target_mean, target_std)
        behavior_pdf = stats.norm.pdf(samples, behavior_mean, behavior_std)
        weights = target_pdf / behavior_pdf
        
        # æ™®é€šISä¼°è®¡
        # Ordinary IS estimate
        is_estimate = np.mean(weights * samples)
        print(f"\næ™®é€šISä¼°è®¡ Ordinary IS: {is_estimate:.3f}")
        
        # åŠ æƒISä¼°è®¡
        # Weighted IS estimate
        weighted_is_estimate = np.sum(weights * samples) / np.sum(weights)
        print(f"åŠ æƒISä¼°è®¡ Weighted IS: {weighted_is_estimate:.3f}")
        
        # åˆ†ææƒé‡
        # Analyze weights
        print(f"\næƒé‡ç»Ÿè®¡ Weight statistics:")
        print(f"  å‡å€¼ Mean: {np.mean(weights):.3f}")
        print(f"  æ ‡å‡†å·® Std: {np.std(weights):.3f}")
        print(f"  æœ€å° Min: {np.min(weights):.3f}")
        print(f"  æœ€å¤§ Max: {np.max(weights):.3f}")
        
        # æœ‰æ•ˆæ ·æœ¬å¤§å°
        # Effective sample size
        ess = np.sum(weights)**2 / np.sum(weights**2)
        print(f"  æœ‰æ•ˆæ ·æœ¬å¤§å° ESS: {ess:.0f} / {n_samples}")
        print(f"  æ•ˆç‡ Efficiency: {ess/n_samples:.2%}")
        
        # å¯è§†åŒ–
        # Visualization
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        
        # å›¾1ï¼šåˆ†å¸ƒæ¯”è¾ƒ
        # Plot 1: Distribution comparison
        ax1 = axes[0, 0]
        x = np.linspace(-2, 10, 1000)
        target_y = stats.norm.pdf(x, target_mean, target_std)
        behavior_y = stats.norm.pdf(x, behavior_mean, behavior_std)
        
        ax1.plot(x, target_y, 'r-', linewidth=2, label='Target Ï€')
        ax1.plot(x, behavior_y, 'b-', linewidth=2, label='Behavior b')
        ax1.fill_between(x, target_y, alpha=0.3, color='red')
        ax1.fill_between(x, behavior_y, alpha=0.3, color='blue')
        ax1.axvline(x=target_mean, color='red', linestyle='--', alpha=0.5)
        ax1.axvline(x=behavior_mean, color='blue', linestyle='--', alpha=0.5)
        ax1.set_xlabel('x')
        ax1.set_ylabel('Probability Density')
        ax1.set_title('Target vs Behavior Distribution')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # å›¾2ï¼šé‡è¦æ€§æƒé‡
        # Plot 2: Importance weights
        ax2 = axes[0, 1]
        weight_func = lambda x: stats.norm.pdf(x, target_mean, target_std) / stats.norm.pdf(x, behavior_mean, behavior_std)
        weights_x = [weight_func(xi) for xi in x]
        ax2.plot(x, weights_x, 'g-', linewidth=2)
        ax2.axhline(y=1.0, color='gray', linestyle='--', alpha=0.5, label='Ï=1')
        ax2.set_xlabel('x')
        ax2.set_ylabel('Importance Weight Ï(x)')
        ax2.set_title('Importance Weights Ï€(x)/b(x)')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        ax2.set_ylim([0, 5])
        
        # å›¾3ï¼šä¼°è®¡æ”¶æ•›
        # Plot 3: Estimation convergence
        ax3 = axes[1, 0]
        
        # è®¡ç®—ç´¯ç§¯ä¼°è®¡
        # Compute cumulative estimates
        cumulative_ordinary = []
        cumulative_weighted = []
        cumulative_naive = []
        
        for i in range(100, n_samples, 100):
            batch_samples = samples[:i]
            batch_weights = target_pdf[:i] / behavior_pdf[:i]
            
            ordinary = np.mean(batch_weights * batch_samples)
            weighted = np.sum(batch_weights * batch_samples) / np.sum(batch_weights)
            naive = np.mean(batch_samples)
            
            cumulative_ordinary.append(ordinary)
            cumulative_weighted.append(weighted)
            cumulative_naive.append(naive)
        
        x_axis = range(100, n_samples, 100)
        ax3.plot(x_axis, cumulative_ordinary, 'g-', alpha=0.7, label='Ordinary IS')
        ax3.plot(x_axis, cumulative_weighted, 'b-', alpha=0.7, label='Weighted IS')
        ax3.plot(x_axis, cumulative_naive, 'gray', alpha=0.5, label='Naive (wrong)')
        ax3.axhline(y=target_mean, color='red', linestyle='--', linewidth=2, label='True value')
        ax3.set_xlabel('Number of Samples')
        ax3.set_ylabel('Estimate')
        ax3.set_title('Convergence of Estimates')
        ax3.legend()
        ax3.grid(True, alpha=0.3)
        
        # å›¾4ï¼šæƒé‡åˆ†å¸ƒ
        # Plot 4: Weight distribution
        ax4 = axes[1, 1]
        ax4.hist(weights[:1000], bins=50, alpha=0.7, color='purple', density=True)
        ax4.axvline(x=1.0, color='red', linestyle='--', linewidth=2, label='Ï=1')
        ax4.set_xlabel('Weight Value')
        ax4.set_ylabel('Frequency')
        ax4.set_title('Distribution of Importance Weights')
        ax4.legend()
        ax4.grid(True, alpha=0.3)
        
        plt.suptitle('Importance Sampling Principle Demonstration', fontsize=14, fontweight='bold')
        plt.tight_layout()
        
        print("\n" + "="*60)
        print("å…³é”®æ´å¯Ÿ Key Insights:")
        print("="*60)
        print("""
        1. é‡è¦æ€§æƒé‡ä¿®æ­£åˆ†å¸ƒå·®å¼‚
           Importance weights correct distribution mismatch
           
        2. æƒé‡æ–¹å·®å½±å“ä¼°è®¡è´¨é‡
           Weight variance affects estimate quality
           
        3. åˆ†å¸ƒå·®å¼‚è¶Šå¤§ï¼Œæ•ˆç‡è¶Šä½
           Larger distribution difference, lower efficiency
           
        4. åŠ æƒISç”¨åå·®æ¢å–æ›´ä½æ–¹å·®
           Weighted IS trades bias for lower variance
        """)
        
        return fig


# ================================================================================
# ç¬¬4.4.2èŠ‚ï¼šé‡è¦æ€§é‡‡æ ·åŸºç±»
# Section 4.4.2: Importance Sampling Base Class
# ================================================================================

class ImportanceSampling(ABC):
    """
    é‡è¦æ€§é‡‡æ ·åŸºç±»
    Importance Sampling Base Class
    
    å®šä¹‰ISæ–¹æ³•çš„å…±åŒæ¥å£
    Define common interface for IS methods
    
    è®¾è®¡è€ƒè™‘ï¼š
    Design considerations:
    1. æ”¯æŒæ™®é€šå’ŒåŠ æƒIS
       Support ordinary and weighted IS
    2. å¢é‡å’Œæ‰¹é‡æ›´æ–°
       Incremental and batch updates
    3. è¯Šæ–­å’Œåˆ†æå·¥å…·
       Diagnostic and analysis tools
    4. æ–¹å·®å‡å°‘æŠ€æœ¯
       Variance reduction techniques
    """
    
    def __init__(self,
                 env: MDPEnvironment,
                 target_policy: Policy,
                 behavior_policy: Policy,
                 gamma: float = 1.0):
        """
        åˆå§‹åŒ–é‡è¦æ€§é‡‡æ ·
        Initialize importance sampling
        
        Args:
            env: ç¯å¢ƒ
            target_policy: ç›®æ ‡ç­–ç•¥Ï€
            behavior_policy: è¡Œä¸ºç­–ç•¥b
            gamma: æŠ˜æ‰£å› å­
        """
        self.env = env
        self.target_policy = target_policy
        self.behavior_policy = behavior_policy
        self.gamma = gamma
        
        # ä»·å€¼å‡½æ•°ä¼°è®¡
        # Value function estimates
        self.V = StateValueFunction(env.state_space, initial_value=0.0)
        self.Q = ActionValueFunction(env.state_space, env.action_space, initial_value=0.0)
        
        # ç»Ÿè®¡
        # Statistics
        self.statistics = MCStatistics()
        
        # ISæ¯”ç‡è®°å½•
        # IS ratio records
        self.is_ratios: List[float] = []
        self.trajectory_ratios: List[List[float]] = []
        
        # è®¿é—®è®¡æ•°
        # Visit counts
        self.state_visits = defaultdict(int)
        self.sa_visits = defaultdict(int)
        
        logger.info("åˆå§‹åŒ–é‡è¦æ€§é‡‡æ ·")
    
    @abstractmethod
    def update_value(self, episode: Episode):
        """
        æ›´æ–°ä»·å€¼ä¼°è®¡ï¼ˆå­ç±»å®ç°ï¼‰
        Update value estimate (implemented by subclasses)
        """
        pass
    
    def compute_trajectory_ratio(self, episode: Episode) -> float:
        """
        è®¡ç®—æ•´æ¡è½¨è¿¹çš„é‡è¦æ€§æ¯”ç‡
        Compute importance ratio for entire trajectory
        
        Ï(Ï„) = âˆ_t [Ï€(aâ‚œ|sâ‚œ)/b(aâ‚œ|sâ‚œ)]
        
        è¿™æ˜¯æœ€åŸºæœ¬çš„ISè®¡ç®—
        This is the most basic IS computation
        """
        ratio = 1.0
        
        for exp in episode.experiences:
            # è·å–åŠ¨ä½œæ¦‚ç‡
            # Get action probabilities
            target_probs = self.target_policy.get_action_probabilities(
                exp.state
            )
            behavior_probs = self.behavior_policy.get_action_probabilities(
                exp.state
            )
            
            target_prob = target_probs.get(exp.action, 0.0)
            behavior_prob = behavior_probs.get(exp.action, 1e-10)  # é¿å…é™¤é›¶
            
            ratio *= target_prob / behavior_prob
            
            # å¦‚æœæ¯”ç‡ä¸º0ï¼Œæ•´æ¡è½¨è¿¹æƒé‡ä¸º0
            # If ratio is 0, entire trajectory has weight 0
            if ratio == 0:
                break
        
        return ratio
    
    def compute_per_step_ratios(self, episode: Episode) -> List[float]:
        """
        è®¡ç®—æ¯æ­¥çš„ç´¯ç§¯é‡è¦æ€§æ¯”ç‡
        Compute cumulative importance ratios per step
        
        Ïâ‚œ:T = âˆ_{k=t}^T [Ï€(aâ‚–|sâ‚–)/b(aâ‚–|sâ‚–)]
        
        ç”¨äºper-decision IS
        Used for per-decision IS
        """
        ratios = []
        cumulative_ratio = 1.0
        
        # åå‘è®¡ç®—ï¼ˆä»åå‘å‰ç´¯ç§¯ï¼‰
        # Backward computation (accumulate from back to front)
        for exp in reversed(episode.experiences):
            target_probs = self.target_policy.get_action_probabilities(
                exp.state
            )
            behavior_probs = self.behavior_policy.get_action_probabilities(
                exp.state
            )
            
            target_prob = target_probs.get(exp.action, 0.0)
            behavior_prob = behavior_probs.get(exp.action, 1e-10)
            
            cumulative_ratio *= target_prob / behavior_prob
            ratios.append(cumulative_ratio)
        
        ratios.reverse()
        return ratios
    
    def diagnose_coverage(self):
        """
        è¯Šæ–­è¦†ç›–æ€§æ¡ä»¶
        Diagnose coverage condition
        
        æ£€æŸ¥bæ˜¯å¦å……åˆ†è¦†ç›–Ï€
        Check if b adequately covers Ï€
        """
        print("\n" + "="*60)
        print("è¦†ç›–æ€§è¯Šæ–­")
        print("Coverage Diagnosis")
        print("="*60)
        
        coverage_violations = 0
        total_checks = 0
        
        for state in self.env.state_space:
            if state.is_terminal:
                continue
            
            target_probs = self.target_policy.get_action_probabilities(
                state
            )
            behavior_probs = self.behavior_policy.get_action_probabilities(
                state
            )
            
            for action in self.env.action_space:
                total_checks += 1
                target_p = target_probs.get(action, 0.0)
                behavior_p = behavior_probs.get(action, 0.0)
                
                if target_p > 0 and behavior_p == 0:
                    coverage_violations += 1
                    print(f"  è¿å: Ï€({action.id}|{state.id}) = {target_p:.3f}, "
                          f"b({action.id}|{state.id}) = 0")
        
        if coverage_violations > 0:
            print(f"\nâš ï¸ å‘ç°{coverage_violations}ä¸ªè¦†ç›–æ€§è¿åï¼")
            print(f"   Found {coverage_violations} coverage violations!")
            print("   ISä¼°è®¡å¯èƒ½æœ‰åæˆ–æ— é™æ–¹å·®")
            print("   IS estimates may be biased or have infinite variance")
        else:
            print("âœ“ è¦†ç›–æ€§æ¡ä»¶æ»¡è¶³")
            print("  Coverage condition satisfied")
        
        print(f"\næ£€æŸ¥çš„(s,a)å¯¹: {total_checks}")
        print(f"è¿åæ¯”ä¾‹: {coverage_violations/total_checks:.2%}")
    
    def analyze_variance(self):
        """
        åˆ†æISä¼°è®¡çš„æ–¹å·®
        Analyze variance of IS estimates
        
        å±•ç¤ºISçš„ä¸»è¦é—®é¢˜ï¼šé«˜æ–¹å·®
        Show main problem of IS: high variance
        """
        if not self.is_ratios:
            print("æ²¡æœ‰ISæ¯”ç‡æ•°æ®")
            return
        
        ratios = np.array(self.is_ratios)
        
        print("\n" + "="*60)
        print("é‡è¦æ€§é‡‡æ ·æ–¹å·®åˆ†æ")
        print("Importance Sampling Variance Analysis")
        print("="*60)
        
        print(f"\nISæ¯”ç‡ç»Ÿè®¡:")
        print(f"  æ ·æœ¬æ•°: {len(ratios)}")
        print(f"  å‡å€¼: {np.mean(ratios):.3f}")
        print(f"  æ ‡å‡†å·®: {np.std(ratios):.3f}")
        print(f"  å˜å¼‚ç³»æ•°(CV): {np.std(ratios)/np.mean(ratios):.3f}")
        
        # åˆ†ä½æ•°
        # Quantiles
        quantiles = [0, 0.25, 0.5, 0.75, 0.95, 0.99, 1.0]
        quantile_values = np.quantile(ratios, quantiles)
        
        print(f"\nåˆ†ä½æ•°:")
        for q, v in zip(quantiles, quantile_values):
            print(f"  {q*100:3.0f}%: {v:.3f}")
        
        # æç«¯å€¼åˆ†æ
        # Extreme value analysis
        extreme_threshold = np.mean(ratios) + 3 * np.std(ratios)
        extreme_count = np.sum(ratios > extreme_threshold)
        
        print(f"\næç«¯å€¼ (>å‡å€¼+3Ïƒ): {extreme_count} ({extreme_count/len(ratios):.2%})")
        
        # æœ‰æ•ˆæ ·æœ¬å¤§å°
        # Effective sample size
        if len(ratios) > 0:
            sum_w = np.sum(ratios)
            sum_w2 = np.sum(ratios ** 2)
            if sum_w2 > 0:
                ess = (sum_w ** 2) / sum_w2
                print(f"\næœ‰æ•ˆæ ·æœ¬å¤§å°(ESS): {ess:.1f} / {len(ratios)}")
                print(f"æ•ˆç‡: {ess/len(ratios):.2%}")
        
        print("\n" + "="*40)
        print("æ–¹å·®é—®é¢˜è¯Šæ–­:")
        print("Variance Problem Diagnosis:")
        print("="*40)
        
        cv = np.std(ratios) / np.mean(ratios) if np.mean(ratios) > 0 else float('inf')
        
        if cv > 1:
            print("âš ï¸ é«˜å˜å¼‚ç³»æ•°(CV>1): ä¼°è®¡ä¸ç¨³å®š")
            print("   High CV: Unstable estimates")
        
        if extreme_count / len(ratios) > 0.01:
            print("âš ï¸ è¿‡å¤šæç«¯å€¼: å°‘æ•°æ ·æœ¬ä¸»å¯¼ä¼°è®¡")
            print("   Too many extremes: Few samples dominate")
        
        efficiency = ess / len(ratios) if len(ratios) > 0 else 0
        if efficiency < 0.1:
            print("âš ï¸ ä½æ•ˆç‡(<10%): å¤§éƒ¨åˆ†æ ·æœ¬è¢«æµªè´¹")
            print("   Low efficiency: Most samples wasted")


# ================================================================================
# ç¬¬4.4.3èŠ‚ï¼šæ™®é€šé‡è¦æ€§é‡‡æ ·
# Section 4.4.3: Ordinary Importance Sampling
# ================================================================================

class OrdinaryImportanceSampling(ImportanceSampling):
    """
    æ™®é€šé‡è¦æ€§é‡‡æ ·
    Ordinary Importance Sampling
    
    æœ€ç›´æ¥çš„ISå®ç°
    Most straightforward IS implementation
    
    ä¼°è®¡å™¨ï¼š
    Estimator:
    V^Ï€(s) = (1/n(s)) Î£áµ¢ Ïáµ¢Gáµ¢(s)
    
    å…¶ä¸­ï¼š
    where:
    - n(s)æ˜¯çŠ¶æ€sçš„è®¿é—®æ¬¡æ•°
      n(s) is number of visits to state s
    - Ïáµ¢æ˜¯ç¬¬iæ¬¡è®¿é—®çš„é‡è¦æ€§æ¯”ç‡
      Ïáµ¢ is importance ratio for i-th visit
    - Gáµ¢(s)æ˜¯ä»så¼€å§‹çš„å›æŠ¥
      Gáµ¢(s) is return starting from s
    
    æ€§è´¨ï¼š
    Properties:
    - æ— åï¼šE_b[ÏG] = E_Ï€[G] = v_Ï€(s)
      Unbiased: E_b[ÏG] = E_Ï€[G] = v_Ï€(s)
    - é«˜æ–¹å·®ï¼šVar[ÏG]å¯èƒ½å¾ˆå¤§
      High variance: Var[ÏG] can be large
    - ä¸ç¨³å®šï¼šå°‘æ•°å¤§æƒé‡æ ·æœ¬å¯èƒ½ä¸»å¯¼
      Unstable: Few high-weight samples may dominate
    
    æ•°å­¦è¯æ˜ï¼ˆæ— åæ€§ï¼‰ï¼š
    Mathematical proof (unbiasedness):
    E_b[ÏG] = E_b[Ï€(Ï„)/b(Ï„) Ã— G(Ï„)]
            = Î£_Ï„ [Ï€(Ï„)/b(Ï„) Ã— G(Ï„) Ã— b(Ï„)]
            = Î£_Ï„ Ï€(Ï„) Ã— G(Ï„)
            = E_Ï€[G]
    
    è¿™æ˜¯æœ€çº¯ç²¹çš„ISï¼Œä½†å®è·µä¸­å¸¸æœ‰é—®é¢˜
    This is the purest IS, but often problematic in practice
    """
    
    def __init__(self,
                 env: MDPEnvironment,
                 target_policy: Policy,
                 behavior_policy: Policy,
                 gamma: float = 1.0,
                 first_visit: bool = True):
        """
        åˆå§‹åŒ–æ™®é€šIS
        Initialize ordinary IS
        
        Args:
            env: ç¯å¢ƒ
            target_policy: ç›®æ ‡ç­–ç•¥
            behavior_policy: è¡Œä¸ºç­–ç•¥
            gamma: æŠ˜æ‰£å› å­
            first_visit: æ˜¯å¦ä½¿ç”¨first-visit
        """
        super().__init__(env, target_policy, behavior_policy, gamma)
        
        self.first_visit = first_visit
        
        # è®°å½•æ‰€æœ‰åŠ æƒå›æŠ¥ï¼ˆç”¨äºåˆ†æï¼‰
        # Record all weighted returns (for analysis)
        self.weighted_returns: Dict[str, List[float]] = defaultdict(list)
        
        logger.info(f"åˆå§‹åŒ–æ™®é€šIS: first_visit={first_visit}")
    
    def update_value(self, episode: Episode):
        """
        ä½¿ç”¨æ™®é€šISæ›´æ–°ä»·å€¼
        Update value using ordinary IS
        
        æ ¸å¿ƒï¼šç›´æ¥å¹³å‡åŠ æƒå›æŠ¥
        Core: Direct average of weighted returns
        """
        # è®¡ç®—å›æŠ¥
        # Compute returns
        returns = episode.compute_returns(self.gamma)
        
        # è®¡ç®—æ¯æ­¥çš„ISæ¯”ç‡
        # Compute per-step IS ratios
        ratios = self.compute_per_step_ratios(episode)
        
        if self.first_visit:
            # First-visit
            visited_states = set()
            
            for t, exp in enumerate(episode.experiences):
                if exp.state.id not in visited_states:
                    visited_states.add(exp.state.id)
                    
                    G = returns[t]
                    rho = ratios[t]
                    
                    # åŠ æƒå›æŠ¥
                    # Weighted return
                    weighted_G = rho * G
                    
                    # è®°å½•
                    # Record
                    self.weighted_returns[exp.state.id].append(weighted_G)
                    self.is_ratios.append(rho)
                    self.state_visits[exp.state.id] += 1
                    
                    # æ™®é€šISæ›´æ–°ï¼šç®€å•å¹³å‡
                    # Ordinary IS update: simple average
                    all_weighted = self.weighted_returns[exp.state.id]
                    new_v = np.mean(all_weighted)
                    self.V.set_value(exp.state, new_v)
                    
                    # æ›´æ–°ç»Ÿè®¡
                    # Update statistics
                    self.statistics.update_state_value(exp.state, weighted_G)
        
        else:
            # Every-visit
            for t, exp in enumerate(episode.experiences):
                G = returns[t]
                rho = ratios[t]
                
                weighted_G = rho * G
                
                self.weighted_returns[exp.state.id].append(weighted_G)
                self.is_ratios.append(rho)
                self.state_visits[exp.state.id] += 1
                
                # æ›´æ–°ä»·å€¼
                # Update value
                all_weighted = self.weighted_returns[exp.state.id]
                new_v = np.mean(all_weighted)
                self.V.set_value(exp.state, new_v)
                
                self.statistics.update_state_value(exp.state, weighted_G)
    
    def analyze_estimator_properties(self):
        """
        åˆ†ææ™®é€šISä¼°è®¡å™¨çš„æ€§è´¨
        Analyze properties of ordinary IS estimator
        
        å±•ç¤ºæ— åæ€§å’Œé«˜æ–¹å·®
        Show unbiasedness and high variance
        """
        print("\n" + "="*60)
        print("æ™®é€šISä¼°è®¡å™¨æ€§è´¨")
        print("Ordinary IS Estimator Properties")
        print("="*60)
        
        print("""
        ğŸ“Š æ•°å­¦æ€§è´¨ Mathematical Properties
        ====================================
        
        1. æ— åæ€§ Unbiasedness:
           E_b[ÏG] = v_Ï€(s) âœ“
           
           è¯æ˜å…³é”®ï¼š
           Key proof:
           Ïå°†bçš„æ¦‚ç‡æµ‹åº¦è½¬æ¢ä¸ºÏ€çš„
           Ï transforms b's probability measure to Ï€'s
        
        2. æ–¹å·® Variance:
           Var[ÏG] = E_b[(ÏG)Â²] - (v_Ï€(s))Â²
                   = E_Ï€[ÏGÂ²] - (v_Ï€(s))Â²
           
           é—®é¢˜ï¼šÏå¯èƒ½å¾ˆå¤§
           Problem: Ï can be very large
           
           æœ€åæƒ…å†µï¼š
           Worst case:
           å¦‚æœÏ€å’Œbå¾ˆä¸åŒï¼ŒÏçš„æ–¹å·®å¯èƒ½æ— é™
           If Ï€ and b very different, Ï variance can be infinite
        
        3. æ”¶æ•›é€Ÿåº¦ Convergence Rate:
           âˆšn(VÌ‚ - v_Ï€) â†’ N(0, ÏƒÂ²)
           
           å…¶ä¸­ÏƒÂ² = Var[ÏG]
           where ÏƒÂ² = Var[ÏG]
           
           é—®é¢˜ï¼šÏƒÂ²å¯èƒ½éå¸¸å¤§
           Problem: ÏƒÂ² can be very large
        
        4. æ ·æœ¬æ•ˆç‡ Sample Efficiency:
           æœ‰æ•ˆæ ·æœ¬å¤§å° ESS = n Ã— (E[Ï])Â² / E[ÏÂ²]
           Effective Sample Size
           
           é€šå¸¸ESS << n
           Usually ESS << n
        """)
        
        # åˆ†æå®é™…æ•°æ®
        # Analyze actual data
        if self.weighted_returns:
            print("\nå®é™…ä¼°è®¡åˆ†æ:")
            print("Actual Estimation Analysis:")
            
            for state_id, weighted_list in list(self.weighted_returns.items())[:3]:
                if len(weighted_list) > 1:
                    mean = np.mean(weighted_list)
                    std = np.std(weighted_list)
                    cv = std / abs(mean) if mean != 0 else float('inf')
                    
                    print(f"\nçŠ¶æ€ {state_id}:")
                    print(f"  æ ·æœ¬æ•°: {len(weighted_list)}")
                    print(f"  ä¼°è®¡å€¼: {mean:.3f}")
                    print(f"  æ ‡å‡†å·®: {std:.3f}")
                    print(f"  å˜å¼‚ç³»æ•°: {cv:.3f}")
                    
                    # æ£€æŸ¥æç«¯å€¼å½±å“
                    # Check extreme value impact
                    if len(weighted_list) >= 10:
                        sorted_weights = sorted(weighted_list, reverse=True)
                        top_10_percent = int(len(weighted_list) * 0.1)
                        top_contribution = sum(sorted_weights[:top_10_percent]) / sum(weighted_list)
                        print(f"  å‰10%æ ·æœ¬è´¡çŒ®: {top_contribution:.1%}")


# ================================================================================
# ç¬¬4.4.4èŠ‚ï¼šåŠ æƒé‡è¦æ€§é‡‡æ ·
# Section 4.4.4: Weighted Importance Sampling
# ================================================================================

class WeightedImportanceSampling(ImportanceSampling):
    """
    åŠ æƒé‡è¦æ€§é‡‡æ ·
    Weighted Importance Sampling
    
    ç”¨å½’ä¸€åŒ–å‡å°‘æ–¹å·®
    Reduce variance through normalization
    
    ä¼°è®¡å™¨ï¼š
    Estimator:
    V^Ï€(s) = Î£áµ¢(Ïáµ¢Gáµ¢) / Î£áµ¢Ïáµ¢
    
    ä¸æ™®é€šISçš„åŒºåˆ«ï¼š
    Difference from ordinary IS:
    - æ™®é€šï¼š(1/n)Î£Ïáµ¢(ÏG)áµ¢
      Ordinary: (1/n)Î£Ïáµ¢(ÏG)áµ¢
    - åŠ æƒï¼šÎ£áµ¢(Ïáµ¢Gáµ¢)/Î£áµ¢Ïáµ¢
      Weighted: Î£áµ¢(Ïáµ¢Gáµ¢)/Î£áµ¢Ïáµ¢
    
    æ€§è´¨ï¼š
    Properties:
    - æœ‰åï¼ˆä½†æ¸è¿‘æ— åï¼‰
      Biased (but asymptotically unbiased)
    - ä½æ–¹å·®
      Lower variance
    - æ›´ç¨³å®š
      More stable
    - å®è·µä¸­é€šå¸¸æ›´å¥½
      Usually better in practice
    
    åå·®åˆ†æï¼š
    Bias analysis:
    - æœ‰é™æ ·æœ¬æœ‰åï¼šE[Î£(ÏG)/Î£Ï] â‰  v_Ï€
      Finite sample biased
    - æ¸è¿‘æ— åï¼šå½“nâ†’âˆæ—¶æ”¶æ•›åˆ°v_Ï€
      Asymptotically unbiased: converges to v_Ï€ as nâ†’âˆ
    - åå·®éšæ ·æœ¬æ•°å¿«é€Ÿå‡å°
      Bias decreases quickly with samples
    
    ä¸ºä»€ä¹ˆæ–¹å·®æ›´å°ï¼Ÿ
    Why lower variance?
    å½’ä¸€åŒ–ä½¿ä¼°è®¡å¯¹æç«¯æƒé‡æ›´é²æ£’
    Normalization makes estimate more robust to extreme weights
    """
    
    def __init__(self,
                 env: MDPEnvironment,
                 target_policy: Policy,
                 behavior_policy: Policy,
                 gamma: float = 1.0,
                 first_visit: bool = True):
        """
        åˆå§‹åŒ–åŠ æƒIS
        Initialize weighted IS
        """
        super().__init__(env, target_policy, behavior_policy, gamma)
        
        self.first_visit = first_visit
        
        # ç´¯ç§¯åˆ†å­å’Œåˆ†æ¯ï¼ˆç”¨äºå¢é‡æ›´æ–°ï¼‰
        # Cumulative numerator and denominator (for incremental update)
        self.C = defaultdict(float)  # åˆ†æ¯ï¼šÎ£Ï
        self.weighted_sum = defaultdict(float)  # åˆ†å­ï¼šÎ£(ÏG)
        
        logger.info(f"åˆå§‹åŒ–åŠ æƒIS: first_visit={first_visit}")
    
    def update_value(self, episode: Episode):
        """
        ä½¿ç”¨åŠ æƒISæ›´æ–°ä»·å€¼
        Update value using weighted IS
        
        æ ¸å¿ƒï¼šå½’ä¸€åŒ–çš„åŠ æƒå¹³å‡
        Core: Normalized weighted average
        """
        returns = episode.compute_returns(self.gamma)
        ratios = self.compute_per_step_ratios(episode)
        
        if self.first_visit:
            visited_states = set()
            
            for t, exp in enumerate(episode.experiences):
                if exp.state.id not in visited_states:
                    visited_states.add(exp.state.id)
                    
                    G = returns[t]
                    rho = ratios[t]
                    
                    # æ›´æ–°åˆ†å­å’Œåˆ†æ¯
                    # Update numerator and denominator
                    self.weighted_sum[exp.state.id] += rho * G
                    self.C[exp.state.id] += rho
                    
                    # è®°å½•
                    # Record
                    self.is_ratios.append(rho)
                    self.state_visits[exp.state.id] += 1
                    
                    # åŠ æƒISæ›´æ–°ï¼šå½’ä¸€åŒ–
                    # Weighted IS update: normalized
                    if self.C[exp.state.id] > 0:
                        new_v = self.weighted_sum[exp.state.id] / self.C[exp.state.id]
                        self.V.set_value(exp.state, new_v)
                    
                    # ç»Ÿè®¡
                    # Statistics
                    self.statistics.update_state_value(exp.state, G)
        
        else:
            for t, exp in enumerate(episode.experiences):
                G = returns[t]
                rho = ratios[t]
                
                self.weighted_sum[exp.state.id] += rho * G
                self.C[exp.state.id] += rho
                
                self.is_ratios.append(rho)
                self.state_visits[exp.state.id] += 1
                
                if self.C[exp.state.id] > 0:
                    new_v = self.weighted_sum[exp.state.id] / self.C[exp.state.id]
                    self.V.set_value(exp.state, new_v)
                
                self.statistics.update_state_value(exp.state, G)
    
    def compare_with_ordinary(self, ordinary_is: OrdinaryImportanceSampling):
        """
        ä¸æ™®é€šISæ¯”è¾ƒ
        Compare with ordinary IS
        
        å±•ç¤ºåå·®-æ–¹å·®æƒè¡¡
        Show bias-variance tradeoff
        """
        print("\n" + "="*60)
        print("åŠ æƒIS vs æ™®é€šIS")
        print("Weighted IS vs Ordinary IS")
        print("="*60)
        
        # æ¯”è¾ƒä¼°è®¡å€¼
        # Compare estimates
        print("\nä¼°è®¡å€¼æ¯”è¾ƒ:")
        print("Estimate Comparison:")
        
        sample_states = list(self.state_visits.keys())[:5]
        
        print(f"{'State':<10} {'Weighted IS':<12} {'Ordinary IS':<12} {'Difference':<12}")
        print("-" * 46)
        
        for state_id in sample_states:
            # è·å–åŠ æƒISä¼°è®¡
            # Get weighted IS estimate
            if state_id in self.C and self.C[state_id] > 0:
                weighted_v = self.weighted_sum[state_id] / self.C[state_id]
            else:
                weighted_v = 0.0
            
            # è·å–æ™®é€šISä¼°è®¡
            # Get ordinary IS estimate
            if state_id in ordinary_is.weighted_returns:
                ordinary_v = np.mean(ordinary_is.weighted_returns[state_id])
            else:
                ordinary_v = 0.0
            
            diff = abs(weighted_v - ordinary_v)
            
            print(f"{state_id:<10} {weighted_v:<12.3f} {ordinary_v:<12.3f} {diff:<12.3f}")
        
        # æ¯”è¾ƒæ–¹å·®
        # Compare variance
        print("\næ–¹å·®æ¯”è¾ƒ:")
        print("Variance Comparison:")
        
        # åŠ æƒISçš„æœ‰æ•ˆæ–¹å·®ï¼ˆè¿‘ä¼¼ï¼‰
        # Effective variance of weighted IS (approximate)
        weighted_vars = []
        ordinary_vars = []
        
        for state_id in sample_states:
            if state_id in ordinary_is.weighted_returns:
                ordinary_var = np.var(ordinary_is.weighted_returns[state_id])
                ordinary_vars.append(ordinary_var)
            
            # åŠ æƒISæ–¹å·®æ›´éš¾ç›´æ¥è®¡ç®—
            # Weighted IS variance harder to compute directly
            # ä½¿ç”¨bootstrapæˆ–å…¶ä»–æ–¹æ³•ä¼°è®¡
            # Use bootstrap or other methods to estimate
        
        if ordinary_vars:
            print(f"  æ™®é€šISå¹³å‡æ–¹å·®: {np.mean(ordinary_vars):.3f}")
            print(f"  ï¼ˆåŠ æƒISæ–¹å·®é€šå¸¸æ›´å°ä½†éš¾ä»¥ç›´æ¥è®¡ç®—ï¼‰")
            print(f"  (Weighted IS variance usually smaller but hard to compute)")
        
        print("\n" + "="*40)
        print("ç†è®ºæ¯”è¾ƒ:")
        print("Theoretical Comparison:")
        print("="*40)
        print("""
        æ™®é€šIS Ordinary IS:
        ------------------
        âœ“ æ— å Unbiased
        âœ— é«˜æ–¹å·® High variance
        âœ— å¯¹æç«¯æƒé‡æ•æ„Ÿ Sensitive to extreme weights
        
        åŠ æƒIS Weighted IS:
        ------------------
        âœ— æœ‰é™æ ·æœ¬æœ‰å Finite sample biased
        âœ“ ä½æ–¹å·® Lower variance
        âœ“ å¯¹æç«¯æƒé‡é²æ£’ Robust to extreme weights
        âœ“ å®è·µä¸­é€šå¸¸æ›´å¥½ Usually better in practice
        
        é€‰æ‹©å»ºè®® Selection Advice:
        ------------------------
        - å°æ ·æœ¬+éœ€è¦æ— åï¼šæ™®é€šIS
          Small sample + need unbiased: Ordinary IS
        - å¤§æ ·æœ¬+éœ€è¦ç¨³å®šï¼šåŠ æƒIS
          Large sample + need stable: Weighted IS
        - ä¸€èˆ¬æƒ…å†µï¼šåŠ æƒIS
          General case: Weighted IS
        """)


# ================================================================================
# ç¬¬4.4.5èŠ‚ï¼šå¢é‡é‡è¦æ€§é‡‡æ ·MC
# Section 4.4.5: Incremental Importance Sampling MC
# ================================================================================

class IncrementalISMC(ImportanceSampling):
    """
    å¢é‡é‡è¦æ€§é‡‡æ ·MC
    Incremental Importance Sampling MC
    
    ä½¿ç”¨å¢é‡å…¬å¼çš„åŠ æƒIS
    Weighted IS with incremental formula
    
    è¿™æ˜¯å®è·µä¸­æœ€å¸¸ç”¨çš„å½¢å¼
    This is the most commonly used form in practice
    
    æ›´æ–°å…¬å¼ï¼š
    Update formula:
    Q(s,a) â† Q(s,a) + (W/C(s,a))[G - Q(s,a)]
    
    å…¶ä¸­ï¼š
    where:
    - Wæ˜¯é‡è¦æ€§æƒé‡
      W is importance weight
    - C(s,a)æ˜¯ç´¯ç§¯æƒé‡
      C(s,a) is cumulative weight
    - Gæ˜¯å›æŠ¥
      G is return
    
    ç­‰ä»·äºï¼š
    Equivalent to:
    Q(s,a) = Î£áµ¢(Wáµ¢Gáµ¢) / Î£áµ¢ Wáµ¢
    
    ä¼˜åŠ¿ï¼š
    Advantages:
    1. å†…å­˜æ•ˆç‡ï¼ˆä¸å­˜å‚¨å†å²ï¼‰
       Memory efficient (no history storage)
    2. åœ¨çº¿å­¦ä¹ 
       Online learning
    3. è‡ªç„¶çš„off-policyæ§åˆ¶
       Natural off-policy control
    
    è¿™æ˜¯Q-learningçš„å‰èº«ï¼
    This is the predecessor of Q-learning!
    """
    
    def __init__(self,
                 env: MDPEnvironment,
                 target_policy: Policy,
                 behavior_policy: Policy,
                 gamma: float = 1.0):
        """
        åˆå§‹åŒ–å¢é‡IS MC
        Initialize incremental IS MC
        """
        super().__init__(env, target_policy, behavior_policy, gamma)
        
        # ç´¯ç§¯æƒé‡ï¼ˆåˆ†æ¯ï¼‰
        # Cumulative weights (denominator)
        self.C_state = defaultdict(float)
        self.C_sa = defaultdict(float)
        
        logger.info("åˆå§‹åŒ–å¢é‡IS MC")
    
    def update_value(self, episode: Episode):
        """
        æ›´æ–°ä»·å€¼ï¼ˆä½¿ç”¨å¢é‡åŠ æƒISï¼‰
        Update value (using incremental weighted IS)
        
        æ³¨æ„ï¼šIncrementalISMCä¸»è¦é€šè¿‡learnæ–¹æ³•å·¥ä½œ
        Note: IncrementalISMC primarily works through learn method
        """
        # è¿™ä¸ªæ–¹æ³•ä¸»è¦æ˜¯ä¸ºäº†æ»¡è¶³åŸºç±»æ¥å£
        # This method is mainly to satisfy base class interface
        # å®é™…çš„å¢é‡æ›´æ–°åœ¨learnæ–¹æ³•ä¸­å®ç°
        # Actual incremental update is implemented in learn method
        pass
    
    def learn(self, n_episodes: int = 1000, 
             verbose: bool = True) -> Tuple[Policy, ActionValueFunction]:
        """
        å­¦ä¹ æœ€ä¼˜ç­–ç•¥
        Learn optimal policy
        
        å®ç°off-policy MCæ§åˆ¶
        Implement off-policy MC control
        """
        if verbose:
            print("\n" + "="*60)
            print("å¢é‡IS MCå­¦ä¹ ")
            print("Incremental IS MC Learning")
            print("="*60)
            print(f"  ç›®æ ‡ç­–ç•¥: {type(self.target_policy).__name__}")
            print(f"  è¡Œä¸ºç­–ç•¥: {type(self.behavior_policy).__name__}")
            print(f"  å›åˆæ•°: {n_episodes}")
        
        learning_curve = []
        
        for episode_num in range(n_episodes):
            # ç”¨è¡Œä¸ºç­–ç•¥ç”Ÿæˆå›åˆ
            # Generate episode using behavior policy
            episode = self.generate_episode(self.behavior_policy)
            
            # è®¡ç®—å›æŠ¥
            # Compute returns
            returns = episode.compute_returns(self.gamma)
            
            # åå‘å¤„ç†ï¼ˆç´¯ç§¯é‡è¦æ€§æƒé‡ï¼‰
            # Process backward (accumulate importance weights)
            W = 1.0
            
            for t in reversed(range(len(episode.experiences))):
                exp = episode.experiences[t]
                sa_pair = (exp.state.id, exp.action.id)
                G = returns[t]
                
                # æ›´æ–°Qï¼ˆå¢é‡åŠ æƒISï¼‰
                # Update Q (incremental weighted IS)
                self.C_sa[sa_pair] += W
                
                if self.C_sa[sa_pair] > 0:
                    old_q = self.Q.get_value(exp.state, exp.action)
                    new_q = old_q + (W / self.C_sa[sa_pair]) * (G - old_q)
                    self.Q.set_value(exp.state, exp.action, new_q)
                
                # æ›´æ–°Vï¼ˆç±»ä¼¼ï¼‰
                # Update V (similarly)
                self.C_state[exp.state.id] += W
                
                if self.C_state[exp.state.id] > 0:
                    old_v = self.V.get_value(exp.state)
                    new_v = old_v + (W / self.C_state[exp.state.id]) * (G - old_v)
                    self.V.set_value(exp.state, new_v)
                
                # æ”¹è¿›ç›®æ ‡ç­–ç•¥ï¼ˆè´ªå©ªï¼‰
                # Improve target policy (greedy)
                if isinstance(self.target_policy, DeterministicPolicy):
                    best_action = None
                    best_value = float('-inf')
                    
                    for action in self.env.action_space:
                        q_value = self.Q.get_value(exp.state, action)
                        if q_value > best_value:
                            best_value = q_value
                            best_action = action
                    
                    if best_action:
                        self.target_policy.policy_map[exp.state] = best_action
                
                # å¦‚æœä¸æ˜¯ç›®æ ‡ç­–ç•¥çš„åŠ¨ä½œï¼Œç»ˆæ­¢
                # If not target policy action, terminate
                if isinstance(self.target_policy, DeterministicPolicy):
                    if exp.state in self.target_policy.policy_map:
                        if exp.action.id != self.target_policy.policy_map[exp.state].id:
                            break
                
                # æ›´æ–°W
                # Update W
                behavior_probs = self.behavior_policy.get_action_probabilities(
                    exp.state
                )
                behavior_prob = behavior_probs.get(exp.action, 1e-10)
                
                # ç›®æ ‡ç­–ç•¥æ˜¯ç¡®å®šæ€§çš„ï¼Œæ¦‚ç‡æ˜¯1
                # Target policy is deterministic, probability is 1
                W = W / behavior_prob
                
                # è®°å½•
                # Record
                self.is_ratios.append(W)
            
            # è®°å½•å­¦ä¹ è¿›åº¦
            # Record learning progress
            if returns:
                learning_curve.append(returns[0])
            
            if verbose and (episode_num + 1) % 100 == 0:
                avg_return = np.mean(learning_curve[-100:]) if learning_curve else 0
                print(f"  Episode {episode_num + 1}: å¹³å‡å›æŠ¥={avg_return:.2f}")
        
        if verbose:
            print(f"\nå­¦ä¹ å®Œæˆ:")
            print(f"  è®¿é—®çš„çŠ¶æ€: {len(self.C_state)}")
            print(f"  è®¿é—®çš„(s,a)å¯¹: {len(self.C_sa)}")
            
            # åˆ†æISæ¯”ç‡
            # Analyze IS ratios
            if self.is_ratios:
                print(f"  å¹³å‡ISæ¯”ç‡: {np.mean(self.is_ratios):.3f}")
                print(f"  ISæ¯”ç‡æ ‡å‡†å·®: {np.std(self.is_ratios):.3f}")
        
        return self.target_policy, self.Q
    
    def generate_episode(self, policy: Policy, max_steps: int = 1000) -> Episode:
        """
        ç”Ÿæˆå›åˆ
        Generate episode
        """
        episode = Episode()
        state = self.env.reset()
        
        for t in range(max_steps):
            action = policy.select_action(state)
            next_state, reward, done, _ = self.env.step(action)
            
            exp = Experience(state, action, reward, next_state, done)
            episode.add_experience(exp)
            
            state = next_state
            
            if done:
                break
        
        return episode
    
    def demonstrate_incremental_update(self):
        """
        æ¼”ç¤ºå¢é‡æ›´æ–°
        Demonstrate incremental update
        
        å±•ç¤ºå¢é‡å…¬å¼çš„ç­‰ä»·æ€§
        Show equivalence of incremental formula
        """
        print("\n" + "="*60)
        print("å¢é‡ISæ›´æ–°æ¼”ç¤º")
        print("Incremental IS Update Demo")
        print("="*60)
        
        print("""
        ğŸ“ å¢é‡å…¬å¼æ¨å¯¼
        Incremental Formula Derivation
        ===============================
        
        æ‰¹é‡åŠ æƒIS Batch weighted IS:
        Q_n = Î£áµ¢â‚Œâ‚â¿(Wáµ¢Gáµ¢) / Î£áµ¢â‚Œâ‚â¿ Wáµ¢
        
        å¢é‡å½¢å¼ Incremental form:
        Q_n = Q_{n-1} + (W_n/C_n)[G_n - Q_{n-1}]
        
        å…¶ä¸­ where:
        C_n = Î£áµ¢â‚Œâ‚â¿ Wáµ¢ = C_{n-1} + W_n
        
        è¯æ˜ç­‰ä»· Prove equivalence:
        -------------------------------
        ä»¤ Let A_n = Î£áµ¢â‚Œâ‚â¿(Wáµ¢Gáµ¢), C_n = Î£áµ¢â‚Œâ‚â¿ Wáµ¢
        
        åˆ™ Then Q_n = A_n / C_n
        
        A_n = A_{n-1} + W_nG_n
            = C_{n-1}Q_{n-1} + W_nG_n
        
        Q_n = A_n / C_n
            = (C_{n-1}Q_{n-1} + W_nG_n) / C_n
            = (C_{n-1}/C_n)Q_{n-1} + (W_n/C_n)G_n
            = Q_{n-1} - (W_n/C_n)Q_{n-1} + (W_n/C_n)G_n
            = Q_{n-1} + (W_n/C_n)[G_n - Q_{n-1}]  âœ“
        
        ä¼˜åŠ¿ Advantages:
        ----------------
        1. åªéœ€å­˜å‚¨Qå’ŒC
           Only store Q and C
        2. O(1)æ›´æ–°å¤æ‚åº¦
           O(1) update complexity
        3. è‡ªç„¶çš„åœ¨çº¿å­¦ä¹ 
           Natural online learning
        
        ä¸TDçš„è”ç³» Connection to TD:
        ---------------------------
        å½“W=1ï¼ˆon-policyï¼‰æ—¶ï¼š
        When W=1 (on-policy):
        Q â† Q + (1/n)[G - Q]
        
        è¿™å°±æ˜¯MCçš„å¢é‡æ›´æ–°ï¼
        This is incremental MC update!
        
        è¿›ä¸€æ­¥ï¼Œå¦‚æœç”¨ä¼°è®¡ä»£æ›¿G...
        Further, if replace G with estimate...
        â†’ TDæ–¹æ³•ï¼
        â†’ TD methods!
        """)


# ================================================================================
# ç¬¬4.4.6èŠ‚ï¼šISå¯è§†åŒ–å™¨
# Section 4.4.6: IS Visualizer
# ================================================================================

class ISVisualizer:
    """
    é‡è¦æ€§é‡‡æ ·å¯è§†åŒ–å™¨
    Importance Sampling Visualizer
    
    æä¾›ä¸°å¯Œçš„å¯è§†åŒ–æ¥ç†è§£IS
    Provides rich visualizations to understand IS
    """
    
    @staticmethod
    def plot_is_comparison(ordinary: OrdinaryImportanceSampling,
                          weighted: WeightedImportanceSampling,
                          true_values: Optional[Dict[str, float]] = None):
        """
        æ¯”è¾ƒæ™®é€šå’ŒåŠ æƒIS
        Compare ordinary and weighted IS
        """
        fig, axes = plt.subplots(2, 3, figsize=(15, 10))
        
        # å›¾1ï¼šä¼°è®¡å€¼æ¯”è¾ƒ
        # Plot 1: Estimate comparison
        ax1 = axes[0, 0]
        
        states = list(set(ordinary.state_visits.keys()) & 
                     set(weighted.state_visits.keys()))[:10]
        
        if states:
            ordinary_estimates = []
            weighted_estimates = []
            
            for state_id in states:
                # æ™®é€šISä¼°è®¡
                # Ordinary IS estimate
                if state_id in ordinary.weighted_returns:
                    ordinary_v = np.mean(ordinary.weighted_returns[state_id])
                else:
                    ordinary_v = 0
                ordinary_estimates.append(ordinary_v)
                
                # åŠ æƒISä¼°è®¡
                # Weighted IS estimate
                if state_id in weighted.C and weighted.C[state_id] > 0:
                    weighted_v = weighted.weighted_sum[state_id] / weighted.C[state_id]
                else:
                    weighted_v = 0
                weighted_estimates.append(weighted_v)
            
            x = np.arange(len(states))
            width = 0.35
            
            ax1.bar(x - width/2, ordinary_estimates, width, 
                   label='Ordinary IS', alpha=0.7, color='blue')
            ax1.bar(x + width/2, weighted_estimates, width,
                   label='Weighted IS', alpha=0.7, color='red')
            
            if true_values:
                true_v = [true_values.get(s, 0) for s in states]
                ax1.plot(x, true_v, 'go-', label='True values', markersize=8)
            
            ax1.set_xticks(x)
            ax1.set_xticklabels(states, rotation=45)
            ax1.set_ylabel('Value Estimate')
            ax1.set_title('Value Estimates Comparison')
            ax1.legend()
            ax1.grid(True, alpha=0.3)
        
        # å›¾2ï¼šISæ¯”ç‡åˆ†å¸ƒ
        # Plot 2: IS ratio distribution
        ax2 = axes[0, 1]
        
        if ordinary.is_ratios:
            ax2.hist(np.clip(ordinary.is_ratios, 0, 10), bins=30,
                    alpha=0.5, color='blue', label='Ordinary IS')
        if weighted.is_ratios:
            ax2.hist(np.clip(weighted.is_ratios, 0, 10), bins=30,
                    alpha=0.5, color='red', label='Weighted IS')
        
        ax2.axvline(x=1.0, color='black', linestyle='--', label='Ï=1')
        ax2.set_xlabel('IS Ratio (clipped at 10)')
        ax2.set_ylabel('Frequency')
        ax2.set_title('IS Ratio Distribution')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # å›¾3ï¼šæ–¹å·®æ¯”è¾ƒ
        # Plot 3: Variance comparison
        ax3 = axes[0, 2]
        
        # è®¡ç®—æ¯ä¸ªçŠ¶æ€çš„ä¼°è®¡æ–¹å·®
        # Compute estimation variance per state
        ordinary_vars = []
        weighted_vars = []
        
        for state_id in states[:5]:
            if state_id in ordinary.weighted_returns:
                if len(ordinary.weighted_returns[state_id]) > 1:
                    ordinary_vars.append(np.var(ordinary.weighted_returns[state_id]))
            
            # åŠ æƒISçš„æ–¹å·®æ›´å¤æ‚ï¼Œè¿™é‡Œç”¨è¿‘ä¼¼
            # Weighted IS variance is complex, use approximation
            if state_id in weighted.statistics.state_returns:
                returns_obj = weighted.statistics.state_returns[state_id]
                if returns_obj.count > 1:
                    weighted_vars.append(returns_obj.variance / returns_obj.count)
        
        if ordinary_vars or weighted_vars:
            labels = ['Ordinary IS', 'Weighted IS']
            variances = [ordinary_vars, weighted_vars]
            
            bp = ax3.boxplot(variances, labels=labels, patch_artist=True)
            colors = ['lightblue', 'lightcoral']
            for patch, color in zip(bp['boxes'], colors):
                patch.set_facecolor(color)
                patch.set_alpha(0.5)
            
            ax3.set_ylabel('Variance')
            ax3.set_title('Estimation Variance')
            ax3.grid(True, alpha=0.3, axis='y')
        
        # å›¾4ï¼šæ”¶æ•›æ›²çº¿
        # Plot 4: Convergence curves
        ax4 = axes[1, 0]
        ax4.set_title('Convergence Comparison')
        ax4.set_xlabel('Number of Episodes')
        ax4.set_ylabel('Average Estimation Error')
        ax4.grid(True, alpha=0.3)
        # (éœ€è¦è¿è¡Œæ—¶æ•°æ®)
        # (Needs runtime data)
        
        # å›¾5ï¼šæœ‰æ•ˆæ ·æœ¬å¤§å°
        # Plot 5: Effective sample size
        ax5 = axes[1, 1]
        
        # è®¡ç®—ESS
        # Compute ESS
        if ordinary.is_ratios and weighted.is_ratios:
            # æ»‘åŠ¨çª—å£ESS
            # Sliding window ESS
            window = 100
            ordinary_ess = []
            weighted_ess = []
            
            for i in range(window, min(len(ordinary.is_ratios), 
                                     len(weighted.is_ratios)), 10):
                # æ™®é€šIS
                # Ordinary IS
                o_batch = ordinary.is_ratios[i-window:i]
                o_sum_w = np.sum(o_batch)
                o_sum_w2 = np.sum(np.array(o_batch)**2)
                if o_sum_w2 > 0:
                    o_ess = (o_sum_w**2) / o_sum_w2 / window
                    ordinary_ess.append(o_ess)
                
                # åŠ æƒIS
                # Weighted IS
                w_batch = weighted.is_ratios[i-window:i]
                w_sum_w = np.sum(w_batch)
                w_sum_w2 = np.sum(np.array(w_batch)**2)
                if w_sum_w2 > 0:
                    w_ess = (w_sum_w**2) / w_sum_w2 / window
                    weighted_ess.append(w_ess)
            
            if ordinary_ess and weighted_ess:
                x_axis = range(len(ordinary_ess))
                ax5.plot(x_axis, ordinary_ess, 'b-', alpha=0.7, label='Ordinary IS')
                ax5.plot(x_axis, weighted_ess, 'r-', alpha=0.7, label='Weighted IS')
                ax5.axhline(y=1.0, color='gray', linestyle='--', label='Perfect')
                ax5.set_xlabel('Episode Batch')
                ax5.set_ylabel('ESS / n')
                ax5.set_title('Effective Sample Size Efficiency')
                ax5.legend()
                ax5.grid(True, alpha=0.3)
        
        # å›¾6ï¼šåå·®-æ–¹å·®æƒè¡¡
        # Plot 6: Bias-variance tradeoff
        ax6 = axes[1, 2]
        ax6.set_title('Bias-Variance Tradeoff')
        
        # æ¦‚å¿µå›¾
        # Conceptual plot
        methods = ['Ordinary IS', 'Weighted IS']
        bias = [0, 0.2]  # æ™®é€šæ— åï¼ŒåŠ æƒæœ‰å°åå·®
        variance = [1.0, 0.3]  # æ™®é€šé«˜æ–¹å·®ï¼ŒåŠ æƒä½æ–¹å·®
        
        ax6.scatter(bias, variance, s=200, alpha=0.6)
        for i, method in enumerate(methods):
            ax6.annotate(method, (bias[i], variance[i]), 
                        ha='center', va='center')
        
        ax6.set_xlabel('Bias')
        ax6.set_ylabel('Variance')
        ax6.set_xlim([-0.1, 0.5])
        ax6.set_ylim([0, 1.2])
        ax6.grid(True, alpha=0.3)
        
        # æ·»åŠ MSEç­‰é«˜çº¿ï¼ˆæ¦‚å¿µæ€§ï¼‰
        # Add MSE contours (conceptual)
        bias_grid = np.linspace(-0.1, 0.5, 100)
        variance_grid = np.linspace(0, 1.2, 100)
        B, V = np.meshgrid(bias_grid, variance_grid)
        MSE = B**2 + V  # MSE = BiasÂ² + Variance
        
        contour = ax6.contour(B, V, MSE, levels=5, colors='gray', alpha=0.3)
        ax6.clabel(contour, inline=True, fontsize=8, fmt='MSE=%.1f')
        
        plt.suptitle('Importance Sampling Methods Comparison', 
                    fontsize=14, fontweight='bold')
        plt.tight_layout()
        
        return fig


# ================================================================================
# ç¬¬4.4.7èŠ‚ï¼šISç»¼åˆæ¼”ç¤º
# Section 4.4.7: IS Comprehensive Demo
# ================================================================================

def demonstrate_importance_sampling():
    """
    ç»¼åˆæ¼”ç¤ºé‡è¦æ€§é‡‡æ ·
    Comprehensive demonstration of importance sampling
    """
    print("\n" + "="*80)
    print("é‡è¦æ€§é‡‡æ ·ç»¼åˆæ¼”ç¤º")
    print("Importance Sampling Comprehensive Demo")
    print("="*80)
    
    # 1. åŸºæœ¬åŸç†æ¼”ç¤º
    # 1. Basic principle demo
    print("\n1. åŸºæœ¬åŸç†")
    print("1. Basic Principle")
    fig1 = ImportanceSamplingTheory.demonstrate_basic_principle()
    
    # 2. åœ¨RLç¯å¢ƒä¸­çš„IS
    # 2. IS in RL environment
    print("\n2. åœ¨RLä¸­çš„é‡è¦æ€§é‡‡æ ·")
    print("2. Importance Sampling in RL")
    
    # åˆ›å»ºç¯å¢ƒ
    # Create environment
    from src.ch03_finite_mdp.gridworld import GridWorld
    env = GridWorld(rows=3, cols=3,
                   start_pos=(0,0),
                   goal_pos=(2,2))
    
    # åˆ›å»ºè¡Œä¸ºç­–ç•¥ï¼ˆæ¢ç´¢æ€§ï¼‰
    # Create behavior policy (exploratory)
    from src.ch03_finite_mdp.policies_and_values import UniformRandomPolicy
    behavior_policy = UniformRandomPolicy(env.action_space)
    
    # åˆ›å»ºç›®æ ‡ç­–ç•¥ï¼ˆè´ªå©ªï¼‰
    # Create target policy (greedy)
    from src.ch03_finite_mdp.policies_and_values import ActionValueFunction, DeterministicPolicy
    Q_init = ActionValueFunction(env.state_space, env.action_space, initial_value=0.0)
    
    # åˆå§‹åŒ–ä¸€ä¸ªç®€å•çš„è´ªå©ªç­–ç•¥
    # Initialize a simple greedy policy
    policy_map = {}
    for state in env.state_space:
        if not state.is_terminal:
            # å‘ç›®æ ‡æ–¹å‘çš„ç®€å•å¯å‘å¼
            # Simple heuristic toward goal
            policy_map[state] = env.action_space[1]  # å‡è®¾æ˜¯'right'æˆ–'down'
    
    target_policy = DeterministicPolicy(policy_map)
    
    # è¿è¡Œä¸åŒISæ–¹æ³•
    # Run different IS methods
    n_episodes = 500
    
    print(f"\nè¿è¡Œ{n_episodes}ä¸ªå›åˆ...")
    print(f"Running {n_episodes} episodes...")
    
    # 3. æ™®é€šIS
    # 3. Ordinary IS
    print("\n3. æ™®é€šé‡è¦æ€§é‡‡æ ·")
    print("3. Ordinary Importance Sampling")
    
    ordinary_is = OrdinaryImportanceSampling(
        env, target_policy, behavior_policy, gamma=0.9
    )
    
    # ç”Ÿæˆå›åˆå¹¶æ›´æ–°
    # Generate episodes and update
    for _ in range(n_episodes):
        # ç”¨è¡Œä¸ºç­–ç•¥ç”Ÿæˆå›åˆ
        # Generate episode with behavior policy
        episode = ordinary_is.generate_episode(behavior_policy)
        ordinary_is.update_value(episode)
    
    ordinary_is.diagnose_coverage()
    ordinary_is.analyze_variance()
    ordinary_is.analyze_estimator_properties()
    
    # 4. åŠ æƒIS
    # 4. Weighted IS
    print("\n4. åŠ æƒé‡è¦æ€§é‡‡æ ·")
    print("4. Weighted Importance Sampling")
    
    weighted_is = WeightedImportanceSampling(
        env, target_policy, behavior_policy, gamma=0.9
    )
    
    for _ in range(n_episodes):
        episode = weighted_is.generate_episode(behavior_policy)
        weighted_is.update_value(episode)
    
    weighted_is.compare_with_ordinary(ordinary_is)
    
    # 5. å¢é‡IS MC
    # 5. Incremental IS MC
    print("\n5. å¢é‡é‡è¦æ€§é‡‡æ ·MC")
    print("5. Incremental IS MC")
    
    # åˆ›å»ºÎµ-è´ªå©ªè¡Œä¸ºç­–ç•¥
    # Create Îµ-greedy behavior policy
    from ch04_monte_carlo.mc_control import EpsilonGreedyPolicy
    behavior_policy_eps = EpsilonGreedyPolicy(Q_init, epsilon=0.3, action_space=env.action_space)
    
    incremental_is = IncrementalISMC(
        env, target_policy, behavior_policy_eps, gamma=0.9
    )
    
    learned_policy, learned_Q = incremental_is.learn(n_episodes, verbose=True)
    incremental_is.demonstrate_incremental_update()
    
    # å¯è§†åŒ–æ¯”è¾ƒ
    # Visualization comparison
    print("\nç”Ÿæˆå¯è§†åŒ–...")
    print("Generating visualizations...")
    
    fig2 = ISVisualizer.plot_is_comparison(ordinary_is, weighted_is)
    
    # æ€»ç»“
    # Summary
    print("\n" + "="*80)
    print("å…³é”®è¦ç‚¹")
    print("Key Takeaways")
    print("="*80)
    print("""
    1. é‡è¦æ€§é‡‡æ ·åŸç†:
       Importance Sampling Principle:
       - ç”¨ä¸€ä¸ªåˆ†å¸ƒçš„æ ·æœ¬ä¼°è®¡å¦ä¸€ä¸ªåˆ†å¸ƒ
         Estimate one distribution using samples from another
       - é€šè¿‡é‡è¦æ€§æƒé‡ä¿®æ­£
         Correct through importance weights
    
    2. æ™®é€šIS vs åŠ æƒIS:
       Ordinary IS vs Weighted IS:
       - æ™®é€šï¼šæ— åä½†é«˜æ–¹å·®
         Ordinary: Unbiased but high variance
       - åŠ æƒï¼šæœ‰åä½†ä½æ–¹å·®
         Weighted: Biased but lower variance
       - å®è·µä¸­åŠ æƒé€šå¸¸æ›´å¥½
         Weighted usually better in practice
    
    3. ä¸»è¦æŒ‘æˆ˜:
       Main Challenges:
       - æ–¹å·®çˆ†ç‚¸
         Variance explosion
       - è¦†ç›–æ€§è¦æ±‚
         Coverage requirement
       - æœ‰æ•ˆæ ·æœ¬å¤§å°å‡å°‘
         Effective sample size reduction
    
    4. ä¸ç°ä»£æ–¹æ³•çš„è”ç³»:
       Connection to Modern Methods:
       - ISæ˜¯off-policyå­¦ä¹ çš„åŸºç¡€
         IS is foundation of off-policy learning
       - Q-learningå¯çœ‹ä½œISçš„ç‰¹ä¾‹
         Q-learning can be seen as special case of IS
       - ç°ä»£æ–¹æ³•åŠªåŠ›å‡å°‘ISçš„æ–¹å·®
         Modern methods try to reduce IS variance
    
    5. å®è·µå»ºè®®:
       Practical Advice:
       - ä¿æŒè¡Œä¸ºå’Œç›®æ ‡ç­–ç•¥æ¥è¿‘
         Keep behavior and target policies close
       - ä½¿ç”¨åŠ æƒISæˆ–å…¶ä»–æ–¹å·®å‡å°‘æŠ€æœ¯
         Use weighted IS or other variance reduction
       - ç›‘æ§æœ‰æ•ˆæ ·æœ¬å¤§å°
         Monitor effective sample size
    """)
    print("="*80)
    
    plt.show()


def generate_episode(env, policy, max_steps=1000):
    """è¾…åŠ©å‡½æ•°ï¼šç”Ÿæˆå›åˆ"""
    from ch04_monte_carlo.mc_foundations import Episode, Experience
    
    episode = Episode()
    state = env.reset()
    
    for _ in range(max_steps):
        action = policy.select_action(state)
        next_state, reward, done, _ = env.step(action)
        
        exp = Experience(state, action, reward, next_state, done)
        episode.add_experience(exp)
        
        state = next_state
        if done:
            break
    
    return episode


# ä¸ºISç±»æ·»åŠ è¾…åŠ©æ–¹æ³•
ImportanceSampling.generate_episode = generate_episode


# ================================================================================
# ä¸»å‡½æ•°
# Main Function
# ================================================================================

def main():
    """
    è¿è¡ŒISæ¼”ç¤º
    Run IS Demo
    """
    demonstrate_importance_sampling()


if __name__ == "__main__":
    main()