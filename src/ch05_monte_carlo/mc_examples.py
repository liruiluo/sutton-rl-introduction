"""
================================================================================
ç¬¬4.5èŠ‚ï¼šè’™ç‰¹å¡æ´›æ–¹æ³•ç»å…¸ä¾‹å­
Section 4.5: Classic Monte Carlo Examples
================================================================================

å±•ç¤ºMCæ–¹æ³•åœ¨ç»å…¸é—®é¢˜ä¸Šçš„åº”ç”¨ã€‚
Demonstrate MC methods on classic problems.

ä¸¤ä¸ªé‡è¦ä¾‹å­ï¼š
Two important examples:
1. 21ç‚¹ï¼ˆBlackjackï¼‰
   - Sutton & Bartoæ•™æçš„ç»å…¸ä¾‹å­
     Classic example from Sutton & Barto
   - å±•ç¤ºMCåœ¨éƒ¨åˆ†å¯è§‚æµ‹ç¯å¢ƒçš„åº”ç”¨
     Shows MC in partially observable environment
   - ä¸éœ€è¦ç¯å¢ƒæ¨¡å‹
     No environment model needed

2. èµ›é“é—®é¢˜ï¼ˆRacetrackï¼‰
   - è¿ç»­åŠ¨ä½œç©ºé—´çš„ç¦»æ•£åŒ–
     Discretization of continuous action space
   - å±•ç¤ºMCå¤„ç†å»¶è¿Ÿå¥–åŠ±
     Shows MC handling delayed rewards
   - æ¢ç´¢çš„é‡è¦æ€§
     Importance of exploration

è¿™äº›ä¾‹å­å±•ç¤ºäº†MCçš„å®é™…å¨åŠ›ï¼
These examples show the practical power of MC!
"""

import numpy as np
from typing import List, Dict, Tuple, Optional, Any, Set
from dataclasses import dataclass, field
from enum import Enum
import logging
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from matplotlib.patches import Rectangle, Circle, FancyBboxPatch
import seaborn as sns
from collections import defaultdict
import time

# å¯¼å…¥åŸºç¡€ç»„ä»¶
import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent))

from ch02_mdp.mdp_framework import State, Action, MDPEnvironment
from ch02_mdp.policies_and_values import (
    Policy, StateValueFunction, ActionValueFunction,
    StochasticPolicy, DeterministicPolicy
)
from ch04_monte_carlo.mc_foundations import Episode, Experience
from ch04_monte_carlo.mc_control import OnPolicyMCControl, EpsilonGreedyPolicy

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


# ================================================================================
# ç¬¬4.5.1èŠ‚ï¼š21ç‚¹æ¸¸æˆ
# Section 4.5.1: Blackjack Game
# ================================================================================

class Blackjack(MDPEnvironment):
    """
    21ç‚¹æ¸¸æˆç¯å¢ƒ
    Blackjack Game Environment
    
    ç»å…¸çš„çº¸ç‰Œæ¸¸æˆï¼Œç›®æ ‡æ˜¯ç‰Œé¢æ€»å’Œæ¥è¿‘21ä½†ä¸è¶…è¿‡ã€‚
    Classic card game, goal is to get cards summing close to 21 without going over.
    
    è§„åˆ™ï¼š
    Rules:
    1. ç©å®¶å’Œåº„å®¶å„å‘ä¸¤å¼ ç‰Œ
       Player and dealer each get two cards
    2. ç©å®¶çœ‹åˆ°è‡ªå·±çš„ç‰Œå’Œåº„å®¶çš„ä¸€å¼ ç‰Œ
       Player sees own cards and one dealer card
    3. ç©å®¶å¯ä»¥è¦ç‰Œ(hit)æˆ–åœç‰Œ(stick)
       Player can hit or stick
    4. Aå¯ä»¥ç®—1æˆ–11ï¼ˆå¯ç”¨Aï¼‰
       Ace can count as 1 or 11 (usable ace)
    5. è¶…è¿‡21ç‚¹çˆ†ç‰Œ(bust)ï¼Œç«‹å³è¾“
       Going over 21 is bust, immediate loss
    6. ç©å®¶åœç‰Œåï¼Œåº„å®¶æŒ‰å›ºå®šç­–ç•¥è¦ç‰Œï¼ˆ<17è¦ç‰Œï¼‰
       After player sticks, dealer plays fixed policy (hit if <17)
    
    çŠ¶æ€ç©ºé—´ï¼ˆ3ç»´ï¼‰ï¼š
    State space (3D):
    - ç©å®¶å½“å‰æ€»å’Œ: 12-21
      Player sum: 12-21
    - åº„å®¶æ˜ç‰Œ: 1-10 (Aç®—1)
      Dealer showing: 1-10 (A counts as 1)
    - æ˜¯å¦æœ‰å¯ç”¨A: True/False
      Usable ace: True/False
    
    åŠ¨ä½œç©ºé—´ï¼š
    Action space:
    - è¦ç‰Œ (hit): 0
    - åœç‰Œ (stick): 1
    
    å¥–åŠ±ï¼š
    Rewards:
    - èµ¢: +1
      Win: +1
    - è¾“: -1
      Lose: -1
    - å¹³: 0
      Draw: 0
    
    ä¸ºä»€ä¹ˆé€‚åˆMCï¼Ÿ
    Why suitable for MC?
    1. å›åˆçŸ­ï¼Œå®¹æ˜“é‡‡æ ·
       Short episodes, easy to sample
    2. ä¸éœ€è¦æ¨¡å‹ï¼ˆç‰Œçš„æ¦‚ç‡å¤æ‚ï¼‰
       No model needed (card probabilities complex)
    3. å»¶è¿Ÿå¥–åŠ±ï¼ˆåªåœ¨å›åˆç»“æŸï¼‰
       Delayed reward (only at episode end)
    """
    
    def __init__(self):
        """
        åˆå§‹åŒ–21ç‚¹ç¯å¢ƒ
        Initialize Blackjack environment
        """
        # å…ˆè°ƒç”¨åŸºç±»æ„é€ å‡½æ•°
        # Call base class constructor first
        super().__init__(name="Blackjack")
        
        # ç„¶ååˆ›å»ºçŠ¶æ€ç©ºé—´ï¼ˆè¦†ç›–åŸºç±»çš„ç©ºåˆ—è¡¨ï¼‰
        # Then create state space (overwrite base class empty list)
        self.state_space = []
        
        # æ‰€æœ‰å¯èƒ½çš„çŠ¶æ€ç»„åˆ
        # All possible state combinations
        for player_sum in range(12, 22):  # 12-21
            for dealer_showing in range(1, 11):  # 1-10
                for usable_ace in [False, True]:
                    state = State(
                        id=f"p{player_sum}_d{dealer_showing}_{'ace' if usable_ace else 'no'}",
                        features={
                            'player_sum': player_sum,
                            'dealer_showing': dealer_showing,
                            'usable_ace': usable_ace
                        },
                        is_terminal=False
                    )
                    self.state_space.append(state)
        
        # æ·»åŠ ç»ˆæ­¢çŠ¶æ€
        # Add terminal states
        terminal_state = State(
            id="terminal",
            features={},
            is_terminal=True
        )
        self.state_space.append(terminal_state)
        
        # åŠ¨ä½œç©ºé—´
        # Action space
        self.action_space = [
            Action("hit", "Hit - take another card"),
            Action("stick", "Stick - stop taking cards")
        ]
        
        # å½“å‰æ¸¸æˆçŠ¶æ€
        # Current game state
        self.player_cards = []
        self.dealer_cards = []
        self.current_state = None
        
        logger.info("åˆå§‹åŒ–21ç‚¹ç¯å¢ƒ")
    
    def _draw_card(self) -> int:
        """
        æŠ½ä¸€å¼ ç‰Œ
        Draw a card
        
        ç®€åŒ–ï¼šæ— é™å‰¯ç‰Œï¼Œæ¯å¼ ç‰Œæ¦‚ç‡ç›¸ç­‰
        Simplified: infinite deck, equal probability
        """
        card = np.random.randint(1, 14)  # 1-13
        return min(card, 10)  # J,Q,Kéƒ½ç®—10
    
    def _get_sum(self, cards: List[int]) -> Tuple[int, bool]:
        """
        è®¡ç®—æ‰‹ç‰Œæ€»å’Œ
        Calculate hand sum
        
        Returns:
            (æ€»å’Œ, æ˜¯å¦æœ‰å¯ç”¨A)
            (sum, usable ace)
        """
        total = sum(cards)
        num_aces = cards.count(1)
        
        # å°è¯•å°†ä¸€ä¸ªAå½“ä½œ11
        # Try to use one ace as 11
        usable_ace = False
        if num_aces > 0 and total + 10 <= 21:
            total += 10
            usable_ace = True
        
        return total, usable_ace
    
    def reset(self) -> State:
        """
        é‡ç½®æ¸¸æˆ
        Reset game
        
        å‘åˆå§‹ç‰Œ
        Deal initial cards
        """
        # ç©å®¶ä¸¤å¼ ç‰Œ
        # Player two cards
        self.player_cards = [self._draw_card(), self._draw_card()]
        
        # åº„å®¶ä¸¤å¼ ç‰Œï¼ˆä¸€å¼ æ˜ç‰Œä¸€å¼ æš—ç‰Œï¼‰
        # Dealer two cards (one showing, one hidden)
        self.dealer_cards = [self._draw_card(), self._draw_card()]
        
        # è®¡ç®—åˆå§‹çŠ¶æ€
        # Calculate initial state
        player_sum, usable_ace = self._get_sum(self.player_cards)
        dealer_showing = self.dealer_cards[0]
        
        # å¦‚æœåˆå§‹å°±çˆ†ç‰Œæˆ–21ç‚¹ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†
        # Handle initial bust or blackjack
        if player_sum < 12:
            # ç»§ç»­æŠ½ç‰Œç›´åˆ°>=12
            # Keep drawing until >=12
            while player_sum < 12:
                self.player_cards.append(self._draw_card())
                player_sum, usable_ace = self._get_sum(self.player_cards)
        
        if player_sum > 21:
            # åˆå§‹çˆ†ç‰Œï¼ˆç½•è§ï¼‰
            # Initial bust (rare)
            self.current_state = self.state_space[-1]  # terminal
        else:
            # æ‰¾åˆ°å¯¹åº”çŠ¶æ€
            # Find corresponding state
            for state in self.state_space:
                if not state.is_terminal:
                    features = state.features
                    if (features.get('player_sum') == player_sum and
                        features.get('dealer_showing') == dealer_showing and
                        features.get('usable_ace') == usable_ace):
                        self.current_state = state
                        break
        
        return self.current_state
    
    def step(self, action: Action) -> Tuple[State, float, bool, Dict]:
        """
        æ‰§è¡ŒåŠ¨ä½œ
        Execute action
        
        Returns:
            (ä¸‹ä¸€çŠ¶æ€, å¥–åŠ±, æ˜¯å¦ç»“æŸ, ä¿¡æ¯)
            (next state, reward, done, info)
        """
        if self.current_state.is_terminal:
            return self.current_state, 0, True, {}
        
        player_sum = self.current_state.features['player_sum']
        dealer_showing = self.current_state.features['dealer_showing']
        usable_ace = self.current_state.features['usable_ace']
        
        if action.id == "hit":
            # ç©å®¶è¦ç‰Œ
            # Player hits
            self.player_cards.append(self._draw_card())
            player_sum, usable_ace = self._get_sum(self.player_cards)
            
            if player_sum > 21:
                # çˆ†ç‰Œï¼Œæ¸¸æˆç»“æŸ
                # Bust, game over
                self.current_state = self.state_space[-1]  # terminal
                return self.current_state, -1, True, {'result': 'player_bust'}
            
            # æ›´æ–°çŠ¶æ€
            # Update state
            for state in self.state_space:
                if not state.is_terminal:
                    features = state.features
                    if (features.get('player_sum') == player_sum and
                        features.get('dealer_showing') == dealer_showing and
                        features.get('usable_ace') == usable_ace):
                        self.current_state = state
                        break
            
            return self.current_state, 0, False, {}
        
        else:  # stick
            # ç©å®¶åœç‰Œï¼Œåº„å®¶å¼€å§‹
            # Player sticks, dealer plays
            
            # åº„å®¶æŒ‰å›ºå®šç­–ç•¥ç©
            # Dealer plays fixed policy
            dealer_sum, _ = self._get_sum(self.dealer_cards)
            
            while dealer_sum < 17:
                self.dealer_cards.append(self._draw_card())
                dealer_sum, _ = self._get_sum(self.dealer_cards)
            
            # åˆ¤æ–­è¾“èµ¢
            # Determine outcome
            if dealer_sum > 21:
                # åº„å®¶çˆ†ç‰Œï¼Œç©å®¶èµ¢
                # Dealer bust, player wins
                reward = 1
                result = 'dealer_bust'
            elif dealer_sum > player_sum:
                # åº„å®¶èµ¢
                # Dealer wins
                reward = -1
                result = 'dealer_win'
            elif dealer_sum < player_sum:
                # ç©å®¶èµ¢
                # Player wins
                reward = 1
                result = 'player_win'
            else:
                # å¹³å±€
                # Draw
                reward = 0
                result = 'draw'
            
            self.current_state = self.state_space[-1]  # terminal
            return self.current_state, reward, True, {'result': result}


class BlackjackPolicy(Policy):
    """
    21ç‚¹ç­–ç•¥
    Blackjack Policy
    
    å¯ä»¥æ˜¯é˜ˆå€¼ç­–ç•¥æˆ–å­¦ä¹ çš„ç­–ç•¥
    Can be threshold policy or learned policy
    """
    
    def __init__(self, threshold: int = 20, action_space: Optional[List[Action]] = None):
        """
        åˆå§‹åŒ–ç­–ç•¥
        Initialize policy
        
        Args:
            threshold: åœç‰Œé˜ˆå€¼
                      Stick threshold
            action_space: åŠ¨ä½œç©ºé—´
                        Action space
        """
        super().__init__()
        self.threshold = threshold
        self.Q = None  # å¯ä»¥è®¾ç½®å­¦ä¹ çš„Qå‡½æ•°
        self.action_space = action_space if action_space else [
            Action("hit", "Hit - take another card"),
            Action("stick", "Stick - stop taking cards")
        ]
    
    def select_action(self, state: State) -> Action:
        """
        é€‰æ‹©åŠ¨ä½œ
        Select action
        
        éµå¾ªåŸºç±»Policyæ¥å£
        Follow base Policy interface
        """
        if state.is_terminal:
            return self.action_space[0]  # ä»»æ„
        
        if self.Q is not None:
            # ä½¿ç”¨å­¦ä¹ çš„Qå‡½æ•°
            # Use learned Q function
            q_values = [self.Q.get_value(state, a) for a in self.action_space]
            best_action_idx = np.argmax(q_values)
            return self.action_space[best_action_idx]
        else:
            # ä½¿ç”¨ç®€å•é˜ˆå€¼ç­–ç•¥
            # Use simple threshold policy
            player_sum = state.features.get('player_sum', 0)
            
            if player_sum >= self.threshold:
                return self.action_space[1]  # stick
            else:
                return self.action_space[0]  # hit
    
    def get_action_probabilities(self, state: State, 
                                action_space: Optional[List[Action]] = None) -> Dict[Action, float]:
        """
        è·å–åŠ¨ä½œæ¦‚ç‡ï¼ˆç¡®å®šæ€§ç­–ç•¥ï¼‰
        Get action probabilities (deterministic policy)
        
        å…¼å®¹æ–°æ—§æ¥å£
        Compatible with both old and new interfaces
        """
        actions = action_space if action_space else self.action_space
        selected_action = self.select_action(state)
        probs = {a: 0.0 for a in actions}
        if selected_action in probs:
            probs[selected_action] = 1.0
        return probs


def visualize_blackjack_policy(Q: ActionValueFunction, usable_ace: bool = False):
    """
    å¯è§†åŒ–21ç‚¹ç­–ç•¥
    Visualize Blackjack policy
    
    å±•ç¤ºåœ¨æ¯ä¸ªçŠ¶æ€ä¸‹çš„æœ€ä¼˜åŠ¨ä½œ
    Show optimal action at each state
    """
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))
    
    # å‡†å¤‡æ•°æ®
    # Prepare data
    player_sums = range(12, 22)
    dealer_showings = range(1, 11)
    
    # ç­–ç•¥çŸ©é˜µï¼ˆ1=stick, 0=hitï¼‰
    # Policy matrix (1=stick, 0=hit)
    policy_matrix = np.zeros((10, 10))
    
    # ä»·å€¼çŸ©é˜µ
    # Value matrix
    value_matrix = np.zeros((10, 10))
    
    for i, player_sum in enumerate(player_sums):
        for j, dealer_showing in enumerate(dealer_showings):
            # æ‰¾åˆ°å¯¹åº”çŠ¶æ€
            # Find corresponding state
            state_id = f"p{player_sum}_d{dealer_showing}_{'ace' if usable_ace else 'no'}"
            state = None
            
            # ç®€åŒ–ï¼šåˆ›å»ºä¸´æ—¶çŠ¶æ€
            # Simplified: create temporary state
            state = State(
                id=state_id,
                features={
                    'player_sum': player_sum,
                    'dealer_showing': dealer_showing,
                    'usable_ace': usable_ace
                },
                is_terminal=False
            )
            
            # è·å–åŠ¨ä½œä»·å€¼
            # Get action values
            hit_action = Action("hit", "")
            stick_action = Action("stick", "")
            
            q_hit = Q.get_value(state, hit_action)
            q_stick = Q.get_value(state, stick_action)
            
            # æœ€ä¼˜åŠ¨ä½œ
            # Optimal action
            if q_stick >= q_hit:
                policy_matrix[i, j] = 1  # stick
                value_matrix[i, j] = q_stick
            else:
                policy_matrix[i, j] = 0  # hit
                value_matrix[i, j] = q_hit
    
    # å›¾1ï¼šç­–ç•¥
    # Plot 1: Policy
    im1 = ax1.imshow(policy_matrix, cmap='coolwarm', aspect='auto')
    ax1.set_xlabel('Dealer Showing')
    ax1.set_ylabel('Player Sum')
    ax1.set_title(f'Optimal Policy ({"Usable" if usable_ace else "No Usable"} Ace)')
    ax1.set_xticks(range(10))
    ax1.set_xticklabels(range(1, 11))
    ax1.set_yticks(range(10))
    ax1.set_yticklabels(range(12, 22))
    
    # æ·»åŠ ç½‘æ ¼å’Œæ ‡ç­¾
    # Add grid and labels
    for i in range(10):
        for j in range(10):
            action = 'S' if policy_matrix[i, j] == 1 else 'H'
            color = 'white' if policy_matrix[i, j] == 0 else 'black'
            ax1.text(j, i, action, ha='center', va='center',
                    color=color, fontweight='bold')
    
    # å›¾2ï¼šä»·å€¼å‡½æ•°
    # Plot 2: Value function
    im2 = ax2.imshow(value_matrix, cmap='YlOrRd', aspect='auto')
    ax2.set_xlabel('Dealer Showing')
    ax2.set_ylabel('Player Sum')
    ax2.set_title(f'State Values ({"Usable" if usable_ace else "No Usable"} Ace)')
    ax2.set_xticks(range(10))
    ax2.set_xticklabels(range(1, 11))
    ax2.set_yticks(range(10))
    ax2.set_yticklabels(range(12, 22))
    
    # æ·»åŠ æ•°å€¼
    # Add values
    for i in range(10):
        for j in range(10):
            value = value_matrix[i, j]
            color = 'white' if value < np.mean(value_matrix) else 'black'
            ax2.text(j, i, f'{value:.2f}', ha='center', va='center',
                    color=color, fontsize=8)
    
    # é¢œè‰²æ¡
    # Colorbars
    plt.colorbar(im1, ax=ax1, label='Action (0=Hit, 1=Stick)')
    plt.colorbar(im2, ax=ax2, label='State Value')
    
    plt.suptitle('Blackjack Optimal Policy and Values', fontsize=14, fontweight='bold')
    plt.tight_layout()
    
    return fig


# ================================================================================
# ç¬¬4.5.2èŠ‚ï¼šèµ›é“é—®é¢˜
# Section 4.5.2: Racetrack Problem
# ================================================================================

class RaceTrack(MDPEnvironment):
    """
    èµ›é“é—®é¢˜ç¯å¢ƒ
    Racetrack Problem Environment
    
    èµ›è½¦å¿…é¡»ä»èµ·ç‚¹åˆ°è¾¾ç»ˆç‚¹ï¼Œæ§åˆ¶é€Ÿåº¦ã€‚
    Race car must reach finish line from start, controlling velocity.
    
    çŠ¶æ€ç©ºé—´ï¼š
    State space:
    - ä½ç½®: (x, y)
    - é€Ÿåº¦: (vx, vy)
    
    åŠ¨ä½œç©ºé—´ï¼š
    Action space:
    - åŠ é€Ÿåº¦: (ax, ay) âˆˆ {-1, 0, 1} Ã— {-1, 0, 1}
    
    åŠ¨åŠ›å­¦ï¼š
    Dynamics:
    - æ–°é€Ÿåº¦: v' = v + a (é™åˆ¶åœ¨[0, vmax])
      New velocity: v' = v + a (bounded to [0, vmax])
    - æ–°ä½ç½®: p' = p + v'
      New position: p' = p + v'
    
    å¥–åŠ±ï¼š
    Rewards:
    - æ¯æ­¥: -1 (é¼“åŠ±å¿«é€Ÿåˆ°è¾¾)
      Per step: -1 (encourage quick finish)
    - æ’å¢™: å›åˆ°èµ·ç‚¹
      Hit wall: return to start
    
    ä¸ºä»€ä¹ˆé€‚åˆMCï¼Ÿ
    Why suitable for MC?
    1. è¿ç»­ç©ºé—´çš„ç¦»æ•£åŒ–
       Discretization of continuous space
    2. å»¶è¿Ÿå¥–åŠ±
       Delayed rewards
    3. éœ€è¦æ¢ç´¢æ‰¾åˆ°å¥½è·¯å¾„
       Need exploration to find good path
    """
    
    def __init__(self, track_name: str = "simple"):
        """
        åˆå§‹åŒ–èµ›é“
        Initialize racetrack
        
        Args:
            track_name: èµ›é“åç§°
                       Track name
        """
        # å…ˆè°ƒç”¨åŸºç±»æ„é€ å‡½æ•°
        # Call base class constructor first
        super().__init__(name=f"RaceTrack-{track_name}")
        
        # åˆ›å»ºèµ›é“åœ°å›¾
        # Create track map
        self.track_map = self._create_track(track_name)
        self.height, self.width = self.track_map.shape
        
        # æ‰¾å‡ºèµ·ç‚¹å’Œç»ˆç‚¹
        # Find start and finish lines
        self.start_positions = []
        self.finish_positions = []
        
        for i in range(self.height):
            for j in range(self.width):
                if self.track_map[i, j] == 2:  # èµ·ç‚¹
                    self.start_positions.append((i, j))
                elif self.track_map[i, j] == 3:  # ç»ˆç‚¹
                    self.finish_positions.append((i, j))
        
        # é€Ÿåº¦é™åˆ¶
        # Velocity limits
        self.max_velocity = 5
        
        # åˆ›å»ºçŠ¶æ€å’ŒåŠ¨ä½œç©ºé—´ï¼ˆè¦†ç›–åŸºç±»çš„ç©ºåˆ—è¡¨ï¼‰
        # Create state and action spaces (overwrite base class empty lists)
        self._create_spaces()
        
        # å½“å‰çŠ¶æ€
        # Current state
        self.position = None
        self.velocity = None
        self.current_state = None
        
        logger.info(f"åˆå§‹åŒ–èµ›é“ç¯å¢ƒ: {track_name}")
    
    def _create_track(self, track_name: str) -> np.ndarray:
        """
        åˆ›å»ºèµ›é“åœ°å›¾
        Create track map
        
        0: å¢™
        1: èµ›é“
        2: èµ·ç‚¹
        3: ç»ˆç‚¹
        """
        if track_name == "simple":
            # ç®€å•Lå½¢èµ›é“
            # Simple L-shaped track
            track = np.zeros((10, 15), dtype=int)
            
            # èµ›é“
            # Track
            track[7:10, 1:10] = 1  # æ¨ªå‘éƒ¨åˆ†
            track[2:10, 7:10] = 1  # çºµå‘éƒ¨åˆ†
            
            # èµ·ç‚¹ï¼ˆåº•éƒ¨ï¼‰
            # Start line (bottom)
            track[9, 1:3] = 2
            
            # ç»ˆç‚¹ï¼ˆé¡¶éƒ¨ï¼‰
            # Finish line (top)
            track[2, 7:10] = 3
            
            return track
        
        elif track_name == "complex":
            # å¤æ‚èµ›é“
            # Complex track
            track = np.zeros((20, 30), dtype=int)
            
            # Så½¢èµ›é“
            # S-shaped track
            # ... (æ›´å¤æ‚çš„è®¾è®¡)
            
            return track
        
        else:
            raise ValueError(f"Unknown track: {track_name}")
    
    def _create_spaces(self):
        """
        åˆ›å»ºçŠ¶æ€å’ŒåŠ¨ä½œç©ºé—´
        Create state and action spaces
        """
        # ç®€åŒ–ï¼šåªåˆ›å»ºéƒ¨åˆ†çŠ¶æ€ç©ºé—´
        # Simplified: only create partial state space
        self.state_space = []
        
        # æ·»åŠ ä¸€äº›ä»£è¡¨æ€§çŠ¶æ€
        # Add some representative states
        for i in range(self.height):
            for j in range(self.width):
                if self.track_map[i, j] > 0:  # èµ›é“æˆ–èµ·/ç»ˆç‚¹
                    for vx in range(-self.max_velocity, self.max_velocity + 1):
                        for vy in range(-self.max_velocity, self.max_velocity + 1):
                            state = State(
                                id=f"p({i},{j})_v({vx},{vy})",
                                features={
                                    'x': i, 'y': j,
                                    'vx': vx, 'vy': vy
                                },
                                is_terminal=False
                            )
                            self.state_space.append(state)
        
        # ç»ˆæ­¢çŠ¶æ€
        # Terminal state
        terminal_state = State(
            id="finish",
            features={},
            is_terminal=True
        )
        self.state_space.append(terminal_state)
        
        # åŠ¨ä½œç©ºé—´ï¼š9ä¸ªåŠ é€Ÿåº¦ç»„åˆ
        # Action space: 9 acceleration combinations
        self.action_space = []
        for ax in [-1, 0, 1]:
            for ay in [-1, 0, 1]:
                action = Action(
                    id=f"a({ax},{ay})",
                    name=f"Accelerate ({ax},{ay})"
                )
                action.ax = ax  # å­˜å‚¨åŠ é€Ÿåº¦
                action.ay = ay
                self.action_space.append(action)
    
    def reset(self) -> State:
        """
        é‡ç½®åˆ°èµ·ç‚¹
        Reset to start
        """
        # éšæœºé€‰æ‹©èµ·ç‚¹
        # Random start position
        self.position = self.start_positions[np.random.randint(len(self.start_positions))]
        self.velocity = (0, 0)
        
        # æ‰¾åˆ°å¯¹åº”çŠ¶æ€
        # Find corresponding state
        self.current_state = self._get_state(self.position, self.velocity)
        
        return self.current_state
    
    def _get_state(self, position: Tuple[int, int], 
                  velocity: Tuple[int, int]) -> State:
        """
        è·å–å¯¹åº”çš„çŠ¶æ€å¯¹è±¡
        Get corresponding state object
        """
        # ç®€åŒ–ï¼šåˆ›å»ºæ–°çŠ¶æ€
        # Simplified: create new state
        state = State(
            id=f"p{position}_v{velocity}",
            features={
                'x': position[0], 'y': position[1],
                'vx': velocity[0], 'vy': velocity[1]
            },
            is_terminal=False
        )
        return state
    
    def step(self, action: Action) -> Tuple[State, float, bool, Dict]:
        """
        æ‰§è¡ŒåŠ¨ä½œ
        Execute action
        """
        # è·å–åŠ é€Ÿåº¦
        # Get acceleration
        ax = action.ax if hasattr(action, 'ax') else 0
        ay = action.ay if hasattr(action, 'ay') else 0
        
        # æ›´æ–°é€Ÿåº¦
        # Update velocity
        new_vx = np.clip(self.velocity[0] + ax, -self.max_velocity, self.max_velocity)
        new_vy = np.clip(self.velocity[1] + ay, -self.max_velocity, self.max_velocity)
        
        # è‡³å°‘è¦æœ‰ä¸€ä¸ªæ–¹å‘çš„é€Ÿåº¦
        # At least one velocity component
        if new_vx == 0 and new_vy == 0:
            new_vx = 1
        
        # æ›´æ–°ä½ç½®ï¼ˆæ£€æŸ¥ç¢°æ’ï¼‰
        # Update position (check collision)
        old_x, old_y = self.position
        new_x = old_x + new_vx
        new_y = old_y + new_vy
        
        # æ£€æŸ¥è·¯å¾„ä¸Šçš„ç¢°æ’
        # Check collision along path
        hit_wall = False
        
        # ç®€å•çº¿æ€§æ’å€¼æ£€æŸ¥
        # Simple linear interpolation check
        steps = max(abs(new_vx), abs(new_vy))
        for i in range(1, steps + 1):
            check_x = old_x + (new_x - old_x) * i // steps
            check_y = old_y + (new_y - old_y) * i // steps
            
            # è¾¹ç•Œæ£€æŸ¥
            # Boundary check
            if (check_x < 0 or check_x >= self.height or
                check_y < 0 or check_y >= self.width or
                self.track_map[check_x, check_y] == 0):
                hit_wall = True
                break
            
            # æ£€æŸ¥æ˜¯å¦åˆ°è¾¾ç»ˆç‚¹
            # Check if reached finish
            if self.track_map[check_x, check_y] == 3:
                self.current_state = self.state_space[-1]  # terminal
                return self.current_state, -1, True, {'finished': True}
        
        if hit_wall:
            # æ’å¢™ï¼Œå›åˆ°èµ·ç‚¹
            # Hit wall, return to start
            self.position = self.start_positions[np.random.randint(len(self.start_positions))]
            self.velocity = (0, 0)
            reward = -10  # æ’å¢™æƒ©ç½š
        else:
            # æ­£å¸¸ç§»åŠ¨
            # Normal move
            self.position = (new_x, new_y)
            self.velocity = (new_vx, new_vy)
            reward = -1  # æ—¶é—´æˆæœ¬
        
        self.current_state = self._get_state(self.position, self.velocity)
        
        return self.current_state, reward, False, {'hit_wall': hit_wall}
    
    def visualize_track(self, policy: Optional[Policy] = None,
                       trajectory: Optional[List[Tuple[int, int]]] = None):
        """
        å¯è§†åŒ–èµ›é“å’Œç­–ç•¥
        Visualize track and policy
        """
        fig, ax = plt.subplots(figsize=(12, 8))
        
        # ç»˜åˆ¶èµ›é“
        # Draw track
        track_display = np.zeros_like(self.track_map, dtype=float)
        track_display[self.track_map == 0] = 0  # å¢™ - é»‘è‰²
        track_display[self.track_map == 1] = 0.5  # èµ›é“ - ç°è‰²
        track_display[self.track_map == 2] = 0.3  # èµ·ç‚¹ - æ·±ç°
        track_display[self.track_map == 3] = 1.0  # ç»ˆç‚¹ - ç™½è‰²
        
        im = ax.imshow(track_display, cmap='gray', aspect='equal')
        
        # æ ‡è®°èµ·ç‚¹å’Œç»ˆç‚¹
        # Mark start and finish
        for x, y in self.start_positions:
            rect = Rectangle((y-0.5, x-0.5), 1, 1,
                           fill=False, edgecolor='green', linewidth=2)
            ax.add_patch(rect)
            ax.text(y, x, 'S', ha='center', va='center',
                   color='green', fontweight='bold')
        
        for x, y in self.finish_positions:
            rect = Rectangle((y-0.5, x-0.5), 1, 1,
                           fill=False, edgecolor='red', linewidth=2)
            ax.add_patch(rect)
            ax.text(y, x, 'F', ha='center', va='center',
                   color='red', fontweight='bold')
        
        # ç»˜åˆ¶è½¨è¿¹
        # Draw trajectory
        if trajectory:
            traj_x = [pos[0] for pos in trajectory]
            traj_y = [pos[1] for pos in trajectory]
            ax.plot(traj_y, traj_x, 'b-', linewidth=2, alpha=0.7, label='Trajectory')
            ax.plot(traj_y[0], traj_x[0], 'go', markersize=10, label='Start')
            ax.plot(traj_y[-1], traj_x[-1], 'ro', markersize=10, label='End')
        
        # ç»˜åˆ¶ç­–ç•¥ç®­å¤´ï¼ˆå¦‚æœæä¾›ï¼‰
        # Draw policy arrows (if provided)
        if policy and hasattr(policy, 'Q'):
            # åœ¨ä¸€äº›å…³é”®ä½ç½®æ˜¾ç¤ºç­–ç•¥
            # Show policy at some key positions
            sample_positions = []
            for i in range(self.height):
                for j in range(self.width):
                    if self.track_map[i, j] == 1:  # èµ›é“ä¸Š
                        if np.random.random() < 0.2:  # 20%é‡‡æ ·ç‡
                            sample_positions.append((i, j))
            
            for x, y in sample_positions:
                # å‡è®¾é€Ÿåº¦ä¸º0
                # Assume velocity is 0
                state = self._get_state((x, y), (0, 0))
                
                # è·å–æœ€ä¼˜åŠ¨ä½œ
                # Get optimal action
                q_values = [policy.Q.get_value(state, a) for a in self.action_space]
                best_action_idx = np.argmax(q_values)
                best_action = self.action_space[best_action_idx]
                
                if hasattr(best_action, 'ax'):
                    ax_val = best_action.ax
                    ay_val = best_action.ay
                    
                    # ç»˜åˆ¶ç®­å¤´
                    # Draw arrow
                    if ax_val != 0 or ay_val != 0:
                        ax.arrow(y, x, ay_val*0.3, ax_val*0.3,
                               head_width=0.1, head_length=0.05,
                               fc='blue', ec='blue', alpha=0.5)
        
        ax.set_xlim(-0.5, self.width - 0.5)
        ax.set_ylim(self.height - 0.5, -0.5)
        ax.set_xlabel('Y')
        ax.set_ylabel('X')
        ax.set_title('Race Track')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        return fig


# ================================================================================
# ç¬¬4.5.3èŠ‚ï¼šMCä¾‹å­è¿è¡Œå™¨
# Section 4.5.3: MC Example Runner
# ================================================================================

class MCExampleRunner:
    """
    MCä¾‹å­è¿è¡Œå™¨
    MC Example Runner
    
    ç»Ÿä¸€è¿è¡Œå’Œåˆ†æç»å…¸MCä¾‹å­
    Run and analyze classic MC examples uniformly
    """
    
    @staticmethod
    def run_blackjack_example(n_episodes: int = 100000):
        """
        è¿è¡Œ21ç‚¹ä¾‹å­
        Run Blackjack example
        
        å±•ç¤ºMCæ§åˆ¶å­¦ä¹ æœ€ä¼˜ç­–ç•¥
        Show MC control learning optimal policy
        """
        print("\n" + "="*80)
        print("21ç‚¹æ¸¸æˆ - MCæ§åˆ¶")
        print("Blackjack - MC Control")
        print("="*80)
        
        # åˆ›å»ºç¯å¢ƒ
        # Create environment
        env = Blackjack()
        
        print(f"\nç¯å¢ƒä¿¡æ¯:")
        print(f"  çŠ¶æ€ç©ºé—´å¤§å°: {len(env.state_space)}")
        print(f"  åŠ¨ä½œç©ºé—´: {[a.id for a in env.action_space]}")
        
        # ä½¿ç”¨MCæ§åˆ¶å­¦ä¹ 
        # Learn using MC control
        print(f"\nå¼€å§‹MCæ§åˆ¶å­¦ä¹  ({n_episodes}å›åˆ)...")
        print(f"Starting MC control learning ({n_episodes} episodes)...")
        
        controller = OnPolicyMCControl(
            env,
            gamma=1.0,  # æ— æŠ˜æ‰£
            epsilon=0.1,
            epsilon_decay=0.9999,
            epsilon_min=0.01,
            visit_type='first'
        )
        
        # å­¦ä¹ 
        # Learn
        start_time = time.time()
        learned_policy = controller.learn(n_episodes, verbose=False)
        elapsed_time = time.time() - start_time
        
        print(f"\nå­¦ä¹ å®Œæˆ:")
        print(f"  æ—¶é—´: {elapsed_time:.1f}ç§’")
        print(f"  æœ€ç»ˆÎµ: {controller.policy.epsilon:.4f}")
        print(f"  è®¿é—®çš„(s,a)å¯¹: {len(controller.sa_visits)}")
        
        # åˆ†æå­¦ä¹ çš„ç­–ç•¥
        # Analyze learned policy
        print(f"\nç­–ç•¥åˆ†æ:")
        
        # æµ‹è¯•å­¦ä¹ çš„ç­–ç•¥
        # Test learned policy
        test_episodes = 10000
        wins = 0
        losses = 0
        draws = 0
        
        for _ in range(test_episodes):
            state = env.reset()
            done = False
            
            while not done:
                action = learned_policy.select_action(state)
                state, reward, done, info = env.step(action)
            
            if reward > 0:
                wins += 1
            elif reward < 0:
                losses += 1
            else:
                draws += 1
        
        print(f"\næµ‹è¯•ç»“æœ ({test_episodes}å›åˆ):")
        print(f"  èƒœç‡: {wins/test_episodes:.1%}")
        print(f"  è´¥ç‡: {losses/test_episodes:.1%}")
        print(f"  å¹³å±€ç‡: {draws/test_episodes:.1%}")
        
        # ä¸ç®€å•ç­–ç•¥æ¯”è¾ƒ
        # Compare with simple policy
        print(f"\nä¸é˜ˆå€¼ç­–ç•¥(threshold=20)æ¯”è¾ƒ:")
        simple_policy = BlackjackPolicy(threshold=20)
        
        simple_wins = 0
        for _ in range(test_episodes):
            state = env.reset()
            done = False
            
            while not done:
                action = simple_policy.select_action(state)
                state, reward, done, info = env.step(action)
            
            if reward > 0:
                simple_wins += 1
        
        print(f"  ç®€å•ç­–ç•¥èƒœç‡: {simple_wins/test_episodes:.1%}")
        print(f"  MCå­¦ä¹ æå‡: {(wins-simple_wins)/test_episodes:.1%}")
        
        # å¯è§†åŒ–ç­–ç•¥
        # Visualize policy
        print(f"\nç”Ÿæˆç­–ç•¥å¯è§†åŒ–...")
        
        # ä¸ºå¯ç”¨Aå’Œéå¯ç”¨Aåˆ†åˆ«å¯è§†åŒ–
        # Visualize for usable and non-usable ace
        fig1 = visualize_blackjack_policy(controller.Q, usable_ace=False)
        fig2 = visualize_blackjack_policy(controller.Q, usable_ace=True)
        
        return controller, [fig1, fig2]
    
    @staticmethod
    def run_racetrack_example(n_episodes: int = 5000):
        """
        è¿è¡Œèµ›é“ä¾‹å­
        Run Racetrack example
        
        å±•ç¤ºMCåœ¨è¿ç»­ç©ºé—´é—®é¢˜çš„åº”ç”¨
        Show MC application in continuous space problem
        """
        print("\n" + "="*80)
        print("èµ›é“é—®é¢˜ - MCæ§åˆ¶")
        print("Racetrack - MC Control")
        print("="*80)
        
        # åˆ›å»ºç¯å¢ƒ
        # Create environment
        env = RaceTrack(track_name="simple")
        
        print(f"\nç¯å¢ƒä¿¡æ¯:")
        print(f"  èµ›é“å¤§å°: {env.height}Ã—{env.width}")
        print(f"  èµ·ç‚¹æ•°: {len(env.start_positions)}")
        print(f"  ç»ˆç‚¹æ•°: {len(env.finish_positions)}")
        print(f"  æœ€å¤§é€Ÿåº¦: {env.max_velocity}")
        print(f"  åŠ¨ä½œæ•°: {len(env.action_space)}")
        
        # MCæ§åˆ¶å­¦ä¹ 
        # MC control learning
        print(f"\nå¼€å§‹MCæ§åˆ¶å­¦ä¹  ({n_episodes}å›åˆ)...")
        
        controller = OnPolicyMCControl(
            env,
            gamma=1.0,
            epsilon=0.2,
            epsilon_decay=0.995,
            epsilon_min=0.05,
            visit_type='first'
        )
        
        # è®°å½•ä¸€äº›æˆåŠŸçš„è½¨è¿¹
        # Record some successful trajectories
        successful_trajectories = []
        episode_lengths = []
        
        for episode_num in range(n_episodes):
            trajectory = []
            state = env.reset()
            trajectory.append(env.position)
            
            done = False
            steps = 0
            max_steps = 1000
            
            while not done and steps < max_steps:
                action = controller.policy.select_action(state)
                next_state, reward, done, info = env.step(action)
                
                trajectory.append(env.position)
                
                # åˆ›å»ºç»éªŒå¹¶æ›´æ–°
                # Create experience and update
                exp = Experience(state, action, reward, next_state, done)
                episode = Episode()
                episode.add_experience(exp)
                
                # ç®€åŒ–ï¼šåªæ›´æ–°è¿™ä¸€æ­¥
                # Simplified: only update this step
                if done and info.get('finished'):
                    controller.update_Q(episode)
                    successful_trajectories.append(trajectory)
                    episode_lengths.append(steps)
                
                state = next_state
                steps += 1
            
            # è¡°å‡epsilon
            # Decay epsilon
            controller.policy.decay_epsilon()
            
            if (episode_num + 1) % 1000 == 0:
                if successful_trajectories:
                    avg_length = np.mean(episode_lengths[-100:]) if len(episode_lengths) > 100 else np.mean(episode_lengths)
                    print(f"  Episode {episode_num + 1}: "
                          f"æˆåŠŸç‡={len(successful_trajectories)/(episode_num+1):.1%}, "
                          f"å¹³å‡é•¿åº¦={avg_length:.1f}")
        
        print(f"\nå­¦ä¹ å®Œæˆ:")
        print(f"  æ€»æˆåŠŸæ¬¡æ•°: {len(successful_trajectories)}")
        print(f"  æˆåŠŸç‡: {len(successful_trajectories)/n_episodes:.1%}")
        
        if successful_trajectories:
            print(f"  æœ€çŸ­è·¯å¾„: {min(episode_lengths)}æ­¥")
            print(f"  å¹³å‡è·¯å¾„: {np.mean(episode_lengths):.1f}æ­¥")
            
            # å¯è§†åŒ–æœ€ä½³è½¨è¿¹
            # Visualize best trajectory
            best_idx = np.argmin(episode_lengths)
            best_trajectory = successful_trajectories[best_idx]
            
            print(f"\nç”Ÿæˆèµ›é“å¯è§†åŒ–...")
            fig = env.visualize_track(
                policy=controller.policy,
                trajectory=best_trajectory
            )
            
            return controller, [fig]
        else:
            print("  è­¦å‘Šï¼šæ²¡æœ‰æˆåŠŸåˆ°è¾¾ç»ˆç‚¹çš„è½¨è¿¹")
            return controller, []
    
    @staticmethod
    def analyze_exploration_importance():
        """
        åˆ†ææ¢ç´¢çš„é‡è¦æ€§
        Analyze importance of exploration
        
        æ¯”è¾ƒä¸åŒÎµå€¼çš„æ•ˆæœ
        Compare effects of different Îµ values
        """
        print("\n" + "="*80)
        print("æ¢ç´¢çš„é‡è¦æ€§åˆ†æ")
        print("Analysis of Exploration Importance")
        print("="*80)
        
        # åœ¨21ç‚¹ä¸Šæµ‹è¯•ä¸åŒçš„Îµ
        # Test different Îµ on Blackjack
        env = Blackjack()
        epsilons = [0.01, 0.05, 0.1, 0.2, 0.3]
        n_episodes = 10000
        
        results = {}
        
        for eps in epsilons:
            print(f"\næµ‹è¯• Îµ={eps}...")
            
            controller = OnPolicyMCControl(
                env,
                gamma=1.0,
                epsilon=eps,
                epsilon_decay=1.0,  # ä¸è¡°å‡
                epsilon_min=eps,
                visit_type='first'
            )
            
            # å­¦ä¹ 
            # Learn
            controller.learn(n_episodes, verbose=False)
            
            # æµ‹è¯•
            # Test
            test_episodes = 1000
            wins = 0
            
            for _ in range(test_episodes):
                state = env.reset()
                done = False
                
                # æµ‹è¯•æ—¶ç”¨è´ªå©ªç­–ç•¥
                # Use greedy policy for testing
                controller.policy.epsilon = 0
                
                while not done:
                    action = controller.policy.select_action(state)
                    state, reward, done, info = env.step(action)
                
                if reward > 0:
                    wins += 1
            
            # æ¢å¤epsilon
            # Restore epsilon
            controller.policy.epsilon = eps
            
            win_rate = wins / test_episodes
            coverage = len(controller.sa_visits)
            
            results[eps] = {
                'win_rate': win_rate,
                'coverage': coverage,
                'visits': controller.sa_visits
            }
            
            print(f"  èƒœç‡: {win_rate:.1%}")
            print(f"  è¦†ç›–(s,a)å¯¹: {coverage}")
        
        # å¯è§†åŒ–æ¯”è¾ƒ
        # Visualize comparison
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
        
        # èƒœç‡vs Îµ
        # Win rate vs Îµ
        eps_list = list(results.keys())
        win_rates = [results[e]['win_rate'] for e in eps_list]
        
        ax1.plot(eps_list, win_rates, 'bo-', linewidth=2, markersize=8)
        ax1.set_xlabel('Epsilon (Îµ)')
        ax1.set_ylabel('Win Rate')
        ax1.set_title('Performance vs Exploration Rate')
        ax1.grid(True, alpha=0.3)
        
        # è¦†ç›–ç‡vs Îµ
        # Coverage vs Îµ
        coverages = [results[e]['coverage'] for e in eps_list]
        
        ax2.plot(eps_list, coverages, 'ro-', linewidth=2, markersize=8)
        ax2.set_xlabel('Epsilon (Îµ)')
        ax2.set_ylabel('State-Action Pairs Covered')
        ax2.set_title('Exploration Coverage vs Îµ')
        ax2.grid(True, alpha=0.3)
        
        plt.suptitle('Exploration-Exploitation Trade-off', fontsize=14, fontweight='bold')
        plt.tight_layout()
        
        print("\n" + "="*60)
        print("å…³é”®è§‚å¯Ÿ:")
        print("Key Observations:")
        print("="*60)
        print("""
        1. ä½Îµ (0.01-0.05):
           - è¦†ç›–ç‡ä½ï¼Œå¯èƒ½é”™è¿‡å¥½ç­–ç•¥
             Low coverage, may miss good policies
           - æ”¶æ•›å¿«ä½†å¯èƒ½æ¬¡ä¼˜
             Fast convergence but possibly suboptimal
        
        2. ä¸­Îµ (0.1-0.2):
           - å¹³è¡¡çš„æ¢ç´¢å’Œåˆ©ç”¨
             Balanced exploration and exploitation
           - é€šå¸¸è·å¾—æœ€ä½³æ€§èƒ½
             Usually achieves best performance
        
        3. é«˜Îµ (0.3+):
           - é«˜è¦†ç›–ç‡ä½†æ”¶æ•›æ…¢
             High coverage but slow convergence
           - è¿‡åº¦æ¢ç´¢å½±å“æ€§èƒ½
             Excessive exploration hurts performance
        
        4. æœ€ä¼˜ç­–ç•¥:
           - å¼€å§‹é«˜Îµï¼Œé€æ¸è¡°å‡
             Start with high Îµ, gradually decay
           - æ—©æœŸæ¢ç´¢ï¼ŒåæœŸåˆ©ç”¨
             Early exploration, later exploitation
        """)
        
        return fig


# ================================================================================
# ç¬¬4.5.4èŠ‚ï¼šç»¼åˆæ¼”ç¤º
# Section 4.5.4: Comprehensive Demo
# ================================================================================

def demonstrate_mc_examples():
    """
    ç»¼åˆæ¼”ç¤ºMCç»å…¸ä¾‹å­
    Comprehensive demonstration of MC classic examples
    """
    print("\n" + "="*80)
    print("è’™ç‰¹å¡æ´›æ–¹æ³•ç»å…¸ä¾‹å­æ¼”ç¤º")
    print("Monte Carlo Classic Examples Demonstration")
    print("="*80)
    
    # 1. 21ç‚¹ä¾‹å­
    # 1. Blackjack example
    print("\n" + "="*60)
    print("ä¾‹å­1ï¼š21ç‚¹æ¸¸æˆ")
    print("Example 1: Blackjack")
    print("="*60)
    
    blackjack_controller, blackjack_figs = MCExampleRunner.run_blackjack_example(
        n_episodes=50000  # å‡å°‘ç”¨äºæ¼”ç¤º
    )
    
    # 2. èµ›é“ä¾‹å­
    # 2. Racetrack example
    print("\n" + "="*60)
    print("ä¾‹å­2ï¼šèµ›é“é—®é¢˜")
    print("Example 2: Racetrack")
    print("="*60)
    
    racetrack_controller, racetrack_figs = MCExampleRunner.run_racetrack_example(
        n_episodes=2000  # å‡å°‘ç”¨äºæ¼”ç¤º
    )
    
    # 3. æ¢ç´¢é‡è¦æ€§åˆ†æ
    # 3. Exploration importance analysis
    print("\n" + "="*60)
    print("åˆ†æï¼šæ¢ç´¢çš„é‡è¦æ€§")
    print("Analysis: Importance of Exploration")
    print("="*60)
    
    exploration_fig = MCExampleRunner.analyze_exploration_importance()
    
    # æ€»ç»“
    # Summary
    print("\n" + "="*80)
    print("MCæ–¹æ³•æ€»ç»“")
    print("MC Methods Summary")
    print("="*80)
    print("""
    ğŸ“š å…³é”®å­¦ä¹ ç‚¹ Key Learning Points:
    =====================================
    
    1. MCçš„é€‚ç”¨åœºæ™¯:
       When to use MC:
       - å›åˆå¼ä»»åŠ¡
         Episodic tasks
       - ä¸éœ€è¦/æ²¡æœ‰æ¨¡å‹
         No model needed/available
       - å¯ä»¥æ¨¡æ‹Ÿ/é‡‡æ ·
         Can simulate/sample
    
    2. MCçš„ä¼˜åŠ¿:
       MC Advantages:
       - æ— æ¨¡å‹å­¦ä¹ 
         Model-free learning
       - å¤„ç†éšæœºæ€§
         Handles stochasticity
       - æ”¶æ•›åˆ°çœŸå®å€¼
         Converges to true values
    
    3. MCçš„æŒ‘æˆ˜:
       MC Challenges:
       - é«˜æ–¹å·®
         High variance
       - éœ€è¦å®Œæ•´å›åˆ
         Needs complete episodes
       - æ¢ç´¢-åˆ©ç”¨æƒè¡¡
         Exploration-exploitation trade-off
    
    4. 21ç‚¹ä¾‹å­å±•ç¤º:
       Blackjack shows:
       - éƒ¨åˆ†å¯è§‚æµ‹ä¹Ÿèƒ½å­¦ä¹ 
         Can learn with partial observability
       - ç®€å•ç¯å¢ƒçš„æœ€ä¼˜ç­–ç•¥
         Optimal policy for simple environment
       - MCæ§åˆ¶çš„æœ‰æ•ˆæ€§
         Effectiveness of MC control
    
    5. èµ›é“ä¾‹å­å±•ç¤º:
       Racetrack shows:
       - è¿ç»­ç©ºé—´çš„å¤„ç†
         Handling continuous space
       - å»¶è¿Ÿå¥–åŠ±çš„å­¦ä¹ 
         Learning with delayed rewards
       - æ¢ç´¢çš„å¿…è¦æ€§
         Necessity of exploration
    
    6. å‘TDæ–¹æ³•çš„è¿‡æ¸¡:
       Transition to TD:
       - MCçš„é«˜æ–¹å·®æ¿€å‘äº†TD
         MC's high variance motivated TD
       - TD = MC + DPçš„ä¼˜ç‚¹
         TD = MC + DP advantages
       - ä¸‹ä¸€ç« ï¼šTDå­¦ä¹ 
         Next chapter: TD learning
    """)
    print("="*80)
    
    # æ˜¾ç¤ºæ‰€æœ‰å›¾
    # Show all figures
    plt.show()


# ================================================================================
# ä¸»å‡½æ•°
# Main Function
# ================================================================================

def main():
    """
    è¿è¡ŒMCä¾‹å­æ¼”ç¤º
    Run MC examples demo
    """
    demonstrate_mc_examples()


if __name__ == "__main__":
    main()