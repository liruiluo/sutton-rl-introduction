"""
================================================================================
ç¬¬5.2èŠ‚ï¼šè’™ç‰¹å¡æ´›é¢„æµ‹ - ä»ç»éªŒä¸­ä¼°è®¡ä»·å€¼å‡½æ•°
Section 5.2: Monte Carlo Prediction - Estimating Value Functions from Experience
================================================================================

MCé¢„æµ‹æ˜¯MCæ–¹æ³•çš„æ ¸å¿ƒï¼Œå±•ç¤ºäº†å¦‚ä½•ä¸ç”¨æ¨¡å‹æ¥ä¼°è®¡ä»·å€¼å‡½æ•°ã€‚
MC prediction is the core of MC methods, showing how to estimate value functions without a model.

ä¸¤ç§ä¸»è¦å˜ä½“ï¼š
Two main variants:
1. First-Visit MCï¼šåªä½¿ç”¨æ¯ä¸ªçŠ¶æ€çš„ç¬¬ä¸€æ¬¡è®¿é—®
   First-Visit MC: Only use first visit to each state
2. Every-Visit MCï¼šä½¿ç”¨æ¯ä¸ªçŠ¶æ€çš„æ‰€æœ‰è®¿é—®
   Every-Visit MC: Use all visits to each state

å…³é”®ç®—æ³•ï¼š
Key algorithms:
1. æ‰¹é‡MCï¼ˆå­˜å‚¨æ‰€æœ‰å›æŠ¥ï¼‰
   Batch MC (stores all returns)
2. å¢é‡MCï¼ˆåªå­˜å‚¨å¹³å‡å€¼ï¼‰
   Incremental MC (only stores mean)
3. å¸¸æ•°æ­¥é•¿MCï¼ˆé€‚åº”éå¹³ç¨³ï¼‰
   Constant-step MC (adapts to non-stationarity)

ç†è®ºä¿è¯ï¼š
Theoretical guarantees:
- First-Visit MCï¼šæ— åã€ç‹¬ç«‹æ ·æœ¬ã€æ”¶æ•›åˆ°çœŸå®å€¼
  First-Visit MC: Unbiased, independent samples, converges to true value
- Every-Visit MCï¼šæ— åã€ç›¸å…³æ ·æœ¬ã€ä¹Ÿæ”¶æ•›åˆ°çœŸå®å€¼
  Every-Visit MC: Unbiased, correlated samples, also converges to true value

å®è·µè€ƒè™‘ï¼š
Practical considerations:
- First-Visitæ›´ç†è®ºå‹å¥½ï¼ˆç‹¬ç«‹æ€§ï¼‰
  First-Visit more theory-friendly (independence)
- Every-Visitæ›´æ•°æ®é«˜æ•ˆï¼ˆæ›´å¤šæ ·æœ¬ï¼‰
  Every-Visit more data-efficient (more samples)
- å¢é‡æ›´æ–°èŠ‚çœå†…å­˜
  Incremental updates save memory
- å¸¸æ•°æ­¥é•¿é€‚åº”å˜åŒ–
  Constant step-size adapts to changes
"""

import numpy as np
from typing import List, Dict, Tuple, Optional, Any, Callable, Union
from dataclasses import dataclass, field
from collections import defaultdict, deque
import logging
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
import time
from abc import ABC, abstractmethod

# å¯¼å…¥åŸºç¡€ç»„ä»¶
import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent))

from src.ch03_finite_mdp.mdp_framework import State, Action, MDPEnvironment
from src.ch03_finite_mdp.policies_and_values import (
    Policy, StateValueFunction, ActionValueFunction,
    StochasticPolicy, DeterministicPolicy
)
from .mc_foundations import (
    Episode, Experience, Return, MCStatistics
)

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


# ================================================================================
# ç¬¬5.2.1èŠ‚ï¼šMCé¢„æµ‹åŸºç±»
# Section 5.2.1: MC Prediction Base Class
# ================================================================================

class MCPrediction(ABC):
    """
    è’™ç‰¹å¡æ´›é¢„æµ‹åŸºç±»
    Monte Carlo Prediction Base Class
    
    å®šä¹‰äº†æ‰€æœ‰MCé¢„æµ‹ç®—æ³•çš„å…±åŒæ¥å£
    Defines common interface for all MC prediction algorithms
    
    è®¾è®¡åŸåˆ™ï¼š
    Design principles:
    1. ç»Ÿä¸€çš„ä¼°è®¡æ¥å£
       Unified estimation interface
    2. çµæ´»çš„è®¿é—®ç­–ç•¥ï¼ˆfirst/everyï¼‰
       Flexible visit strategy (first/every)
    3. æ”¯æŒå¢é‡å’Œæ‰¹é‡æ›´æ–°
       Support incremental and batch updates
    4. å®Œæ•´çš„ç»Ÿè®¡è·Ÿè¸ª
       Complete statistics tracking
    
    ä¸ºä»€ä¹ˆéœ€è¦åŸºç±»ï¼Ÿ
    Why need base class?
    - ç¡®ä¿æ‰€æœ‰MCç®—æ³•æœ‰ä¸€è‡´çš„æ¥å£
      Ensure all MC algorithms have consistent interface
    - å…±äº«é€šç”¨åŠŸèƒ½ï¼ˆé‡‡æ ·ã€ç»Ÿè®¡ï¼‰
      Share common functionality (sampling, statistics)
    - æ–¹ä¾¿ç®—æ³•æ¯”è¾ƒå’Œåˆ‡æ¢
      Easy algorithm comparison and switching
    """
    
    def __init__(self, 
                 env: MDPEnvironment,
                 gamma: float = 1.0,
                 visit_type: str = 'first'):
        """
        åˆå§‹åŒ–MCé¢„æµ‹
        Initialize MC Prediction
        
        Args:
            env: MDPç¯å¢ƒ
                MDP environment
            gamma: æŠ˜æ‰£å› å­
                  Discount factor
            visit_type: è®¿é—®ç±»å‹ 'first' æˆ– 'every'
                       Visit type 'first' or 'every'
        
        è®¾è®¡è€ƒè™‘ï¼š
        Design considerations:
        - gamma=1.0æ˜¯MCçš„å¸¸è§é€‰æ‹©ï¼ˆæ— æŠ˜æ‰£ï¼‰
          gamma=1.0 is common for MC (undiscounted)
        - visit_typeå½±å“æ”¶æ•›é€Ÿåº¦å’Œæ–¹å·®
          visit_type affects convergence speed and variance
        """
        self.env = env
        self.gamma = gamma
        self.visit_type = visit_type
        
        # åˆå§‹åŒ–ä»·å€¼å‡½æ•°
        # Initialize value functions
        self.V = StateValueFunction(env.state_space, initial_value=0.0)
        self.Q = ActionValueFunction(env.state_space, env.action_space, initial_value=0.0)
        
        # ç»Ÿè®¡æ”¶é›†
        # Statistics collection
        self.statistics = MCStatistics()
        
        # è®°å½•æ‰€æœ‰å›åˆ
        # Record all episodes
        self.episodes: List[Episode] = []
        
        # è®¿é—®è®¡æ•°
        # Visit counts
        self.state_visits: Dict[str, int] = defaultdict(int)
        self.state_action_visits: Dict[Tuple[str, str], int] = defaultdict(int)
        
        # æ”¶æ•›å†å²
        # Convergence history
        self.convergence_history: List[float] = []
        
        logger.info(f"åˆå§‹åŒ–{visit_type}-visit MCé¢„æµ‹, Î³={gamma}")
    
    @abstractmethod
    def update_value(self, episode: Episode):
        """
        æ›´æ–°ä»·å€¼å‡½æ•°ï¼ˆå­ç±»å®ç°ï¼‰
        Update value function (implemented by subclasses)
        
        è¿™æ˜¯MCç®—æ³•çš„æ ¸å¿ƒå·®å¼‚ç‚¹
        This is the key difference point of MC algorithms
        """
        pass
    
    def generate_episode(self, policy: Policy, max_steps: int = 1000) -> Episode:
        """
        ç”Ÿæˆä¸€ä¸ªå›åˆ
        Generate an episode
        
        éµå¾ªç­–ç•¥Ï€ç›´åˆ°ç»ˆæ­¢
        Follow policy Ï€ until termination
        
        Args:
            policy: è¦è¯„ä¼°çš„ç­–ç•¥
                   Policy to evaluate
            max_steps: æœ€å¤§æ­¥æ•°ï¼ˆé¿å…æ— é™å¾ªç¯ï¼‰
                      Maximum steps (avoid infinite loop)
        
        Returns:
            å®Œæ•´çš„å›åˆ
            Complete episode
        
        å®ç°ç»†èŠ‚ï¼š
        Implementation details:
        - ä½¿ç”¨ç¯å¢ƒçš„stepå‡½æ•°æ¨¡æ‹Ÿ
          Use environment's step function to simulate
        - è®°å½•å®Œæ•´è½¨è¿¹
          Record complete trajectory
        - å¤„ç†æœ€å¤§æ­¥æ•°é™åˆ¶
          Handle maximum steps limit
        """
        episode = Episode()
        state = self.env.reset()
        
        for t in range(max_steps):
            # æ ¹æ®ç­–ç•¥é€‰æ‹©åŠ¨ä½œ
            # Select action according to policy
            action = policy.select_action(state)
            
            # æ‰§è¡ŒåŠ¨ä½œ
            # Execute action
            next_state, reward, done, info = self.env.step(action)
            
            # è®°å½•ç»éªŒ
            # Record experience
            exp = Experience(
                state=state,
                action=action,
                reward=reward,
                next_state=next_state,
                done=done
            )
            episode.add_experience(exp)
            
            # æ›´æ–°çŠ¶æ€
            # Update state
            state = next_state
            
            if done:
                break
        
        if not episode.is_complete():
            logger.warning(f"å›åˆæœªæ­£å¸¸ç»“æŸï¼ˆè¾¾åˆ°æœ€å¤§æ­¥æ•°{max_steps}ï¼‰")
        
        return episode
    
    def estimate_V(self, 
                   policy: Policy,
                   n_episodes: int = 1000,
                   verbose: bool = True) -> StateValueFunction:
        """
        ä¼°è®¡çŠ¶æ€ä»·å€¼å‡½æ•° V^Ï€
        Estimate state value function V^Ï€
        
        è¿™æ˜¯MCé¢„æµ‹çš„ä¸»å‡½æ•°
        This is the main function of MC prediction
        
        Args:
            policy: è¦è¯„ä¼°çš„ç­–ç•¥
                   Policy to evaluate
            n_episodes: é‡‡æ ·å›åˆæ•°
                       Number of episodes to sample
            verbose: æ˜¯å¦è¾“å‡ºè¿›åº¦
                    Whether to output progress
        
        Returns:
            ä¼°è®¡çš„çŠ¶æ€ä»·å€¼å‡½æ•°
            Estimated state value function
        
        ç®—æ³•æµç¨‹ï¼š
        Algorithm flow:
        1. ç”Ÿæˆå›åˆ
           Generate episode
        2. è®¡ç®—å›æŠ¥
           Compute returns
        3. æ›´æ–°ä¼°è®¡
           Update estimates
        4. é‡å¤ç›´åˆ°æ”¶æ•›
           Repeat until convergence
        """
        if verbose:
            print(f"\nå¼€å§‹{self.visit_type}-visit MCä¼°è®¡V^Ï€")
            print(f"Starting {self.visit_type}-visit MC estimation of V^Ï€")
            print(f"  ç¯å¢ƒ: {self.env.name}")
            print(f"  å›åˆæ•°: {n_episodes}")
            print(f"  æŠ˜æ‰£å› å­: {self.gamma}")
        
        start_time = time.time()
        
        for episode_num in range(n_episodes):
            # ç”Ÿæˆå›åˆ
            # Generate episode
            episode = self.generate_episode(policy)
            self.episodes.append(episode)
            
            # æ›´æ–°ä»·å€¼ä¼°è®¡
            # Update value estimates
            self.update_value(episode)
            
            # è®°å½•æ”¶æ•›å†å²ï¼ˆæ¯10ä¸ªå›åˆï¼‰
            # Record convergence history (every 10 episodes)
            if (episode_num + 1) % 10 == 0:
                # è®¡ç®—æœ€å¤§å˜åŒ–
                # Compute maximum change
                max_change = self._compute_max_change()
                self.convergence_history.append(max_change)
                
                if verbose and (episode_num + 1) % 100 == 0:
                    elapsed = time.time() - start_time
                    print(f"  Episode {episode_num + 1}/{n_episodes}: "
                          f"max_change={max_change:.6f}, "
                          f"time={elapsed:.1f}s")
        
        total_time = time.time() - start_time
        
        if verbose:
            print(f"\nMCé¢„æµ‹å®Œæˆ:")
            print(f"  æ€»æ—¶é—´: {total_time:.2f}ç§’")
            print(f"  å¹³å‡æ¯å›åˆ: {total_time/n_episodes*1000:.1f}æ¯«ç§’")
            print(f"  è®¿é—®çŠ¶æ€æ•°: {len(self.state_visits)}")
            print(f"  å¹³å‡è®¿é—®æ¬¡æ•°: {np.mean(list(self.state_visits.values())):.1f}")
        
        return self.V
    
    def estimate_Q(self,
                   policy: Policy,
                   n_episodes: int = 1000,
                   verbose: bool = True) -> ActionValueFunction:
        """
        ä¼°è®¡åŠ¨ä½œä»·å€¼å‡½æ•° Q^Ï€
        Estimate action value function Q^Ï€
        
        ä¸Vä¼°è®¡ç±»ä¼¼ï¼Œä½†è¿½è¸ª(s,a)å¯¹
        Similar to V estimation but tracks (s,a) pairs
        
        Qå‡½æ•°å¯¹æ§åˆ¶æ›´é‡è¦ï¼
        Q function is more important for control!
        """
        if verbose:
            print(f"\nå¼€å§‹{self.visit_type}-visit MCä¼°è®¡Q^Ï€")
            print(f"Starting {self.visit_type}-visit MC estimation of Q^Ï€")
        
        start_time = time.time()
        
        for episode_num in range(n_episodes):
            # ç”Ÿæˆå›åˆ
            episode = self.generate_episode(policy)
            self.episodes.append(episode)
            
            # æ›´æ–°Qå€¼ä¼°è®¡
            self._update_Q_values(episode)
            
            if verbose and (episode_num + 1) % 100 == 0:
                print(f"  Episode {episode_num + 1}/{n_episodes}")
        
        total_time = time.time() - start_time
        
        if verbose:
            print(f"\nQä¼°è®¡å®Œæˆ: {total_time:.2f}ç§’")
            print(f"  è®¿é—®(s,a)å¯¹: {len(self.state_action_visits)}")
        
        return self.Q
    
    def _update_Q_values(self, episode: Episode):
        """
        æ›´æ–°Qå€¼
        Update Q values
        
        å¤„ç†(çŠ¶æ€,åŠ¨ä½œ)å¯¹çš„å›æŠ¥
        Process returns for (state,action) pairs
        """
        # è®¡ç®—å›æŠ¥
        returns = episode.compute_returns(self.gamma)
        
        # æ ¹æ®è®¿é—®ç±»å‹è·å–ç´¢å¼•
        if self.visit_type == 'first':
            # First-visit: åªä½¿ç”¨é¦–æ¬¡è®¿é—®
            sa_pairs_seen = set()
            for t, exp in enumerate(episode.experiences):
                sa_pair = (exp.state.id, exp.action.id)
                if sa_pair not in sa_pairs_seen:
                    sa_pairs_seen.add(sa_pair)
                    G = returns[t]
                    self.statistics.update_action_value(exp.state, exp.action, G)
                    self.state_action_visits[sa_pair] += 1
                    
                    # å¢é‡æ›´æ–°Q
                    old_q = self.Q.get_value(exp.state, exp.action)
                    n = self.state_action_visits[sa_pair]
                    new_q = old_q + (G - old_q) / n
                    self.Q.set_value(exp.state, exp.action, new_q)
        
        else:  # every-visit
            # Every-visit: ä½¿ç”¨æ‰€æœ‰è®¿é—®
            for t, exp in enumerate(episode.experiences):
                sa_pair = (exp.state.id, exp.action.id)
                G = returns[t]
                self.statistics.update_action_value(exp.state, exp.action, G)
                self.state_action_visits[sa_pair] += 1
                
                # å¢é‡æ›´æ–°Q
                old_q = self.Q.get_value(exp.state, exp.action)
                n = self.state_action_visits[sa_pair]
                new_q = old_q + (G - old_q) / n
                self.Q.set_value(exp.state, exp.action, new_q)
    
    def _compute_max_change(self) -> float:
        """
        è®¡ç®—æœ€å¤§ä»·å€¼å˜åŒ–
        Compute maximum value change
        
        ç”¨äºç›‘æ§æ”¶æ•›
        Used to monitor convergence
        """
        if len(self.episodes) < 2:
            return float('inf')
        
        # ç®€åŒ–ï¼šè¿”å›æœ€è¿‘çš„å¹³å‡å›æŠ¥å˜åŒ–
        # Simplified: return recent average return change
        recent_returns = []
        for episode in self.episodes[-10:]:
            if episode.experiences:
                returns = episode.compute_returns(self.gamma)
                if returns:
                    recent_returns.append(returns[0])
        
        if len(recent_returns) < 2:
            return float('inf')
        
        return np.std(recent_returns)


# ================================================================================
# ç¬¬5.2.2èŠ‚ï¼šFirst-Visit MCé¢„æµ‹
# Section 5.2.2: First-Visit MC Prediction
# ================================================================================

class FirstVisitMC(MCPrediction):
    """
    First-Visitè’™ç‰¹å¡æ´›é¢„æµ‹
    First-Visit Monte Carlo Prediction
    
    åªä½¿ç”¨æ¯ä¸ªçŠ¶æ€çš„é¦–æ¬¡è®¿é—®æ¥æ›´æ–°ä¼°è®¡
    Only use first visit to each state to update estimates
    
    ç†è®ºæ€§è´¨ï¼š
    Theoretical properties:
    - æ¯ä¸ªå›åˆå¯¹æ¯ä¸ªçŠ¶æ€æœ€å¤šè´¡çŒ®ä¸€ä¸ªæ ·æœ¬
      Each episode contributes at most one sample per state
    - æ ·æœ¬ä¹‹é—´ç‹¬ç«‹ï¼ˆé‡è¦ï¼ï¼‰
      Samples are independent (important!)
    - æ”¶æ•›åˆ°çœŸå®ä»·å€¼ï¼ˆå¤§æ•°å®šå¾‹ï¼‰
      Converges to true value (law of large numbers)
    
    ç®—æ³•æ­¥éª¤ï¼š
    Algorithm steps:
    1. ç”Ÿæˆå›åˆ Ï„ = (Sâ‚€, Aâ‚€, Râ‚, Sâ‚, ..., Sâ‚œ)
       Generate episode Ï„ = (Sâ‚€, Aâ‚€, Râ‚, Sâ‚, ..., Sâ‚œ)
    2. å¯¹æ¯ä¸ªå‡ºç°çš„çŠ¶æ€s:
       For each state s appearing in episode:
       - æ‰¾åˆ°sçš„é¦–æ¬¡å‡ºç°æ—¶é—´t
         Find first occurrence time t of s
       - è®¡ç®—ä»tå¼€å§‹çš„å›æŠ¥G_t
         Compute return G_t from time t
       - æ›´æ–°: V(s) â† V(s) + Î±[G_t - V(s)]
         Update: V(s) â† V(s) + Î±[G_t - V(s)]
    
    ä¸ºä»€ä¹ˆå«"First-Visit"ï¼Ÿ
    Why called "First-Visit"?
    å› ä¸ºå¦‚æœä¸€ä¸ªçŠ¶æ€åœ¨å›åˆä¸­å‡ºç°å¤šæ¬¡ï¼Œåªä½¿ç”¨ç¬¬ä¸€æ¬¡
    Because if a state appears multiple times, only use the first
    
    ç±»æ¯”ï¼šç¬¬ä¸€å°è±¡
    Analogy: First impression
    å°±åƒåªç”¨ç¬¬ä¸€å°è±¡æ¥åˆ¤æ–­ä¸€ä¸ªäººï¼Œå¿½ç•¥åç»­çš„æ¥è§¦
    Like judging a person only by first impression, ignoring later encounters
    """
    
    def __init__(self, env: MDPEnvironment, gamma: float = 1.0):
        """
        åˆå§‹åŒ–First-Visit MC
        Initialize First-Visit MC
        """
        super().__init__(env, gamma, visit_type='first')
        
        # First-visitç‰¹æœ‰ï¼šè®°å½•æ¯ä¸ªçŠ¶æ€çš„æ‰€æœ‰é¦–æ¬¡å›æŠ¥
        # First-visit specific: record all first-visit returns for each state
        self.first_returns: Dict[str, List[float]] = defaultdict(list)
        
        logger.info("åˆå§‹åŒ–First-Visit MCé¢„æµ‹")
    
    def update_value(self, episode: Episode):
        """
        ä½¿ç”¨first-visitæ›´æ–°ä»·å€¼
        Update value using first-visit
        
        æ ¸å¿ƒé€»è¾‘ï¼šåªå¤„ç†æ¯ä¸ªçŠ¶æ€çš„ç¬¬ä¸€æ¬¡å‡ºç°
        Core logic: only process first occurrence of each state
        
        æ•°å­¦æ›´æ–°ï¼š
        Mathematical update:
        V(s) = (1/n(s)) Î£áµ¢ Gáµ¢(s)
        å…¶ä¸­Gáµ¢(s)æ˜¯ç¬¬iä¸ªå›åˆä¸­sé¦–æ¬¡å‡ºç°çš„å›æŠ¥
        where Gáµ¢(s) is return from first occurrence of s in episode i
        """
        # è®¡ç®—æ•´ä¸ªå›åˆçš„å›æŠ¥
        # Compute returns for entire episode
        returns = episode.compute_returns(self.gamma)
        
        # è®°å½•å·²è®¿é—®çŠ¶æ€ï¼ˆç”¨äºfirst-visitï¼‰
        # Track visited states (for first-visit)
        visited_states = set()
        
        # éå†å›åˆä¸­çš„æ¯ä¸€æ­¥
        # Iterate through each step in episode
        for t, exp in enumerate(episode.experiences):
            state = exp.state
            
            # First-visit: åªå¤„ç†é¦–æ¬¡è®¿é—®
            # First-visit: only process first visit
            if state.id not in visited_states:
                visited_states.add(state.id)
                
                # è·å–ä»æ—¶é—´tå¼€å§‹çš„å›æŠ¥
                # Get return starting from time t
                G = returns[t]
                
                # è®°å½•è¿™ä¸ªå›æŠ¥
                # Record this return
                self.first_returns[state.id].append(G)
                
                # æ›´æ–°ç»Ÿè®¡
                # Update statistics
                self.statistics.update_state_value(state, G)
                self.state_visits[state.id] += 1
                
                # å¢é‡æ›´æ–°ä»·å€¼ä¼°è®¡
                # Incremental value update
                # V(s) â† V(s) + (1/n)[G - V(s)]
                n = self.state_visits[state.id]
                old_v = self.V.get_value(state)
                new_v = old_v + (G - old_v) / n
                self.V.set_value(state, new_v)
                
                # è¯¦ç»†æ—¥å¿—ï¼ˆè°ƒè¯•ç”¨ï¼‰
                # Detailed logging (for debugging)
                if logger.level == logging.DEBUG:
                    logger.debug(f"First-visitæ›´æ–°: {state.id}")
                    logger.debug(f"  å›æŠ¥G = {G:.3f}")
                    logger.debug(f"  æ—§V = {old_v:.3f}")
                    logger.debug(f"  æ–°V = {new_v:.3f}")
                    logger.debug(f"  è®¿é—®æ¬¡æ•° = {n}")
    
    def analyze_convergence(self, true_values: Optional[Dict[str, float]] = None):
        """
        åˆ†æfirst-visitæ”¶æ•›æ€§
        Analyze first-visit convergence
        
        å±•ç¤ºç‹¬ç«‹æ ·æœ¬çš„ä¼˜åŠ¿
        Show advantage of independent samples
        """
        print("\n" + "="*60)
        print("First-Visit MCæ”¶æ•›åˆ†æ")
        print("First-Visit MC Convergence Analysis")
        print("="*60)
        
        # ç»Ÿè®¡æ¯ä¸ªçŠ¶æ€çš„ä¿¡æ¯
        # Statistics for each state
        for state_id, returns_list in self.first_returns.items():
            if len(returns_list) > 0:
                mean = np.mean(returns_list)
                std = np.std(returns_list) if len(returns_list) > 1 else 0
                n = len(returns_list)
                
                print(f"\nçŠ¶æ€ {state_id}:")
                print(f"  é¦–æ¬¡è®¿é—®æ¬¡æ•°: {n}")
                print(f"  å¹³å‡å›æŠ¥: {mean:.3f}")
                print(f"  æ ‡å‡†å·®: {std:.3f}")
                
                if n > 1:
                    # è®¡ç®—æ ‡å‡†è¯¯å·®
                    # Compute standard error
                    se = std / np.sqrt(n)
                    
                    # 95%ç½®ä¿¡åŒºé—´
                    # 95% confidence interval
                    ci_lower = mean - 1.96 * se
                    ci_upper = mean + 1.96 * se
                    
                    print(f"  æ ‡å‡†è¯¯å·®: {se:.3f}")
                    print(f"  95% CI: [{ci_lower:.3f}, {ci_upper:.3f}]")
                
                # å¦‚æœæœ‰çœŸå®å€¼ï¼Œè®¡ç®—è¯¯å·®
                # If true values available, compute error
                if true_values and state_id in true_values:
                    true_v = true_values[state_id]
                    error = abs(mean - true_v)
                    print(f"  çœŸå®å€¼: {true_v:.3f}")
                    print(f"  è¯¯å·®: {error:.3f}")
                    
                    # æ£€éªŒæ˜¯å¦åœ¨ç½®ä¿¡åŒºé—´å†…
                    # Check if true value in confidence interval
                    if n > 1:
                        if ci_lower <= true_v <= ci_upper:
                            print(f"  âœ“ çœŸå®å€¼åœ¨95% CIå†…")
                        else:
                            print(f"  âœ— çœŸå®å€¼ä¸åœ¨95% CIå†…")
        
        # å±•ç¤ºæ ·æœ¬ç‹¬ç«‹æ€§çš„å¥½å¤„
        # Show benefit of sample independence
        print("\n" + "="*40)
        print("First-Visitçš„ç‹¬ç«‹æ€§ä¼˜åŠ¿:")
        print("Independence Advantage of First-Visit:")
        print("="*40)
        print("""
        1. æ ·æœ¬ç‹¬ç«‹ â†’ æ–¹å·®å…¬å¼ç®€å•
           Independent samples â†’ Simple variance formula
           Var[mean] = Var[G]/n
        
        2. ä¸­å¿ƒæé™å®šç†ç›´æ¥é€‚ç”¨
           Central Limit Theorem directly applies
           mean ~ N(Î¼, ÏƒÂ²/n)
        
        3. ç½®ä¿¡åŒºé—´æ„é€ ç®€å•
           Simple confidence interval construction
           CI = mean Â± z_{Î±/2} Ã— SE
        
        4. ç»Ÿè®¡æ£€éªŒæ›´å¯é 
           More reliable statistical tests
        """)


# ================================================================================
# ç¬¬5.2.3èŠ‚ï¼šEvery-Visit MCé¢„æµ‹
# Section 5.2.3: Every-Visit MC Prediction
# ================================================================================

class EveryVisitMC(MCPrediction):
    """
    Every-Visitè’™ç‰¹å¡æ´›é¢„æµ‹
    Every-Visit Monte Carlo Prediction
    
    ä½¿ç”¨æ¯ä¸ªçŠ¶æ€çš„æ‰€æœ‰è®¿é—®æ¥æ›´æ–°ä¼°è®¡
    Use all visits to each state to update estimates
    
    ç†è®ºæ€§è´¨ï¼š
    Theoretical properties:
    - æ¯ä¸ªå›åˆå¯èƒ½è´¡çŒ®å¤šä¸ªæ ·æœ¬
      Each episode may contribute multiple samples
    - æ ·æœ¬ä¹‹é—´å¯èƒ½ç›¸å…³ï¼ˆæ³¨æ„ï¼ï¼‰
      Samples may be correlated (note!)
    - ä»ç„¶æ”¶æ•›åˆ°çœŸå®ä»·å€¼ï¼ˆä½†ç†è®ºæ›´å¤æ‚ï¼‰
      Still converges to true value (but theory more complex)
    - å®è·µä¸­å¸¸å¸¸æ”¶æ•›æ›´å¿«ï¼ˆæ›´å¤šæ•°æ®ï¼‰
      Often converges faster in practice (more data)
    
    ç®—æ³•æ­¥éª¤ï¼š
    Algorithm steps:
    1. ç”Ÿæˆå›åˆ Ï„
       Generate episode Ï„
    2. å¯¹æ¯ä¸ª(s,t)ï¼Œå…¶ä¸­såœ¨æ—¶é—´tå‡ºç°:
       For each (s,t) where s appears at time t:
       - è®¡ç®—ä»tå¼€å§‹çš„å›æŠ¥G_t
         Compute return G_t from time t
       - æ›´æ–°: V(s) â† V(s) + Î±[G_t - V(s)]
         Update: V(s) â† V(s) + Î±[G_t - V(s)]
    
    ä¸ºä»€ä¹ˆç”¨Every-Visitï¼Ÿ
    Why use Every-Visit?
    - æ›´å¤šæ•°æ® â†’ æ½œåœ¨æ›´å¿«æ”¶æ•›
      More data â†’ Potentially faster convergence
    - æŸäº›ç¯å¢ƒä¸­çŠ¶æ€è®¿é—®ç¨€ç–
      State visits sparse in some environments
    - å®è·µä¸­è¡¨ç°å¸¸å¸¸å¾ˆå¥½
      Often performs well in practice
    
    ç±»æ¯”ï¼šå¤šæ¬¡é‡‡æ ·
    Analogy: Multiple sampling
    å°±åƒå¤šæ¬¡å“å°åŒä¸€é“èœæ¥è¯„ä»·å‘³é“
    Like tasting the same dish multiple times to evaluate taste
    """
    
    def __init__(self, env: MDPEnvironment, gamma: float = 1.0):
        """
        åˆå§‹åŒ–Every-Visit MC
        Initialize Every-Visit MC
        """
        super().__init__(env, gamma, visit_type='every')
        
        # Every-visitç‰¹æœ‰ï¼šè®°å½•æ‰€æœ‰å›æŠ¥ï¼ˆåŒ…æ‹¬é‡å¤è®¿é—®ï¼‰
        # Every-visit specific: record all returns (including repeated visits)
        self.all_returns: Dict[str, List[float]] = defaultdict(list)
        
        # è®°å½•è®¿é—®æ¨¡å¼ï¼ˆç”¨äºåˆ†æç›¸å…³æ€§ï¼‰
        # Record visit patterns (for correlation analysis)
        self.visit_patterns: List[List[str]] = []
        
        logger.info("åˆå§‹åŒ–Every-Visit MCé¢„æµ‹")
    
    def update_value(self, episode: Episode):
        """
        ä½¿ç”¨every-visitæ›´æ–°ä»·å€¼
        Update value using every-visit
        
        æ ¸å¿ƒåŒºåˆ«ï¼šå¤„ç†æ‰€æœ‰è®¿é—®ï¼Œä¸åªæ˜¯ç¬¬ä¸€æ¬¡
        Key difference: process all visits, not just first
        
        æ•°å­¦æ›´æ–°ï¼š
        Mathematical update:
        V(s) = (1/N(s)) Î£áµ¢ Î£â‚œ Gáµ¢,â‚œ(s)
        å…¶ä¸­Gáµ¢,â‚œ(s)æ˜¯ç¬¬iä¸ªå›åˆä¸­såœ¨æ—¶é—´tå‡ºç°çš„å›æŠ¥
        where Gáµ¢,â‚œ(s) is return from occurrence of s at time t in episode i
        """
        # è®¡ç®—æ•´ä¸ªå›åˆçš„å›æŠ¥
        # Compute returns for entire episode
        returns = episode.compute_returns(self.gamma)
        
        # è®°å½•è¿™ä¸ªå›åˆçš„è®¿é—®æ¨¡å¼
        # Record visit pattern for this episode
        visit_pattern = []
        
        # éå†å›åˆä¸­çš„æ¯ä¸€æ­¥
        # Iterate through each step in episode
        for t, exp in enumerate(episode.experiences):
            state = exp.state
            visit_pattern.append(state.id)
            
            # Every-visit: å¤„ç†æ‰€æœ‰è®¿é—®
            # Every-visit: process all visits
            G = returns[t]
            
            # è®°å½•è¿™ä¸ªå›æŠ¥
            # Record this return
            self.all_returns[state.id].append(G)
            
            # æ›´æ–°ç»Ÿè®¡
            # Update statistics
            self.statistics.update_state_value(state, G)
            self.state_visits[state.id] += 1
            
            # å¢é‡æ›´æ–°ä»·å€¼ä¼°è®¡
            # Incremental value update
            n = self.state_visits[state.id]
            old_v = self.V.get_value(state)
            new_v = old_v + (G - old_v) / n
            self.V.set_value(state, new_v)
            
            # è¯¦ç»†æ—¥å¿—
            # Detailed logging
            if logger.level == logging.DEBUG:
                logger.debug(f"Every-visitæ›´æ–°: {state.id}")
                logger.debug(f"  ç¬¬{self.state_visits[state.id]}æ¬¡è®¿é—®")
                logger.debug(f"  å›æŠ¥G = {G:.3f}")
                logger.debug(f"  æ›´æ–°: {old_v:.3f} â†’ {new_v:.3f}")
        
        # è®°å½•è®¿é—®æ¨¡å¼
        # Record visit pattern
        self.visit_patterns.append(visit_pattern)
    
    def analyze_correlation(self):
        """
        åˆ†æevery-visitçš„æ ·æœ¬ç›¸å…³æ€§
        Analyze sample correlation in every-visit
        
        å±•ç¤ºä¸ºä»€ä¹ˆç†è®ºåˆ†ææ›´å¤æ‚
        Show why theoretical analysis is more complex
        """
        print("\n" + "="*60)
        print("Every-Visitæ ·æœ¬ç›¸å…³æ€§åˆ†æ")
        print("Every-Visit Sample Correlation Analysis")
        print("="*60)
        
        # æ‰¾å‡ºé‡å¤è®¿é—®çš„çŠ¶æ€
        # Find states with repeated visits
        repeated_states = {}
        for pattern in self.visit_patterns[-10:]:  # çœ‹æœ€è¿‘10ä¸ªå›åˆ
            state_counts = defaultdict(int)
            for state_id in pattern:
                state_counts[state_id] += 1
            
            for state_id, count in state_counts.items():
                if count > 1:
                    if state_id not in repeated_states:
                        repeated_states[state_id] = []
                    repeated_states[state_id].append(count)
        
        if repeated_states:
            print("\né‡å¤è®¿é—®çš„çŠ¶æ€:")
            print("States with Repeated Visits:")
            for state_id, counts in repeated_states.items():
                avg_repeats = np.mean(counts)
                max_repeats = max(counts)
                print(f"  {state_id}: å¹³å‡é‡å¤{avg_repeats:.1f}æ¬¡, æœ€å¤š{max_repeats}æ¬¡")
            
            print("\nç›¸å…³æ€§å½±å“:")
            print("Correlation Impact:")
            print("""
            1. åŒä¸€å›åˆå†…çš„å›æŠ¥ç›¸å…³
               Returns within same episode are correlated
               å› ä¸ºå…±äº«æœªæ¥è½¨è¿¹
               Because they share future trajectory
            
            2. æœ‰æ•ˆæ ·æœ¬æ•° < æ€»æ ·æœ¬æ•°
               Effective sample size < Total sample size
               n_eff = n / (1 + 2Î£Ï)
               å…¶ä¸­Ïæ˜¯è‡ªç›¸å…³ç³»æ•°
               where Ï is autocorrelation coefficient
            
            3. æ ‡å‡†è¯¯å·®ä¼°è®¡éœ€è¦è°ƒæ•´
               Standard error estimation needs adjustment
               ç®€å•çš„SE = Ïƒ/âˆšnä¼šä½ä¼°çœŸå®è¯¯å·®
               Simple SE = Ïƒ/âˆšn underestimates true error
            
            4. ä½†å®è·µä¸­ä»ç„¶æœ‰æ•ˆï¼
               But still effective in practice!
               æ›´å¤šæ•°æ®é€šå¸¸è¡¥å¿äº†ç›¸å…³æ€§
               More data usually compensates for correlation
            """)
        else:
            print("è¿‘æœŸå›åˆä¸­æ²¡æœ‰é‡å¤è®¿é—®")
            print("No repeated visits in recent episodes")
    
    def compare_with_first_visit(self, first_visit_mc: 'FirstVisitMC'):
        """
        ä¸First-Visitæ¯”è¾ƒ
        Compare with First-Visit
        
        å±•ç¤ºä¸¤ç§æ–¹æ³•çš„å·®å¼‚
        Show differences between two methods
        """
        print("\n" + "="*60)
        print("First-Visit vs Every-Visitæ¯”è¾ƒ")
        print("First-Visit vs Every-Visit Comparison")
        print("="*60)
        
        # æ¯”è¾ƒæ ·æœ¬æ•°
        # Compare sample counts
        print("\næ ·æœ¬æ•°æ¯”è¾ƒ:")
        print("Sample Count Comparison:")
        
        for state_id in self.state_visits.keys():
            every_count = self.state_visits[state_id]
            first_count = first_visit_mc.state_visits.get(state_id, 0)
            
            if first_count > 0:
                ratio = every_count / first_count
                print(f"  {state_id}: Every={every_count}, First={first_count}, "
                      f"æ¯”ç‡={ratio:.2f}")
        
        # æ¯”è¾ƒæ”¶æ•›é€Ÿåº¦ï¼ˆé€šè¿‡æ–¹å·®ï¼‰
        # Compare convergence speed (through variance)
        print("\nä¼°è®¡æ–¹å·®æ¯”è¾ƒ:")
        print("Estimation Variance Comparison:")
        
        for state_id in self.all_returns.keys():
            if state_id in first_visit_mc.first_returns:
                every_returns = self.all_returns[state_id]
                first_returns = first_visit_mc.first_returns[state_id]
                
                if len(every_returns) > 1 and len(first_returns) > 1:
                    every_var = np.var(every_returns)
                    first_var = np.var(first_returns)
                    
                    # æ³¨æ„ï¼šè¿™ä¸æ˜¯å®Œå…¨å…¬å¹³çš„æ¯”è¾ƒï¼ˆæ ·æœ¬æ•°ä¸åŒï¼‰
                    # Note: not entirely fair comparison (different sample sizes)
                    print(f"  {state_id}:")
                    print(f"    Every-visitæ–¹å·®: {every_var:.3f}")
                    print(f"    First-visitæ–¹å·®: {first_var:.3f}")
                    
                    # è°ƒæ•´åçš„æ¯”è¾ƒï¼ˆè€ƒè™‘æ ·æœ¬æ•°ï¼‰
                    # Adjusted comparison (considering sample size)
                    every_se = np.sqrt(every_var / len(every_returns))
                    first_se = np.sqrt(first_var / len(first_returns))
                    print(f"    Every-visit SE: {every_se:.3f}")
                    print(f"    First-visit SE: {first_se:.3f}")


# ================================================================================
# ç¬¬5.2.4èŠ‚ï¼šå¢é‡MCé¢„æµ‹
# Section 5.2.4: Incremental MC Prediction
# ================================================================================

class IncrementalMC(MCPrediction):
    """
    å¢é‡è’™ç‰¹å¡æ´›é¢„æµ‹
    Incremental Monte Carlo Prediction
    
    ä½¿ç”¨å¢é‡æ›´æ–°å…¬å¼ï¼Œä¸å­˜å‚¨æ‰€æœ‰å›æŠ¥
    Use incremental update formula, don't store all returns
    
    æ ¸å¿ƒæ€æƒ³ï¼šè¿è¡Œå¹³å‡
    Core idea: Running average
    V_{n+1} = V_n + (1/(n+1))[G_{n+1} - V_n]
    
    å¯ä»¥æ”¹å†™ä¸ºï¼š
    Can be rewritten as:
    V_{n+1} = V_n + Î±_n[G_{n+1} - V_n]
    å…¶ä¸­ Î±_n = 1/(n+1)
    where Î±_n = 1/(n+1)
    
    ä¼˜åŠ¿ï¼š
    Advantages:
    1. å†…å­˜é«˜æ•ˆï¼ˆO(|S|)è€ŒéO(|S|Ã—n)ï¼‰
       Memory efficient (O(|S|) not O(|S|Ã—n))
    2. è®¡ç®—é«˜æ•ˆï¼ˆO(1)æ›´æ–°ï¼‰
       Computationally efficient (O(1) update)
    3. é€‚åˆåœ¨çº¿å­¦ä¹ 
       Suitable for online learning
    
    å˜ä½“ï¼š
    Variants:
    1. é€’å‡æ­¥é•¿ï¼šÎ±_n = 1/n â†’ æ”¶æ•›åˆ°çœŸå®å€¼
       Decreasing step-size: Î±_n = 1/n â†’ converges to true value
    2. å¸¸æ•°æ­¥é•¿ï¼šÎ±_n = Î± â†’ è·Ÿè¸ªéå¹³ç¨³
       Constant step-size: Î±_n = Î± â†’ tracks non-stationarity
    
    è¿™æ˜¯TDæ–¹æ³•çš„å‰èº«ï¼
    This is the predecessor of TD methods!
    """
    
    def __init__(self, 
                 env: MDPEnvironment,
                 gamma: float = 1.0,
                 alpha: Optional[float] = None,
                 visit_type: str = 'first'):
        """
        åˆå§‹åŒ–å¢é‡MC
        Initialize Incremental MC
        
        Args:
            env: ç¯å¢ƒ
            gamma: æŠ˜æ‰£å› å­
            alpha: æ­¥é•¿ï¼ˆNoneè¡¨ç¤ºç”¨1/nï¼‰
                  Step-size (None means use 1/n)
            visit_type: 'first' æˆ– 'every'
        
        è®¾è®¡é€‰æ‹©ï¼š
        Design choices:
        - alpha=None: ä¿è¯æ”¶æ•›åˆ°çœŸå®å€¼
          alpha=None: Guarantees convergence to true value
        - alpha=å¸¸æ•°: é€‚åº”éå¹³ç¨³ç¯å¢ƒ
          alpha=constant: Adapts to non-stationary environment
        """
        super().__init__(env, gamma, visit_type)
        
        self.alpha = alpha  # å›ºå®šæ­¥é•¿ï¼ˆå¦‚æœæŒ‡å®šï¼‰
        self.use_constant_alpha = (alpha is not None)
        
        # ä¸éœ€è¦å­˜å‚¨æ‰€æœ‰å›æŠ¥ï¼
        # No need to store all returns!
        # è¿™å°±æ˜¯"å¢é‡"çš„å«ä¹‰
        # This is what "incremental" means
        
        logger.info(f"åˆå§‹åŒ–å¢é‡MC: Î±={'constant='+str(alpha) if alpha else '1/n'}")
    
    def update_value(self, episode: Episode):
        """
        å¢é‡æ›´æ–°ä»·å€¼
        Incremental value update
        
        å…³é”®ï¼šä¸å­˜å‚¨å†å²ï¼Œç›´æ¥æ›´æ–°è¿è¡Œå¹³å‡
        Key: Don't store history, directly update running average
        
        æ›´æ–°è§„åˆ™ï¼š
        Update rule:
        V(s) â† V(s) + Î±[G - V(s)]
        
        å…¶ä¸­Î±æ˜¯å­¦ä¹ ç‡ï¼š
        where Î± is learning rate:
        - 1/n(s): ç²¾ç¡®å¹³å‡
          1/n(s): Exact average
        - å¸¸æ•°: æŒ‡æ•°åŠ æƒå¹³å‡
          constant: Exponentially weighted average
        """
        # è®¡ç®—å›æŠ¥
        returns = episode.compute_returns(self.gamma)
        
        if self.visit_type == 'first':
            # First-visitå¢é‡æ›´æ–°
            visited_states = set()
            
            for t, exp in enumerate(episode.experiences):
                state = exp.state
                
                if state.id not in visited_states:
                    visited_states.add(state.id)
                    G = returns[t]
                    
                    # æ›´æ–°è®¿é—®è®¡æ•°
                    self.state_visits[state.id] += 1
                    n = self.state_visits[state.id]
                    
                    # ç¡®å®šæ­¥é•¿
                    if self.use_constant_alpha:
                        alpha = self.alpha
                    else:
                        alpha = 1.0 / n
                    
                    # å¢é‡æ›´æ–°
                    old_v = self.V.get_value(state)
                    new_v = old_v + alpha * (G - old_v)
                    self.V.set_value(state, new_v)
                    
                    # æ›´æ–°ç»Ÿè®¡ï¼ˆç”¨äºåˆ†æï¼‰
                    self.statistics.update_state_value(state, G)
        
        else:  # every-visit
            # Every-visitå¢é‡æ›´æ–°
            for t, exp in enumerate(episode.experiences):
                state = exp.state
                G = returns[t]
                
                # æ›´æ–°è®¿é—®è®¡æ•°
                self.state_visits[state.id] += 1
                n = self.state_visits[state.id]
                
                # ç¡®å®šæ­¥é•¿
                if self.use_constant_alpha:
                    alpha = self.alpha
                else:
                    alpha = 1.0 / n
                
                # å¢é‡æ›´æ–°
                old_v = self.V.get_value(state)
                new_v = old_v + alpha * (G - old_v)
                self.V.set_value(state, new_v)
                
                # æ›´æ–°ç»Ÿè®¡
                self.statistics.update_state_value(state, G)
    
    def demonstrate_step_size_effect(self, policy: Policy, n_episodes: int = 1000):
        """
        æ¼”ç¤ºæ­¥é•¿çš„å½±å“
        Demonstrate effect of step-size
        
        æ¯”è¾ƒé€’å‡æ­¥é•¿vså¸¸æ•°æ­¥é•¿
        Compare decreasing vs constant step-size
        """
        print("\n" + "="*60)
        print("æ­¥é•¿å½±å“æ¼”ç¤º")
        print("Step-size Effect Demonstration")
        print("="*60)
        
        # åˆ›å»ºä¸¤ä¸ªç‰ˆæœ¬
        # Create two versions
        mc_decreasing = IncrementalMC(self.env, self.gamma, alpha=None, visit_type=self.visit_type)
        mc_constant = IncrementalMC(self.env, self.gamma, alpha=0.1, visit_type=self.visit_type)
        
        # è®°å½•å­¦ä¹ æ›²çº¿
        # Record learning curves
        decreasing_curve = []
        constant_curve = []
        
        for ep in range(n_episodes):
            # ç”Ÿæˆç›¸åŒçš„å›åˆ
            episode = self.generate_episode(policy)
            
            # ä¸¤ç§æ–¹æ³•éƒ½æ›´æ–°
            mc_decreasing.update_value(episode)
            mc_constant.update_value(episode)
            
            # è®°å½•ä»·å€¼ï¼ˆé€‰æ‹©ä¸€ä¸ªä»£è¡¨æ€§çŠ¶æ€ï¼‰
            if self.env.state_space:
                sample_state = self.env.state_space[0]
                decreasing_curve.append(mc_decreasing.V.get_value(sample_state))
                constant_curve.append(mc_constant.V.get_value(sample_state))
        
        # å¯è§†åŒ–
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
        
        # å·¦å›¾ï¼šå­¦ä¹ æ›²çº¿
        ax1.plot(decreasing_curve, 'b-', alpha=0.7, label='Î±=1/n (decreasing)')
        ax1.plot(constant_curve, 'r-', alpha=0.7, label='Î±=0.1 (constant)')
        ax1.set_xlabel('Episodes')
        ax1.set_ylabel('Value Estimate')
        ax1.set_title('Learning Curves')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # å³å›¾ï¼šæ­¥é•¿å˜åŒ–
        n_values = np.arange(1, 101)
        decreasing_alphas = 1.0 / n_values
        constant_alphas = np.ones_like(n_values) * 0.1
        
        ax2.plot(n_values, decreasing_alphas, 'b-', label='Î±=1/n')
        ax2.plot(n_values, constant_alphas, 'r-', label='Î±=0.1')
        ax2.set_xlabel('Visit Count n')
        ax2.set_ylabel('Step-size Î±')
        ax2.set_title('Step-size Schedules')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        ax2.set_ylim([0, 0.5])
        
        plt.suptitle('Incremental MC: Step-size Comparison', fontsize=14, fontweight='bold')
        plt.tight_layout()
        
        print("\nå…³é”®è§‚å¯Ÿ:")
        print("Key Observations:")
        print("""
        1. é€’å‡æ­¥é•¿ (Î±=1/n):
           Decreasing step-size (Î±=1/n):
           - ä¿è¯æ”¶æ•›åˆ°çœŸå®å€¼
             Guarantees convergence to true value
           - åæœŸå­¦ä¹ å˜æ…¢
             Learning slows down later
           - é€‚åˆå¹³ç¨³ç¯å¢ƒ
             Suitable for stationary environments
        
        2. å¸¸æ•°æ­¥é•¿ (Î±=constant):
           Constant step-size (Î±=constant):
           - æŒç»­å­¦ä¹ å’Œé€‚åº”
             Continues to learn and adapt
           - å¯èƒ½åœ¨çœŸå®å€¼é™„è¿‘éœ‡è¡
             May oscillate around true value
           - é€‚åˆéå¹³ç¨³ç¯å¢ƒ
             Suitable for non-stationary environments
        
        3. æƒè¡¡ï¼š
           Trade-off:
           - æ”¶æ•›ç²¾åº¦ vs é€‚åº”èƒ½åŠ›
             Convergence accuracy vs Adaptability
           - ç†è®ºä¿è¯ vs å®è·µæ€§èƒ½
             Theoretical guarantee vs Practical performance
        """)
        
        return fig
    
    def explain_incremental_formula(self):
        """
        è§£é‡Šå¢é‡å…¬å¼
        Explain incremental formula
        
        å±•ç¤ºä¸ºä»€ä¹ˆå¢é‡æ›´æ–°ç­‰ä»·äºæ‰¹é‡å¹³å‡
        Show why incremental update equals batch average
        """
        print("\n" + "="*60)
        print("å¢é‡å…¬å¼æ¨å¯¼")
        print("Incremental Formula Derivation")
        print("="*60)
        
        print("""
        ğŸ“ æ•°å­¦æ¨å¯¼ Mathematical Derivation
        =====================================
        
        ç›®æ ‡ï¼šè®¡ç®—nä¸ªå›æŠ¥çš„å¹³å‡å€¼
        Goal: Compute average of n returns
        
        æ‰¹é‡æ–¹æ³• Batch method:
        V_n = (1/n) Î£áµ¢â‚Œâ‚â¿ Gáµ¢
        
        å¢é‡æ–¹æ³• Incremental method:
        V_n = V_{n-1} + (1/n)[G_n - V_{n-1}]
        
        è¯æ˜ç­‰ä»·æ€§ Prove equivalence:
        ----------------------------------------
        V_n = (1/n) Î£áµ¢â‚Œâ‚â¿ Gáµ¢
            = (1/n)[G_n + Î£áµ¢â‚Œâ‚â¿â»Â¹ Gáµ¢]
            = (1/n)[G_n + (n-1)V_{n-1}]
            = (1/n)G_n + ((n-1)/n)V_{n-1}
            = V_{n-1} + (1/n)[G_n - V_{n-1}]  âœ“
        
        ğŸ’¡ å…³é”®æ´å¯Ÿ Key Insights
        ========================
        
        1. è¯¯å·®é¡¹ Error term:
           [G_n - V_{n-1}] 
           = æ–°æ ·æœ¬ä¸å½“å‰ä¼°è®¡çš„å·®
           = Difference between new sample and current estimate
           = "æƒŠå–œ"æˆ–"é¢„æµ‹è¯¯å·®"
           = "Surprise" or "Prediction error"
        
        2. å­¦ä¹ ç‡ Learning rate:
           Î± = 1/n
           = éšç€æ ·æœ¬å¢åŠ è€Œå‡å°
           = Decreases as samples increase
           = æ–°æ ·æœ¬çš„å½±å“é€æ¸é™ä½
           = New samples have decreasing influence
        
        3. æ›´æ–°æ–¹å‘ Update direction:
           å¦‚æœ G_n > V_{n-1}: å‘ä¸Šè°ƒæ•´
           If G_n > V_{n-1}: Adjust upward
           å¦‚æœ G_n < V_{n-1}: å‘ä¸‹è°ƒæ•´
           If G_n < V_{n-1}: Adjust downward
        
        4. è¿™æ˜¯RLä¸­é€šç”¨çš„æ›´æ–°æ¨¡å¼ï¼
           This is the universal update pattern in RL!
           æ–°ä¼°è®¡ = æ—§ä¼°è®¡ + æ­¥é•¿ Ã— [ç›®æ ‡ - æ—§ä¼°è®¡]
           New estimate = Old estimate + StepSize Ã— [Target - Old estimate]
        
        ğŸ”„ å¸¸æ•°æ­¥é•¿çš„å«ä¹‰
        Meaning of Constant Step-size
        ==============================
        
        å½“ Î± = å¸¸æ•°ï¼ˆå¦‚0.1ï¼‰:
        When Î± = constant (e.g., 0.1):
        
        V_n = V_{n-1} + Î±[G_n - V_{n-1}]
            = (1-Î±)V_{n-1} + Î±G_n
            = Î± G_n + (1-Î±)Î± G_{n-1} + (1-Î±)Â²Î± G_{n-2} + ...
            = Î± Î£áµ¢â‚Œâ‚â¿ (1-Î±)^{n-i} Gáµ¢
        
        è¿™æ˜¯æŒ‡æ•°åŠ æƒç§»åŠ¨å¹³å‡ï¼
        This is exponentially weighted moving average!
        
        - æœ€è¿‘çš„æ ·æœ¬æƒé‡æœ€å¤§
          Recent samples have highest weight
        - æ—§æ ·æœ¬æƒé‡æŒ‡æ•°è¡°å‡
          Old samples decay exponentially
        - æ°¸è¿œä¸ä¼šå®Œå…¨"å¿˜è®°"
          Never completely "forgets"
        """)


# ================================================================================
# ç¬¬5.2.5èŠ‚ï¼šMCé¢„æµ‹å¯è§†åŒ–å™¨
# Section 5.2.5: MC Prediction Visualizer
# ================================================================================

class MCPredictionVisualizer:
    """
    MCé¢„æµ‹å¯è§†åŒ–å™¨
    MC Prediction Visualizer
    
    æä¾›ä¸°å¯Œçš„å¯è§†åŒ–æ¥ç†è§£MCé¢„æµ‹
    Provides rich visualizations to understand MC prediction
    
    å¯è§†åŒ–å†…å®¹ï¼š
    Visualization contents:
    1. æ”¶æ•›æ›²çº¿
       Convergence curves
    2. ç½®ä¿¡åŒºé—´
       Confidence intervals
    3. First-visit vs Every-visitæ¯”è¾ƒ
       First-visit vs Every-visit comparison
    4. è®¿é—®é¢‘ç‡çƒ­åŠ›å›¾
       Visit frequency heatmap
    5. å›æŠ¥åˆ†å¸ƒ
       Return distributions
    """
    
    @staticmethod
    def plot_convergence_comparison(mc_methods: Dict[str, MCPrediction],
                                   true_values: Optional[Dict[str, float]] = None):
        """
        æ¯”è¾ƒä¸åŒMCæ–¹æ³•çš„æ”¶æ•›
        Compare convergence of different MC methods
        """
        fig, axes = plt.subplots(2, 2, figsize=(14, 10))
        
        # å‡†å¤‡é¢œè‰²
        colors = ['blue', 'red', 'green', 'orange', 'purple']
        
        # å›¾1ï¼šä»·å€¼ä¼°è®¡æ¼”åŒ–
        ax1 = axes[0, 0]
        ax1.set_title('Value Estimate Evolution')
        ax1.set_xlabel('Episodes')
        ax1.set_ylabel('Value Estimate')
        
        for idx, (name, mc) in enumerate(mc_methods.items()):
            if mc.convergence_history:
                ax1.plot(mc.convergence_history, 
                        color=colors[idx % len(colors)],
                        label=name, alpha=0.7)
        
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # å›¾2ï¼šæœ€ç»ˆä¼°è®¡æ¯”è¾ƒ
        ax2 = axes[0, 1]
        ax2.set_title('Final Value Estimates')
        
        # é€‰æ‹©ä¸€äº›çŠ¶æ€æ¥æ¯”è¾ƒ
        sample_states = []
        for mc in mc_methods.values():
            for state in mc.env.state_space[:5]:  # æœ€å¤š5ä¸ªçŠ¶æ€
                if state not in sample_states:
                    sample_states.append(state)
        
        x = np.arange(len(sample_states))
        width = 0.8 / len(mc_methods)
        
        for idx, (name, mc) in enumerate(mc_methods.items()):
            values = [mc.V.get_value(s) for s in sample_states]
            offset = (idx - len(mc_methods)/2) * width
            ax2.bar(x + offset, values, width, 
                   label=name, alpha=0.7,
                   color=colors[idx % len(colors)])
        
        if true_values:
            true_vals = [true_values.get(s.id, 0) for s in sample_states]
            ax2.plot(x, true_vals, 'k*', markersize=10, label='True Values')
        
        ax2.set_xticks(x)
        ax2.set_xticklabels([s.id for s in sample_states], rotation=45)
        ax2.legend()
        ax2.grid(True, alpha=0.3, axis='y')
        
        # å›¾3ï¼šè®¿é—®é¢‘ç‡æ¯”è¾ƒ
        ax3 = axes[1, 0]
        ax3.set_title('State Visit Frequencies')
        
        for idx, (name, mc) in enumerate(mc_methods.items()):
            visits = list(mc.state_visits.values())
            if visits:
                ax3.hist(visits, bins=20, alpha=0.5, 
                        label=name, color=colors[idx % len(colors)])
        
        ax3.set_xlabel('Visit Count')
        ax3.set_ylabel('Number of States')
        ax3.legend()
        ax3.grid(True, alpha=0.3)
        
        # å›¾4ï¼šæ”¶æ•›é€Ÿåº¦ï¼ˆæ ‡å‡†å·®ï¼‰
        ax4 = axes[1, 1]
        ax4.set_title('Convergence Speed (Std of Returns)')
        
        for idx, (name, mc) in enumerate(mc_methods.items()):
            # è®¡ç®—æ¯ä¸ªçŠ¶æ€å›æŠ¥çš„æ ‡å‡†å·®
            stds = []
            for state_id in mc.statistics.state_returns:
                returns_obj = mc.statistics.state_returns[state_id]
                if returns_obj.count > 1:
                    stds.append(returns_obj.std)
            
            if stds:
                ax4.boxplot(stds, positions=[idx], labels=[name],
                           patch_artist=True, 
                           boxprops=dict(facecolor=colors[idx % len(colors)], alpha=0.5))
        
        ax4.set_ylabel('Standard Deviation')
        ax4.grid(True, alpha=0.3, axis='y')
        
        plt.suptitle('MC Prediction Methods Comparison', fontsize=14, fontweight='bold')
        plt.tight_layout()
        
        return fig
    
    @staticmethod
    def plot_return_distributions(mc: MCPrediction, 
                                 states_to_plot: Optional[List[State]] = None):
        """
        ç»˜åˆ¶å›æŠ¥åˆ†å¸ƒ
        Plot return distributions
        
        å±•ç¤ºMCä¼°è®¡çš„åˆ†å¸ƒç‰¹æ€§
        Show distributional properties of MC estimates
        """
        if states_to_plot is None:
            # é€‰æ‹©è®¿é—®æœ€é¢‘ç¹çš„çŠ¶æ€
            # Select most frequently visited states
            sorted_states = sorted(mc.state_visits.items(), 
                                 key=lambda x: x[1], reverse=True)
            states_to_plot = [mc.env.get_state_by_id(s_id) 
                            for s_id, _ in sorted_states[:4]]
        
        n_states = len(states_to_plot)
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        axes = axes.flatten()
        
        for idx, state in enumerate(states_to_plot):
            if idx >= 4:
                break
            
            ax = axes[idx]
            
            # è·å–è¯¥çŠ¶æ€çš„æ‰€æœ‰å›æŠ¥
            # Get all returns for this state
            returns_obj = mc.statistics.state_returns.get(state.id)
            
            if returns_obj and returns_obj.returns:
                returns = returns_obj.returns
                
                # ç›´æ–¹å›¾
                # Histogram
                n, bins, patches = ax.hist(returns, bins=20, density=True,
                                          alpha=0.7, color='steelblue',
                                          edgecolor='black')
                
                # æ‹Ÿåˆæ­£æ€åˆ†å¸ƒ
                # Fit normal distribution
                mu, sigma = np.mean(returns), np.std(returns)
                x = np.linspace(min(returns), max(returns), 100)
                normal_pdf = stats.norm.pdf(x, mu, sigma)
                ax.plot(x, normal_pdf, 'r-', linewidth=2, 
                       label=f'N({mu:.2f}, {sigma:.2f}Â²)')
                
                # æ·»åŠ å‡å€¼çº¿
                # Add mean line
                ax.axvline(x=mu, color='red', linestyle='--', 
                          linewidth=2, alpha=0.7, label=f'Mean={mu:.2f}')
                
                # æ·»åŠ ç½®ä¿¡åŒºé—´
                # Add confidence interval
                ci = returns_obj.confidence_interval(0.95)
                ax.axvspan(ci[0], ci[1], alpha=0.2, color='yellow',
                          label=f'95% CI: [{ci[0]:.2f}, {ci[1]:.2f}]')
                
                ax.set_xlabel('Return G')
                ax.set_ylabel('Density')
                ax.set_title(f'State: {state.id} (n={len(returns)})')
                ax.legend(fontsize=8)
                ax.grid(True, alpha=0.3)
            else:
                ax.text(0.5, 0.5, f'No data for {state.id}',
                       ha='center', va='center', transform=ax.transAxes)
                ax.set_title(f'State: {state.id}')
        
        plt.suptitle('Return Distributions by State', fontsize=14, fontweight='bold')
        plt.tight_layout()
        
        return fig
    
    @staticmethod
    def plot_learning_curves(mc_methods: Dict[str, MCPrediction],
                           metric: str = 'mean_squared_error',
                           true_values: Optional[Dict[str, float]] = None):
        """
        ç»˜åˆ¶å­¦ä¹ æ›²çº¿
        Plot learning curves
        
        å±•ç¤ºä¸åŒæŒ‡æ ‡éšæ—¶é—´çš„å˜åŒ–
        Show how different metrics change over time
        """
        if not true_values:
            print("è­¦å‘Šï¼šæ²¡æœ‰çœŸå®å€¼ï¼Œä½¿ç”¨ä¼°è®¡å€¼çš„å˜åŒ–ä½œä¸ºä»£ç†")
            print("Warning: No true values, using estimate changes as proxy")
        
        fig, ax = plt.subplots(figsize=(12, 6))
        
        colors = ['blue', 'red', 'green', 'orange', 'purple']
        
        for idx, (name, mc) in enumerate(mc_methods.items()):
            if metric == 'mean_squared_error' and true_values:
                # è®¡ç®—MSE
                # Compute MSE
                mse_history = []
                for ep_idx in range(0, len(mc.episodes), 10):
                    mse = 0
                    count = 0
                    for state in mc.env.state_space:
                        if state.id in true_values:
                            estimate = mc.V.get_value(state)
                            true_val = true_values[state.id]
                            mse += (estimate - true_val) ** 2
                            count += 1
                    if count > 0:
                        mse_history.append(mse / count)
                
                if mse_history:
                    x = np.arange(0, len(mc.episodes), 10)
                    ax.plot(x, mse_history, color=colors[idx % len(colors)],
                           label=name, linewidth=2, alpha=0.7)
            
            elif metric == 'max_change':
                # ä½¿ç”¨æ”¶æ•›å†å²
                # Use convergence history
                if mc.convergence_history:
                    x = np.arange(len(mc.convergence_history)) * 10
                    ax.plot(x, mc.convergence_history,
                           color=colors[idx % len(colors)],
                           label=name, linewidth=2, alpha=0.7)
        
        ax.set_xlabel('Episodes')
        ax.set_ylabel(metric.replace('_', ' ').title())
        ax.set_title(f'Learning Curves: {metric}')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        # å¯¹æ•°å°ºåº¦å¯èƒ½æ›´æœ‰ç”¨
        # Log scale might be more useful
        ax.set_yscale('log')
        
        plt.tight_layout()
        return fig


# ================================================================================
# ç¬¬5.2.6èŠ‚ï¼šMCé¢„æµ‹ç»¼åˆæ¼”ç¤º
# Section 5.2.6: MC Prediction Comprehensive Demo
# ================================================================================

def demonstrate_mc_prediction():
    """
    ç»¼åˆæ¼”ç¤ºMCé¢„æµ‹æ–¹æ³•
    Comprehensive demonstration of MC prediction methods
    
    å±•ç¤ºæ‰€æœ‰MCé¢„æµ‹ç®—æ³•çš„ç‰¹ç‚¹å’Œæ¯”è¾ƒ
    Show characteristics and comparison of all MC prediction algorithms
    """
    print("\n" + "="*80)
    print("è’™ç‰¹å¡æ´›é¢„æµ‹æ–¹æ³•ç»¼åˆæ¼”ç¤º")
    print("Monte Carlo Prediction Methods Comprehensive Demo")
    print("="*80)
    
    # åˆ›å»ºä¸€ä¸ªç®€å•çš„ç½‘æ ¼ä¸–ç•Œç”¨äºæµ‹è¯•
    # Create a simple grid world for testing
    from src.ch03_finite_mdp.gridworld import GridWorld
    from src.ch03_finite_mdp.policies_and_values import UniformRandomPolicy
    
    env = GridWorld(rows=4, cols=4, 
                   start_pos=(0,0), 
                   goal_pos=(3,3),
                   obstacles={(1,1), (2,2)})
    
    print(f"\næµ‹è¯•ç¯å¢ƒ: {env.name}")
    print(f"  çŠ¶æ€æ•°: {len(env.state_space)}")
    print(f"  åŠ¨ä½œæ•°: {len(env.action_space)}")
    
    # åˆ›å»ºè¦è¯„ä¼°çš„ç­–ç•¥ï¼ˆéšæœºç­–ç•¥ï¼‰
    # Create policy to evaluate (random policy)
    policy = UniformRandomPolicy(env.action_space)
    print(f"\nè¯„ä¼°ç­–ç•¥: å‡åŒ€éšæœºç­–ç•¥")
    
    # è¿è¡Œä¸åŒçš„MCæ–¹æ³•
    # Run different MC methods
    n_episodes = 500
    print(f"\nè¿è¡Œ{n_episodes}ä¸ªå›åˆ...")
    
    # 1. First-Visit MC
    print("\n1. First-Visit MC")
    first_visit_mc = FirstVisitMC(env, gamma=0.9)
    V_first = first_visit_mc.estimate_V(policy, n_episodes, verbose=True)
    
    # 2. Every-Visit MC
    print("\n2. Every-Visit MC")
    every_visit_mc = EveryVisitMC(env, gamma=0.9)
    V_every = every_visit_mc.estimate_V(policy, n_episodes, verbose=True)
    
    # 3. Incremental MC (é€’å‡æ­¥é•¿)
    print("\n3. Incremental MC (Î±=1/n)")
    incremental_mc = IncrementalMC(env, gamma=0.9, alpha=None, visit_type='first')
    V_inc = incremental_mc.estimate_V(policy, n_episodes, verbose=True)
    
    # 4. Incremental MC (å¸¸æ•°æ­¥é•¿)
    print("\n4. Incremental MC (Î±=0.1)")
    constant_mc = IncrementalMC(env, gamma=0.9, alpha=0.1, visit_type='first')
    V_const = constant_mc.estimate_V(policy, n_episodes, verbose=True)
    
    # åˆ†æå’Œæ¯”è¾ƒ
    # Analysis and comparison
    print("\n" + "="*60)
    print("æ–¹æ³•æ¯”è¾ƒ")
    print("Method Comparison")
    print("="*60)
    
    # æ¯”è¾ƒç‰¹å®šçŠ¶æ€çš„ä¼°è®¡
    # Compare estimates for specific states
    sample_states = [env.state_space[0], env.state_space[-1]]  # èµ·ç‚¹å’Œç»ˆç‚¹
    
    print("\nä»·å€¼ä¼°è®¡æ¯”è¾ƒ:")
    print("Value Estimate Comparison:")
    print(f"{'State':<15} {'First-Visit':<12} {'Every-Visit':<12} "
          f"{'Incremental':<12} {'Constant-Î±':<12}")
    print("-" * 63)
    
    for state in sample_states:
        if not state.is_terminal:
            v_first = first_visit_mc.V.get_value(state)
            v_every = every_visit_mc.V.get_value(state)
            v_inc = incremental_mc.V.get_value(state)
            v_const = constant_mc.V.get_value(state)
            
            print(f"{state.id:<15} {v_first:<12.3f} {v_every:<12.3f} "
                  f"{v_inc:<12.3f} {v_const:<12.3f}")
    
    # First-visitåˆ†æ
    print("\n" + "="*60)
    first_visit_mc.analyze_convergence()
    
    # Every-visitç›¸å…³æ€§åˆ†æ
    print("\n" + "="*60)
    every_visit_mc.analyze_correlation()
    
    # æ¯”è¾ƒFirstå’ŒEvery
    print("\n" + "="*60)
    every_visit_mc.compare_with_first_visit(first_visit_mc)
    
    # æ­¥é•¿å½±å“æ¼”ç¤º
    print("\n" + "="*60)
    fig_stepsize = incremental_mc.demonstrate_step_size_effect(policy, 200)
    
    # å¢é‡å…¬å¼è§£é‡Š
    incremental_mc.explain_incremental_formula()
    
    # å¯è§†åŒ–æ¯”è¾ƒ
    print("\nç”Ÿæˆå¯è§†åŒ–...")
    
    mc_methods = {
        'First-Visit': first_visit_mc,
        'Every-Visit': every_visit_mc,
        'Incremental(1/n)': incremental_mc,
        'Incremental(Î±=0.1)': constant_mc
    }
    
    # æ”¶æ•›æ¯”è¾ƒ
    fig_conv = MCPredictionVisualizer.plot_convergence_comparison(mc_methods)
    
    # å›æŠ¥åˆ†å¸ƒ
    fig_dist = MCPredictionVisualizer.plot_return_distributions(first_visit_mc)
    
    # å­¦ä¹ æ›²çº¿
    fig_learn = MCPredictionVisualizer.plot_learning_curves(
        mc_methods, metric='max_change')
    
    print("\n" + "="*80)
    print("MCé¢„æµ‹æ¼”ç¤ºå®Œæˆï¼")
    print("MC Prediction Demo Complete!")
    print("\nå…³é”®è¦ç‚¹ Key Takeaways:")
    print("1. First-Visit: ç†è®ºæ€§è´¨å¥½ï¼Œæ ·æœ¬ç‹¬ç«‹")
    print("   First-Visit: Good theoretical properties, independent samples")
    print("2. Every-Visit: æ•°æ®æ•ˆç‡é«˜ï¼Œæ”¶æ•›å¯èƒ½æ›´å¿«")
    print("   Every-Visit: Data efficient, may converge faster")
    print("3. å¢é‡æ›´æ–°: å†…å­˜é«˜æ•ˆï¼Œé€‚åˆåœ¨çº¿å­¦ä¹ ")
    print("   Incremental: Memory efficient, suitable for online learning")
    print("4. å¸¸æ•°æ­¥é•¿: é€‚åº”éå¹³ç¨³ï¼Œä½†å¯èƒ½ä¸æ”¶æ•›åˆ°ç²¾ç¡®å€¼")
    print("   Constant step-size: Adapts to non-stationarity, but may not converge exactly")
    print("5. MCæ˜¯æ— æ¨¡å‹å­¦ä¹ çš„åŸºç¡€ï¼Œä¸ºTDæ–¹æ³•é“ºè·¯")
    print("   MC is foundation of model-free learning, paving way for TD methods")
    print("="*80)
    
    plt.show()


# ================================================================================
# ä¸»å‡½æ•°
# Main Function
# ================================================================================

def main():
    """
    è¿è¡ŒMCé¢„æµ‹æ¼”ç¤º
    Run MC Prediction Demo
    """
    # å®Œæ•´æ¼”ç¤º
    demonstrate_mc_prediction()


if __name__ == "__main__":
    main()