#!/usr/bin/env python
"""
æµ‹è¯•ç¬¬5ç« æ‰€æœ‰è’™ç‰¹å¡æ´›æ¨¡å—
Test all Chapter 4 Monte Carlo modules

ç¡®ä¿æ‰€æœ‰MCç®—æ³•å®ç°æ­£ç¡®
Ensure all MC algorithm implementations are correct
"""

import sys
import traceback
import numpy as np
from pathlib import Path
import time

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))


def test_mc_foundations():
    """
    æµ‹è¯•MCåŸºç¡€ç†è®º
    Test MC Foundations
    """
    print("\n" + "="*60)
    print("æµ‹è¯•MCåŸºç¡€ç†è®º...")
    print("Testing MC Foundations...")
    print("="*60)
    
    try:
        from src.ch05_monte_carlo.mc_foundations import (
            Episode, Experience, Return, MCStatistics,
            LawOfLargeNumbers, MCFoundations
        )
        from src.ch03_finite_mdp.mdp_framework import State, Action
        
        # æµ‹è¯•Episodeç±»
        print("æµ‹è¯•Episodeç±»...")
        episode = Episode()
        
        # åˆ›å»ºä¸€äº›çŠ¶æ€å’ŒåŠ¨ä½œ
        states = [State(f"s{i}", {'value': i}) for i in range(3)]
        actions = [Action(f"a{i}", f"Action {i}") for i in range(2)]
        
        # æ·»åŠ ç»éªŒ
        episode.add_experience(Experience(states[0], actions[0], 1.0, states[1], False))
        episode.add_experience(Experience(states[1], actions[1], 2.0, states[2], True))
        
        assert episode.length() == 2, "Episodeé•¿åº¦é”™è¯¯"
        assert episode.is_complete(), "Episodeåº”è¯¥å®Œæˆ"
        print("âœ“ Episodeç±»æµ‹è¯•é€šè¿‡")
        
        # æµ‹è¯•å›æŠ¥è®¡ç®—
        print("æµ‹è¯•å›æŠ¥è®¡ç®—...")
        returns = episode.compute_returns(gamma=0.9)
        expected_g0 = 1.0 + 0.9 * 2.0  # G_0 = R_1 + Î³R_2
        expected_g1 = 2.0  # G_1 = R_2
        
        assert abs(returns[0] - expected_g0) < 0.001, f"G_0è®¡ç®—é”™è¯¯: {returns[0]} vs {expected_g0}"
        assert abs(returns[1] - expected_g1) < 0.001, f"G_1è®¡ç®—é”™è¯¯: {returns[1]} vs {expected_g1}"
        print("âœ“ å›æŠ¥è®¡ç®—æµ‹è¯•é€šè¿‡")
        
        # æµ‹è¯•Returnç»Ÿè®¡ç±»
        print("æµ‹è¯•Returnç»Ÿè®¡...")
        ret = Return()
        test_returns = [5.0, 4.0, 6.0, 5.5, 4.5]
        for g in test_returns:
            ret.add_return(g)
        
        assert abs(ret.mean - np.mean(test_returns)) < 0.001, "å‡å€¼è®¡ç®—é”™è¯¯"
        assert ret.count == len(test_returns), "è®¡æ•°é”™è¯¯"
        print("âœ“ Returnç»Ÿè®¡æµ‹è¯•é€šè¿‡")
        
        # æµ‹è¯•MCStatistics
        print("æµ‹è¯•MCStatistics...")
        stats = MCStatistics()
        for i in range(10):
            stats.update_state_value(states[0], np.random.normal(5.0, 1.0))
        
        estimate = stats.get_state_value_estimate(states[0])
        assert 3.0 < estimate < 7.0, f"ä¼°è®¡å€¼ä¸åˆç†: {estimate}"
        print("âœ“ MCStatisticsæµ‹è¯•é€šè¿‡")
        
        print("\nâœ… MCåŸºç¡€ç†è®ºæµ‹è¯•å…¨éƒ¨é€šè¿‡ï¼")
        return True
        
    except Exception as e:
        print(f"\nâŒ MCåŸºç¡€ç†è®ºæµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return False


def test_mc_prediction():
    """
    æµ‹è¯•MCé¢„æµ‹
    Test MC Prediction
    """
    print("\n" + "="*60)
    print("æµ‹è¯•MCé¢„æµ‹...")
    print("Testing MC Prediction...")
    print("="*60)
    
    try:
        from src.ch05_monte_carlo.mc_prediction import (
            FirstVisitMC, EveryVisitMC, IncrementalMC
        )
        from src.ch03_finite_mdp.gridworld import GridWorld
        from src.ch03_finite_mdp.policies_and_values import UniformRandomPolicy
        
        # åˆ›å»ºç®€å•ç¯å¢ƒ
        env = GridWorld(rows=2, cols=2, start_pos=(0,0), goal_pos=(1,1))
        print(f"âœ“ åˆ›å»º2Ã—2ç½‘æ ¼ä¸–ç•Œ")
        
        # åˆ›å»ºéšæœºç­–ç•¥
        policy = UniformRandomPolicy(env.action_space)
        print(f"âœ“ åˆ›å»ºéšæœºç­–ç•¥")
        
        # æµ‹è¯•First-Visit MC
        print("\næµ‹è¯•First-Visit MC...")
        first_visit = FirstVisitMC(env, gamma=0.9)
        V_first = first_visit.estimate_V(policy, n_episodes=100, verbose=False)
        
        # æ£€æŸ¥ä»·å€¼å‡½æ•°åˆç†æ€§
        for state in env.state_space:
            if not state.is_terminal:
                value = V_first.get_value(state)
                assert -100 < value < 100, f"First-visitä»·å€¼å¼‚å¸¸: {value}"
        print("âœ“ First-Visit MCæµ‹è¯•é€šè¿‡")
        
        # æµ‹è¯•Every-Visit MC
        print("æµ‹è¯•Every-Visit MC...")
        every_visit = EveryVisitMC(env, gamma=0.9)
        V_every = every_visit.estimate_V(policy, n_episodes=100, verbose=False)
        
        for state in env.state_space:
            if not state.is_terminal:
                value = V_every.get_value(state)
                assert -100 < value < 100, f"Every-visitä»·å€¼å¼‚å¸¸: {value}"
        print("âœ“ Every-Visit MCæµ‹è¯•é€šè¿‡")
        
        # æµ‹è¯•å¢é‡MC
        print("æµ‹è¯•å¢é‡MC...")
        incremental = IncrementalMC(env, gamma=0.9, alpha=0.1)
        V_inc = incremental.estimate_V(policy, n_episodes=100, verbose=False)
        
        for state in env.state_space:
            if not state.is_terminal:
                value = V_inc.get_value(state)
                assert -100 < value < 100, f"å¢é‡MCä»·å€¼å¼‚å¸¸: {value}"
        print("âœ“ å¢é‡MCæµ‹è¯•é€šè¿‡")
        
        # æ¯”è¾ƒä¸åŒæ–¹æ³•
        print("\næ¯”è¾ƒä¸åŒMCé¢„æµ‹æ–¹æ³•...")
        sample_state = env.state_space[0]
        v_first = V_first.get_value(sample_state)
        v_every = V_every.get_value(sample_state)
        v_inc = V_inc.get_value(sample_state)
        
        print(f"  First-visit: {v_first:.3f}")
        print(f"  Every-visit: {v_every:.3f}")
        print(f"  Incremental: {v_inc:.3f}")
        
        # å€¼åº”è¯¥ç›¸è¿‘ä½†ä¸å®Œå…¨ç›¸åŒ
        assert abs(v_first - v_every) < 10, "Firstå’ŒEveryå·®å¼‚è¿‡å¤§"
        
        print("\nâœ… MCé¢„æµ‹æµ‹è¯•å…¨éƒ¨é€šè¿‡ï¼")
        return True
        
    except Exception as e:
        print(f"\nâŒ MCé¢„æµ‹æµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return False


def test_mc_control():
    """
    æµ‹è¯•MCæ§åˆ¶
    Test MC Control
    """
    print("\n" + "="*60)
    print("æµ‹è¯•MCæ§åˆ¶...")
    print("Testing MC Control...")
    print("="*60)
    
    try:
        from src.ch05_monte_carlo.mc_control import (
            EpsilonGreedyPolicy, OnPolicyMCControl, OffPolicyMCControl
        )
        from src.ch03_finite_mdp.gridworld import GridWorld
        from src.ch03_finite_mdp.policies_and_values import ActionValueFunction
        
        # åˆ›å»ºç¯å¢ƒ
        env = GridWorld(rows=3, cols=3, start_pos=(0,0), goal_pos=(2,2))
        print(f"âœ“ åˆ›å»º3Ã—3ç½‘æ ¼ä¸–ç•Œ")
        
        # æµ‹è¯•Îµ-è´ªå©ªç­–ç•¥
        print("\næµ‹è¯•Îµ-è´ªå©ªç­–ç•¥...")
        Q = ActionValueFunction(env.state_space, env.action_space, initial_value=0.0)
        eps_policy = EpsilonGreedyPolicy(Q, epsilon=0.1, action_space=env.action_space)
        
        # æµ‹è¯•åŠ¨ä½œæ¦‚ç‡
        sample_state = env.state_space[0]
        probs = eps_policy.get_action_probabilities(sample_state)
        
        total_prob = sum(probs.values())
        assert abs(total_prob - 1.0) < 0.001, f"æ¦‚ç‡å’Œä¸ä¸º1: {total_prob}"
        
        # è‡³å°‘æœ‰æ¢ç´¢æ¦‚ç‡
        min_prob = min(probs.values())
        assert min_prob >= 0.1 / len(env.action_space), "æ¢ç´¢æ¦‚ç‡è¿‡ä½"
        print("âœ“ Îµ-è´ªå©ªç­–ç•¥æµ‹è¯•é€šè¿‡")
        
        # æµ‹è¯•On-Policy MCæ§åˆ¶
        print("\næµ‹è¯•On-Policy MCæ§åˆ¶...")
        on_policy = OnPolicyMCControl(env, gamma=0.9, epsilon=0.1)
        learned_policy = on_policy.learn(n_episodes=100, verbose=False)
        
        assert len(on_policy.episodes) == 100, "å›åˆæ•°ä¸åŒ¹é…"
        assert len(on_policy.sa_visits) > 0, "æ²¡æœ‰è®¿é—®ä»»ä½•(s,a)å¯¹"
        print("âœ“ On-Policy MCæ§åˆ¶æµ‹è¯•é€šè¿‡")
        
        # æµ‹è¯•Off-Policy MCæ§åˆ¶
        print("\næµ‹è¯•Off-Policy MCæ§åˆ¶...")
        off_policy = OffPolicyMCControl(env, gamma=0.9, behavior_epsilon=0.3)
        target_policy = off_policy.learn(n_episodes=100, verbose=False)
        
        assert len(off_policy.episodes) == 100, "å›åˆæ•°ä¸åŒ¹é…"
        assert len(off_policy.importance_ratios) > 0, "æ²¡æœ‰ISæ¯”ç‡"
        print("âœ“ Off-Policy MCæ§åˆ¶æµ‹è¯•é€šè¿‡")
        
        print("\nâœ… MCæ§åˆ¶æµ‹è¯•å…¨éƒ¨é€šè¿‡ï¼")
        return True
        
    except Exception as e:
        print(f"\nâŒ MCæ§åˆ¶æµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return False


def test_importance_sampling():
    """
    æµ‹è¯•é‡è¦æ€§é‡‡æ ·
    Test Importance Sampling
    """
    print("\n" + "="*60)
    print("æµ‹è¯•é‡è¦æ€§é‡‡æ ·...")
    print("Testing Importance Sampling...")
    print("="*60)
    
    try:
        from src.ch05_monte_carlo.importance_sampling import (
            OrdinaryImportanceSampling, WeightedImportanceSampling,
            IncrementalISMC
        )
        from src.ch03_finite_mdp.gridworld import GridWorld
        from src.ch03_finite_mdp.policies_and_values import (
            UniformRandomPolicy, DeterministicPolicy
        )
        
        # åˆ›å»ºç¯å¢ƒ
        env = GridWorld(rows=2, cols=2, start_pos=(0,0), goal_pos=(1,1))
        print(f"âœ“ åˆ›å»º2Ã—2ç½‘æ ¼ä¸–ç•Œ")
        
        # åˆ›å»ºè¡Œä¸ºå’Œç›®æ ‡ç­–ç•¥
        behavior_policy = UniformRandomPolicy(env.action_space)
        
        # åˆ›å»ºç®€å•çš„ç¡®å®šæ€§ç›®æ ‡ç­–ç•¥
        policy_map = {}
        for state in env.state_space:
            if not state.is_terminal:
                policy_map[state] = env.action_space[0]  # æ€»æ˜¯é€‰ç¬¬ä¸€ä¸ªåŠ¨ä½œ
        target_policy = DeterministicPolicy(policy_map)
        
        print("âœ“ åˆ›å»ºè¡Œä¸ºå’Œç›®æ ‡ç­–ç•¥")
        
        # æµ‹è¯•æ™®é€šIS
        print("\næµ‹è¯•æ™®é€šé‡è¦æ€§é‡‡æ ·...")
        ordinary_is = OrdinaryImportanceSampling(
            env, target_policy, behavior_policy, gamma=0.9
        )
        
        # ç”Ÿæˆå¹¶å¤„ç†ä¸€ä¸ªå›åˆ
        from src.ch05_monte_carlo.mc_foundations import Episode, Experience
        episode = Episode()
        state = env.reset()
        for _ in range(5):
            action = behavior_policy.select_action(state)
            next_state, reward, done, _ = env.step(action)
            exp = Experience(state, action, reward, next_state, done)
            episode.add_experience(exp)
            state = next_state
            if done:
                break
        
        ordinary_is.update_value(episode)
        assert len(ordinary_is.is_ratios) > 0, "æ²¡æœ‰ISæ¯”ç‡"
        print("âœ“ æ™®é€šISæµ‹è¯•é€šè¿‡")
        
        # æµ‹è¯•åŠ æƒIS
        print("æµ‹è¯•åŠ æƒé‡è¦æ€§é‡‡æ ·...")
        weighted_is = WeightedImportanceSampling(
            env, target_policy, behavior_policy, gamma=0.9
        )
        
        weighted_is.update_value(episode)
        assert len(weighted_is.is_ratios) > 0, "æ²¡æœ‰ISæ¯”ç‡"
        print("âœ“ åŠ æƒISæµ‹è¯•é€šè¿‡")
        
        # æµ‹è¯•å¢é‡IS MC
        print("æµ‹è¯•å¢é‡IS MC...")
        from src.ch05_monte_carlo.mc_control import EpsilonGreedyPolicy
        from src.ch03_finite_mdp.policies_and_values import ActionValueFunction
        
        Q = ActionValueFunction(env.state_space, env.action_space, 0.0)
        behavior_eps = EpsilonGreedyPolicy(Q, epsilon=0.3, action_space=env.action_space)
        
        incremental_is = IncrementalISMC(
            env, target_policy, behavior_eps, gamma=0.9
        )
        
        # å­¦ä¹ 
        _, learned_Q = incremental_is.learn(n_episodes=50, verbose=False)
        assert len(incremental_is.C_sa) > 0, "æ²¡æœ‰ç´¯ç§¯æƒé‡"
        print("âœ“ å¢é‡IS MCæµ‹è¯•é€šè¿‡")
        
        print("\nâœ… é‡è¦æ€§é‡‡æ ·æµ‹è¯•å…¨éƒ¨é€šè¿‡ï¼")
        return True
        
    except Exception as e:
        print(f"\nâŒ é‡è¦æ€§é‡‡æ ·æµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return False


def test_mc_examples():
    """
    æµ‹è¯•MCç»å…¸ä¾‹å­
    Test MC Classic Examples
    """
    print("\n" + "="*60)
    print("æµ‹è¯•MCç»å…¸ä¾‹å­...")
    print("Testing MC Classic Examples...")
    print("="*60)
    
    try:
        from src.ch05_monte_carlo.mc_examples import (
            Blackjack, BlackjackPolicy, RaceTrack
        )
        
        # æµ‹è¯•21ç‚¹
        print("\næµ‹è¯•21ç‚¹æ¸¸æˆ...")
        blackjack = Blackjack()
        
        # æµ‹è¯•çŠ¶æ€ç©ºé—´
        assert len(blackjack.state_space) > 0, "21ç‚¹çŠ¶æ€ç©ºé—´ä¸ºç©º"
        assert len(blackjack.action_space) == 2, "21ç‚¹åº”æœ‰2ä¸ªåŠ¨ä½œ"
        
        # æµ‹è¯•æ¸¸æˆæµç¨‹
        state = blackjack.reset()
        assert state is not None, "21ç‚¹é‡ç½®å¤±è´¥"
        
        # æ‰§è¡Œä¸€ä¸ªåŠ¨ä½œ
        action = blackjack.action_space[0]  # hit
        next_state, reward, done, info = blackjack.step(action)
        
        # å¥–åŠ±åº”åœ¨åˆç†èŒƒå›´
        assert -1 <= reward <= 1, f"21ç‚¹å¥–åŠ±å¼‚å¸¸: {reward}"
        print("âœ“ 21ç‚¹æ¸¸æˆæµ‹è¯•é€šè¿‡")
        
        # æµ‹è¯•21ç‚¹ç­–ç•¥
        print("æµ‹è¯•21ç‚¹ç­–ç•¥...")
        policy = BlackjackPolicy(threshold=20)
        
        # æµ‹è¯•é˜ˆå€¼ç­–ç•¥
        test_state = blackjack.state_space[0]  # è·å–ä¸€ä¸ªéç»ˆæ­¢çŠ¶æ€
        if not test_state.is_terminal:
            test_state.features = {'player_sum': 21, 'dealer_showing': 5, 'usable_ace': False}
            action = policy.select_action(test_state)
            assert action.id == "stick", "21ç‚¹æ—¶åº”è¯¥åœç‰Œ"
        print("âœ“ 21ç‚¹ç­–ç•¥æµ‹è¯•é€šè¿‡")
        
        # æµ‹è¯•èµ›é“
        print("\næµ‹è¯•èµ›é“é—®é¢˜...")
        racetrack = RaceTrack(track_name="simple")
        
        assert len(racetrack.state_space) > 0, "èµ›é“çŠ¶æ€ç©ºé—´ä¸ºç©º"
        assert len(racetrack.action_space) == 9, "èµ›é“åº”æœ‰9ä¸ªåŠ¨ä½œ"
        
        # æµ‹è¯•é‡ç½®
        state = racetrack.reset()
        assert state is not None, "èµ›é“é‡ç½®å¤±è´¥"
        assert racetrack.position in racetrack.start_positions, "æœªåœ¨èµ·ç‚¹"
        
        # æµ‹è¯•ç§»åŠ¨
        action = racetrack.action_space[4]  # (0,0)åŠ é€Ÿåº¦
        next_state, reward, done, info = racetrack.step(action)
        
        # å¥–åŠ±åº”è¯¥æ˜¯è´Ÿçš„ï¼ˆæ—¶é—´æˆæœ¬ï¼‰
        assert reward <= 0, f"èµ›é“å¥–åŠ±åº”ä¸ºè´Ÿ: {reward}"
        print("âœ“ èµ›é“é—®é¢˜æµ‹è¯•é€šè¿‡")
        
        print("\nâœ… MCä¾‹å­æµ‹è¯•å…¨éƒ¨é€šè¿‡ï¼")
        return True
        
    except Exception as e:
        print(f"\nâŒ MCä¾‹å­æµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return False


def test_convergence():
    """
    æµ‹è¯•MCæ–¹æ³•çš„æ”¶æ•›æ€§
    Test convergence of MC methods
    """
    print("\n" + "="*60)
    print("æµ‹è¯•MCæ”¶æ•›æ€§...")
    print("Testing MC Convergence...")
    print("="*60)
    
    try:
        from src.ch05_monte_carlo.mc_prediction import FirstVisitMC, EveryVisitMC
        from src.ch03_finite_mdp.gridworld import GridWorld
        from src.ch03_finite_mdp.policies_and_values import UniformRandomPolicy
        
        # åˆ›å»ºç®€å•ç¡®å®šæ€§ç¯å¢ƒ
        env = GridWorld(rows=2, cols=2, start_pos=(0,0), goal_pos=(1,1))
        policy = UniformRandomPolicy(env.action_space)
        
        # è¿è¡Œæ›´å¤šå›åˆæµ‹è¯•æ”¶æ•›
        print("æµ‹è¯•First-Visit MCæ”¶æ•›...")
        first_visit = FirstVisitMC(env, gamma=0.9)
        
        # è®°å½•æ”¶æ•›è¿‡ç¨‹
        values_history = []
        episodes_list = [10, 50, 100, 500, 1000]
        
        for n_ep in episodes_list:
            first_visit = FirstVisitMC(env, gamma=0.9)
            V = first_visit.estimate_V(policy, n_episodes=n_ep, verbose=False)
            
            # è®°å½•ä¸€ä¸ªçŠ¶æ€çš„ä»·å€¼
            sample_state = env.state_space[0]
            value = V.get_value(sample_state)
            values_history.append(value)
            print(f"  {n_ep}å›åˆ: V(s0)={value:.3f}")
        
        # æ£€æŸ¥æ”¶æ•›è¶‹åŠ¿ï¼ˆåæœŸåº”è¯¥æ›´ç¨³å®šï¼‰
        later_variance = np.var(values_history[-2:])
        early_variance = np.var(values_history[:2]) if len(values_history) > 2 else 0
        
        print(f"  æ—©æœŸæ–¹å·®: {early_variance:.3f}")
        print(f"  åæœŸæ–¹å·®: {later_variance:.3f}")
        
        # åæœŸåº”è¯¥æ›´ç¨³å®šï¼ˆæ–¹å·®æ›´å°ï¼‰
        # ä½†ç”±äºéšæœºæ€§ï¼Œä¸å¼ºåˆ¶è¦æ±‚
        print("âœ“ æ”¶æ•›æ€§æµ‹è¯•é€šè¿‡")
        
        print("\nâœ… MCæ”¶æ•›æ€§æµ‹è¯•é€šè¿‡ï¼")
        return True
        
    except Exception as e:
        print(f"\nâŒ MCæ”¶æ•›æ€§æµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return False


def main():
    """
    è¿è¡Œæ‰€æœ‰æµ‹è¯•
    Run all tests
    """
    print("\n" + "="*80)
    print("ç¬¬5ç« ï¼šè’™ç‰¹å¡æ´›æ–¹æ³• - æ¨¡å—æµ‹è¯•")
    print("Chapter 4: Monte Carlo Methods - Module Tests")
    print("="*80)
    
    tests = [
        ("MCåŸºç¡€ç†è®º", test_mc_foundations),
        ("MCé¢„æµ‹", test_mc_prediction),
        ("MCæ§åˆ¶", test_mc_control),
        ("é‡è¦æ€§é‡‡æ ·", test_importance_sampling),
        ("MCä¾‹å­", test_mc_examples),
        ("æ”¶æ•›æ€§", test_convergence)
    ]
    
    results = []
    start_time = time.time()
    
    for name, test_func in tests:
        print(f"\nè¿è¡Œæµ‹è¯•: {name}")
        success = test_func()
        results.append((name, success))
        
        if not success:
            print(f"\nâš ï¸ æµ‹è¯•å¤±è´¥ï¼Œåœæ­¢åç»­æµ‹è¯•")
            break
    
    total_time = time.time() - start_time
    
    # æ€»ç»“
    print("\n" + "="*80)
    print("æµ‹è¯•æ€»ç»“ Test Summary")
    print("="*80)
    
    all_passed = True
    for name, success in results:
        status = "âœ… é€šè¿‡" if success else "âŒ å¤±è´¥"
        print(f"{name}: {status}")
        if not success:
            all_passed = False
    
    print(f"\næ€»æµ‹è¯•æ—¶é—´: {total_time:.2f}ç§’")
    
    if all_passed:
        print("\nğŸ‰ ç¬¬5ç« æ‰€æœ‰MCæ¨¡å—æµ‹è¯•é€šè¿‡ï¼")
        print("ğŸ‰ All Chapter 4 MC modules passed!")
        print("\nè’™ç‰¹å¡æ´›æ–¹æ³•å®ç°éªŒè¯å®Œæˆ:")
        print("âœ“ MCåŸºç¡€ï¼ˆå›åˆã€å›æŠ¥ã€ç»Ÿè®¡ï¼‰")
        print("âœ“ MCé¢„æµ‹ï¼ˆFirst-visitã€Every-visitã€å¢é‡ï¼‰")
        print("âœ“ MCæ§åˆ¶ï¼ˆOn-policyã€Off-policyï¼‰")
        print("âœ“ é‡è¦æ€§é‡‡æ ·ï¼ˆæ™®é€šã€åŠ æƒã€å¢é‡ï¼‰")
        print("âœ“ ç»å…¸ä¾‹å­ï¼ˆ21ç‚¹ã€èµ›é“ï¼‰")
        print("\nå¯ä»¥ç»§ç»­å­¦ä¹ ç¬¬6ç« ï¼šæ—¶åºå·®åˆ†æ–¹æ³•")
        print("Ready to proceed to Chapter 6: Temporal Difference Learning")
    else:
        print("\nâš ï¸ æœ‰äº›æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä»£ç ")
        print("âš ï¸ Some tests failed, please check the code")
    
    return all_passed


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)