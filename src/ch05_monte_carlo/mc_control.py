"""
================================================================================
ç¬¬4.3èŠ‚ï¼šè’™ç‰¹å¡æ´›æ§åˆ¶ - ä»ç»éªŒä¸­å­¦ä¹ æœ€ä¼˜ç­–ç•¥
Section 4.3: Monte Carlo Control - Learning Optimal Policy from Experience
================================================================================

MCæ§åˆ¶å°†MCé¢„æµ‹æ‰©å±•åˆ°å¯»æ‰¾æœ€ä¼˜ç­–ç•¥çš„é—®é¢˜ã€‚
MC control extends MC prediction to the problem of finding optimal policy.

æ ¸å¿ƒæŒ‘æˆ˜ï¼šæ¢ç´¢-åˆ©ç”¨æƒè¡¡
Core challenge: Exploration-Exploitation Trade-off
- éœ€è¦æ¢ç´¢æ‰€æœ‰åŠ¨ä½œæ¥å‡†ç¡®ä¼°è®¡Q
  Need to explore all actions to estimate Q accurately
- éœ€è¦åˆ©ç”¨å½“å‰çŸ¥è¯†æ¥æ”¹è¿›ç­–ç•¥
  Need to exploit current knowledge to improve policy

ä¸¤ç§ä¸»è¦æ–¹æ³•ï¼š
Two main approaches:
1. On-Policy MCæ§åˆ¶ï¼šè¯„ä¼°å’Œæ”¹è¿›åŒä¸€ä¸ªç­–ç•¥
   On-Policy MC Control: Evaluate and improve the same policy
   - ä½¿ç”¨Îµ-è´ªå©ªç­–ç•¥ä¿è¯æ¢ç´¢
     Use Îµ-greedy policy to ensure exploration
   - ç®€å•ä½†å¯èƒ½æ”¶æ•›åˆ°æ¬¡ä¼˜
     Simple but may converge to suboptimal

2. Off-Policy MCæ§åˆ¶ï¼šè¡Œä¸ºç­–ç•¥â‰ ç›®æ ‡ç­–ç•¥
   Off-Policy MC Control: Behavior policy â‰  Target policy
   - è¡Œä¸ºç­–ç•¥æ¢ç´¢ï¼Œç›®æ ‡ç­–ç•¥è´ªå©ª
     Behavior policy explores, target policy is greedy
   - éœ€è¦é‡è¦æ€§é‡‡æ ·ä½†å¯ä»¥æ‰¾åˆ°æœ€ä¼˜
     Needs importance sampling but can find optimal

ç‰¹æ®ŠæŠ€å·§ï¼š
Special techniques:
- æ¢ç´¢æ€§èµ·å§‹ï¼ˆExploring Startsï¼‰ï¼šä»æ‰€æœ‰(s,a)å¯¹å¼€å§‹
  Exploring Starts: Start from all (s,a) pairs
- Îµ-è´ªå©ªç­–ç•¥ï¼šä»¥Îµæ¦‚ç‡éšæœºæ¢ç´¢
  Îµ-greedy policy: Explore randomly with probability Îµ
- è½¯ç­–ç•¥ï¼ˆSoft Policyï¼‰ï¼šæ‰€æœ‰åŠ¨ä½œéƒ½æœ‰éé›¶æ¦‚ç‡
  Soft Policy: All actions have non-zero probability

è¿™æ˜¯é€šå‘Q-learningçš„é‡è¦ä¸€æ­¥ï¼
This is an important step towards Q-learning!
"""

import numpy as np
from typing import List, Dict, Tuple, Optional, Any, Callable, Set, Union
from dataclasses import dataclass, field
from collections import defaultdict, deque
from abc import ABC, abstractmethod
import logging
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import seaborn as sns
from scipy import stats
import time
import random

# å¯¼å…¥åŸºç¡€ç»„ä»¶
import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent))

from ch02_mdp.mdp_framework import State, Action, MDPEnvironment
from ch02_mdp.policies_and_values import (
    Policy, StateValueFunction, ActionValueFunction,
    StochasticPolicy, DeterministicPolicy
)
from ch04_monte_carlo.mc_foundations import (
    Episode, Experience, Return, MCStatistics
)
from ch04_monte_carlo.mc_prediction import MCPrediction, FirstVisitMC

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


# ================================================================================
# ç¬¬4.3.1èŠ‚ï¼šÎµ-è´ªå©ªç­–ç•¥
# Section 4.3.1: Epsilon-Greedy Policy
# ================================================================================

class EpsilonGreedyPolicy(StochasticPolicy):
    """
    Îµ-è´ªå©ªç­–ç•¥
    Epsilon-Greedy Policy
    
    å¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨çš„ç»å…¸æ–¹æ³•
    Classic method to balance exploration and exploitation
    
    ç­–ç•¥å®šä¹‰ï¼š
    Policy definition:
    Ï€(a|s) = {
        1 - Îµ + Îµ/|A|    if a = argmax_a Q(s,a)  (è´ªå©ªåŠ¨ä½œ)
        Îµ/|A|            otherwise                 (æ¢ç´¢åŠ¨ä½œ)
    }
    
    å…³é”®æ€§è´¨ï¼š
    Key properties:
    1. ä¿è¯æ¢ç´¢ï¼šæ¯ä¸ªåŠ¨ä½œè‡³å°‘æœ‰Îµ/|A|çš„æ¦‚ç‡
       Ensures exploration: Each action has at least Îµ/|A| probability
    2. ä¸»è¦åˆ©ç”¨ï¼šè´ªå©ªåŠ¨ä½œæœ‰1-Îµ+Îµ/|A|çš„æ¦‚ç‡
       Mainly exploits: Greedy action has 1-Îµ+Îµ/|A| probability
    3. è½¯ç­–ç•¥ï¼šÏ€(a|s) > 0 å¯¹æ‰€æœ‰a
       Soft policy: Ï€(a|s) > 0 for all a
    4. å¯ä»¥é€€ç«ï¼šÎµå¯ä»¥éšæ—¶é—´å‡å°
       Can be annealed: Îµ can decrease over time
    
    ä¸ºä»€ä¹ˆéœ€è¦Îµ-è´ªå©ªï¼Ÿ
    Why need Îµ-greedy?
    - çº¯è´ªå©ªä¼šé™·å…¥å±€éƒ¨æœ€ä¼˜
      Pure greedy gets stuck in local optimum
    - å®Œå…¨éšæœºå­¦ä¹ å¤ªæ…¢
      Fully random learns too slowly
    - Îµ-è´ªå©ªæ˜¯ç®€å•æœ‰æ•ˆçš„æŠ˜ä¸­
      Îµ-greedy is simple and effective compromise
    
    ç±»æ¯”ï¼šé¤å…é€‰æ‹©
    Analogy: Restaurant selection
    - é€šå¸¸å»æœ€å–œæ¬¢çš„é¤å…ï¼ˆåˆ©ç”¨ï¼‰
      Usually go to favorite restaurant (exploit)
    - å¶å°”å°è¯•æ–°é¤å…ï¼ˆæ¢ç´¢ï¼‰
      Occasionally try new restaurants (explore)
    """
    
    def __init__(self, 
                 Q: ActionValueFunction,
                 epsilon: float = 0.1,
                 epsilon_decay: float = 1.0,
                 epsilon_min: float = 0.01,
                 action_space: Optional[List[Action]] = None):
        """
        åˆå§‹åŒ–Îµ-è´ªå©ªç­–ç•¥
        Initialize Îµ-greedy policy
        
        Args:
            Q: åŠ¨ä½œä»·å€¼å‡½æ•°
               Action-value function
            epsilon: åˆå§‹æ¢ç´¢ç‡
                    Initial exploration rate
            epsilon_decay: è¡°å‡å› å­ï¼ˆæ¯å›åˆä¹˜ä»¥æ­¤å€¼ï¼‰
                          Decay factor (multiply each episode)
            epsilon_min: æœ€å°æ¢ç´¢ç‡
                        Minimum exploration rate
            action_space: åŠ¨ä½œç©ºé—´ï¼ˆå¯é€‰ï¼Œå¦‚æœä¸æä¾›åˆ™ä»Qæ¨æ–­ï¼‰
                        Action space (optional, inferred from Q if not provided)
        
        è®¾è®¡é€‰æ‹©ï¼š
        Design choices:
        - epsilon=0.1æ˜¯å¸¸è§é€‰æ‹©ï¼ˆ10%æ¢ç´¢ï¼‰
          epsilon=0.1 is common choice (10% exploration)
        - è¡°å‡å¸®åŠ©åæœŸæ”¶æ•›
          Decay helps late convergence
        - ä¿æŒæœ€å°å€¼é¿å…å®Œå…¨è´ªå©ª
          Keep minimum to avoid pure greedy
        """
        # åˆå§‹åŒ–åŸºç±»ï¼Œä¼ å…¥ç©ºçš„policy_probsï¼ˆæˆ‘ä»¬åŠ¨æ€è®¡ç®—ï¼‰
        # Initialize base class with empty policy_probs (we calculate dynamically)
        super().__init__(policy_probs={})
        self.Q = Q
        self.epsilon = epsilon
        self.epsilon_initial = epsilon
        self.epsilon_decay = epsilon_decay
        self.epsilon_min = epsilon_min
        # å­˜å‚¨åŠ¨ä½œç©ºé—´ä»¥ç¬¦åˆåŸºç±»Policyæ¥å£
        # Store action space to comply with base Policy interface
        self.action_space = action_space if action_space else Q.actions
        
        # è®°å½•é€‰æ‹©å†å²ï¼ˆç”¨äºåˆ†æï¼‰
        # Record selection history (for analysis)
        self.selection_history = []
        self.exploration_count = 0
        self.exploitation_count = 0
        
        logger.info(f"åˆå§‹åŒ–Îµ-è´ªå©ªç­–ç•¥: Îµ={epsilon}")
    
    def get_action_probabilities(self, state: State, 
                                action_space: Optional[List[Action]] = None) -> Dict[Action, float]:
        """
        è·å–åŠ¨ä½œæ¦‚ç‡åˆ†å¸ƒ
        Get action probability distribution
        
        å®ç°Îµ-è´ªå©ªçš„æ¦‚ç‡åˆ†é…
        Implement Îµ-greedy probability assignment
        
        å…¼å®¹åŸºç±»æ¥å£ä½†ä¹Ÿæ”¯æŒä¼ å…¥action_space
        Compatible with base interface but also supports passing action_space
        """
        # ä½¿ç”¨æä¾›çš„åŠ¨ä½œç©ºé—´æˆ–å­˜å‚¨çš„åŠ¨ä½œç©ºé—´
        # Use provided action space or stored action space
        actions = action_space if action_space else self.action_space
        probs = {}
        
        # æ‰¾åˆ°è´ªå©ªåŠ¨ä½œï¼ˆQå€¼æœ€å¤§çš„ï¼‰
        # Find greedy action (max Q-value)
        q_values = {a: self.Q.get_value(state, a) for a in actions}
        max_q = max(q_values.values())
        
        # å¯èƒ½æœ‰å¤šä¸ªæœ€ä¼˜åŠ¨ä½œï¼ˆæ‰“ç ´å¹³å±€ï¼‰
        # May have multiple optimal actions (tie-breaking)
        greedy_actions = [a for a, q in q_values.items() if q == max_q]
        n_greedy = len(greedy_actions)
        n_actions = len(actions)
        
        # è®¡ç®—æ¦‚ç‡
        # Calculate probabilities
        for action in actions:
            if action in greedy_actions:
                # è´ªå©ªåŠ¨ä½œï¼šåŸºç¡€æ¢ç´¢æ¦‚ç‡ + é¢å¤–çš„åˆ©ç”¨æ¦‚ç‡
                # Greedy action: base exploration + extra exploitation
                probs[action] = self.epsilon / n_actions + (1 - self.epsilon) / n_greedy
            else:
                # éè´ªå©ªåŠ¨ä½œï¼šåªæœ‰æ¢ç´¢æ¦‚ç‡
                # Non-greedy action: only exploration probability
                probs[action] = self.epsilon / n_actions
        
        return probs
    
    def select_action(self, state: State) -> Action:
        """
        é€‰æ‹©åŠ¨ä½œ
        Select action
        
        ä½¿ç”¨Îµ-è´ªå©ªç­–ç•¥
        Use Îµ-greedy strategy
        
        éµå¾ªåŸºç±»Policyæ¥å£ï¼Œä¸éœ€è¦action_spaceå‚æ•°
        Follow base Policy interface, no action_space parameter needed
        """
        # Îµæ¦‚ç‡éšæœºæ¢ç´¢
        # Random exploration with probability Îµ
        if np.random.random() < self.epsilon:
            action = np.random.choice(self.action_space)
            self.exploration_count += 1
            self.selection_history.append(('explore', state.id, action.id))
        else:
            # 1-Îµæ¦‚ç‡é€‰æ‹©è´ªå©ªåŠ¨ä½œ
            # Select greedy action with probability 1-Îµ
            q_values = {a: self.Q.get_value(state, a) for a in self.action_space}
            max_q = max(q_values.values())
            best_actions = [a for a, q in q_values.items() if q == max_q]
            action = np.random.choice(best_actions)  # éšæœºæ‰“ç ´å¹³å±€
            self.exploitation_count += 1
            self.selection_history.append(('exploit', state.id, action.id))
        
        return action
    
    def decay_epsilon(self):
        """
        è¡°å‡æ¢ç´¢ç‡
        Decay exploration rate
        
        ç”¨äºé€€ç«ç­–ç•¥
        Used for annealing schedule
        """
        old_epsilon = self.epsilon
        self.epsilon = max(self.epsilon * self.epsilon_decay, self.epsilon_min)
        
        if old_epsilon != self.epsilon:
            logger.debug(f"Îµè¡°å‡: {old_epsilon:.4f} â†’ {self.epsilon:.4f}")
    
    def reset_epsilon(self):
        """
        é‡ç½®æ¢ç´¢ç‡
        Reset exploration rate
        """
        self.epsilon = self.epsilon_initial
        self.exploration_count = 0
        self.exploitation_count = 0
        self.selection_history.clear()
    
    def get_exploration_stats(self) -> Dict[str, Any]:
        """
        è·å–æ¢ç´¢ç»Ÿè®¡
        Get exploration statistics
        """
        total = self.exploration_count + self.exploitation_count
        if total == 0:
            return {
                'epsilon': self.epsilon,
                'exploration_ratio': 0,
                'exploitation_ratio': 0,
                'total_selections': 0
            }
        
        return {
            'epsilon': self.epsilon,
            'exploration_ratio': self.exploration_count / total,
            'exploitation_ratio': self.exploitation_count / total,
            'total_selections': total,
            'exploration_count': self.exploration_count,
            'exploitation_count': self.exploitation_count
        }
    
    def analyze_exploration_pattern(self):
        """
        åˆ†ææ¢ç´¢æ¨¡å¼
        Analyze exploration pattern
        
        å±•ç¤ºæ¢ç´¢-åˆ©ç”¨çš„å¹³è¡¡
        Show exploration-exploitation balance
        """
        print("\n" + "="*60)
        print("Îµ-è´ªå©ªç­–ç•¥æ¢ç´¢åˆ†æ")
        print("Îµ-Greedy Policy Exploration Analysis")
        print("="*60)
        
        stats = self.get_exploration_stats()
        
        print(f"\nå½“å‰Îµ: {stats['epsilon']:.4f}")
        print(f"æ€»é€‰æ‹©æ¬¡æ•°: {stats['total_selections']}")
        print(f"æ¢ç´¢æ¬¡æ•°: {stats['exploration_count']} ({stats['exploration_ratio']:.2%})")
        print(f"åˆ©ç”¨æ¬¡æ•°: {stats['exploitation_count']} ({stats['exploitation_ratio']:.2%})")
        
        # ç†è®ºvså®é™…
        # Theory vs Actual
        print(f"\nç†è®ºæ¢ç´¢ç‡: {self.epsilon:.2%}")
        print(f"å®é™…æ¢ç´¢ç‡: {stats['exploration_ratio']:.2%}")
        
        # åˆ†ææœ€è¿‘çš„æ¨¡å¼
        # Analyze recent pattern
        if len(self.selection_history) >= 100:
            recent = self.selection_history[-100:]
            recent_explore = sum(1 for t, _, _ in recent if t == 'explore')
            print(f"\næœ€è¿‘100æ¬¡:")
            print(f"  æ¢ç´¢: {recent_explore}%")
            print(f"  åˆ©ç”¨: {100 - recent_explore}%")


# ================================================================================
# ç¬¬4.3.2èŠ‚ï¼šMCæ§åˆ¶åŸºç±»
# Section 4.3.2: MC Control Base Class
# ================================================================================

class MCControl(ABC):
    """
    è’™ç‰¹å¡æ´›æ§åˆ¶åŸºç±»
    Monte Carlo Control Base Class
    
    å®šä¹‰MCæ§åˆ¶ç®—æ³•çš„å…±åŒç»“æ„
    Defines common structure for MC control algorithms
    
    æ§åˆ¶ = é¢„æµ‹ + æ”¹è¿›
    Control = Prediction + Improvement
    
    é€šç”¨æµç¨‹ï¼š
    General flow:
    1. åˆå§‹åŒ–Q(s,a)å’ŒÏ€
       Initialize Q(s,a) and Ï€
    2. é‡å¤ï¼š
       Repeat:
       a. ç”¨Ï€ç”Ÿæˆå›åˆ
          Generate episode using Ï€
       b. æ›´æ–°QåŸºäºå›åˆï¼ˆé¢„æµ‹ï¼‰
          Update Q based on episode (prediction)
       c. æ”¹è¿›Ï€åŸºäºQï¼ˆæ§åˆ¶ï¼‰
          Improve Ï€ based on Q (control)
    3. ç›´åˆ°æ”¶æ•›
       Until convergence
    
    è¿™å°±æ˜¯å¹¿ä¹‰ç­–ç•¥è¿­ä»£ï¼ˆGPIï¼‰åœ¨MCä¸­çš„ä½“ç°ï¼
    This is Generalized Policy Iteration (GPI) in MC!
    
    å…³é”®è®¾è®¡å†³ç­–ï¼š
    Key design decisions:
    - ä½¿ç”¨Qè€Œä¸æ˜¯Vï¼ˆä¸éœ€è¦æ¨¡å‹ï¼‰
      Use Q not V (no model needed)
    - è½¯ç­–ç•¥ä¿è¯æ¢ç´¢
      Soft policy ensures exploration
    - å¢é‡æ›´æ–°èŠ‚çœå†…å­˜
      Incremental updates save memory
    """
    
    def __init__(self,
                 env: MDPEnvironment,
                 gamma: float = 1.0,
                 visit_type: str = 'first'):
        """
        åˆå§‹åŒ–MCæ§åˆ¶
        Initialize MC Control
        
        Args:
            env: MDPç¯å¢ƒ
            gamma: æŠ˜æ‰£å› å­
            visit_type: 'first' æˆ– 'every'
        """
        self.env = env
        self.gamma = gamma
        self.visit_type = visit_type
        
        # åˆå§‹åŒ–Qå‡½æ•°ï¼ˆéšæœºå°å€¼é¿å…å¯¹ç§°æ€§ï¼‰
        # Initialize Q function (small random to break symmetry)
        self.Q = ActionValueFunction(
            env.state_space, 
            env.action_space,
            initial_value=0.0
        )
        
        # æ·»åŠ å°çš„éšæœºå™ªå£°æ‰“ç ´å¯¹ç§°
        # Add small random noise to break symmetry
        for state in env.state_space:
            for action in env.action_space:
                noise = np.random.randn() * 0.01
                self.Q.set_value(state, action, noise)
        
        # ç»Ÿè®¡æ”¶é›†
        # Statistics collection
        self.statistics = MCStatistics()
        
        # è®¿é—®è®¡æ•°
        # Visit counts
        self.sa_visits = defaultdict(int)
        
        # å›åˆå†å²
        # Episode history
        self.episodes = []
        
        # å­¦ä¹ æ›²çº¿
        # Learning curve
        self.learning_curve = []
        self.policy_changes = []
        
        logger.info(f"åˆå§‹åŒ–MCæ§åˆ¶: Î³={gamma}, visit_type={visit_type}")
    
    @abstractmethod
    def learn(self, n_episodes: int, verbose: bool = True) -> Policy:
        """
        å­¦ä¹ æœ€ä¼˜ç­–ç•¥ï¼ˆå­ç±»å®ç°ï¼‰
        Learn optimal policy (implemented by subclasses)
        """
        pass
    
    def generate_episode(self, policy: Policy, 
                        max_steps: int = 1000,
                        exploring_starts: bool = False) -> Episode:
        """
        ç”Ÿæˆå›åˆ
        Generate episode
        
        Args:
            policy: å½“å‰ç­–ç•¥
            max_steps: æœ€å¤§æ­¥æ•°
            exploring_starts: æ˜¯å¦ä½¿ç”¨æ¢ç´¢æ€§èµ·å§‹
                            Whether to use exploring starts
        
        æ¢ç´¢æ€§èµ·å§‹çš„é‡è¦æ€§ï¼š
        Importance of exploring starts:
        - ä¿è¯æ‰€æœ‰(s,a)å¯¹è¢«è®¿é—®
          Ensures all (s,a) pairs are visited
        - è§£å†³æ¢ç´¢é—®é¢˜çš„æ›¿ä»£æ–¹æ¡ˆ
          Alternative solution to exploration problem
        - ä½†å®è·µä¸­å¯èƒ½ä¸å¯è¡Œ
          But may not be feasible in practice
        """
        episode = Episode()
        
        if exploring_starts:
            # éšæœºé€‰æ‹©èµ·å§‹çŠ¶æ€å’ŒåŠ¨ä½œ
            # Randomly select starting state and action
            non_terminal_states = [s for s in self.env.state_space 
                                  if not s.is_terminal]
            if non_terminal_states:
                state = np.random.choice(non_terminal_states)
                action = np.random.choice(self.env.action_space)
                
                # å¼ºåˆ¶æ‰§è¡Œè¿™ä¸ªåŠ¨ä½œ
                # Force execute this action
                self.env.current_state = state
                next_state, reward, done, _ = self.env.step(action)
                
                exp = Experience(state, action, reward, next_state, done)
                episode.add_experience(exp)
                
                state = next_state
                
                if done:
                    return episode
        else:
            state = self.env.reset()
        
        # ç»§ç»­æ­£å¸¸çš„å›åˆç”Ÿæˆ
        # Continue normal episode generation
        for t in range(max_steps):
            action = policy.select_action(state)
            next_state, reward, done, _ = self.env.step(action)
            
            exp = Experience(state, action, reward, next_state, done)
            episode.add_experience(exp)
            
            state = next_state
            
            if done:
                break
        
        return episode
    
    def update_Q(self, episode: Episode):
        """
        æ›´æ–°Qå‡½æ•°
        Update Q function
        
        ä½¿ç”¨MCæ–¹æ³•ä»å›åˆä¸­å­¦ä¹ 
        Learn from episode using MC method
        """
        returns = episode.compute_returns(self.gamma)
        
        if self.visit_type == 'first':
            # First-visitæ›´æ–°
            sa_pairs_seen = set()
            
            for t, exp in enumerate(episode.experiences):
                sa_pair = (exp.state.id, exp.action.id)
                
                if sa_pair not in sa_pairs_seen:
                    sa_pairs_seen.add(sa_pair)
                    G = returns[t]
                    
                    # å¢é‡æ›´æ–°Q
                    self.sa_visits[sa_pair] += 1
                    n = self.sa_visits[sa_pair]
                    
                    old_q = self.Q.get_value(exp.state, exp.action)
                    new_q = old_q + (G - old_q) / n
                    self.Q.set_value(exp.state, exp.action, new_q)
                    
                    # æ›´æ–°ç»Ÿè®¡
                    self.statistics.update_action_value(exp.state, exp.action, G)
        
        else:  # every-visit
            for t, exp in enumerate(episode.experiences):
                sa_pair = (exp.state.id, exp.action.id)
                G = returns[t]
                
                # å¢é‡æ›´æ–°Q
                self.sa_visits[sa_pair] += 1
                n = self.sa_visits[sa_pair]
                
                old_q = self.Q.get_value(exp.state, exp.action)
                new_q = old_q + (G - old_q) / n
                self.Q.set_value(exp.state, exp.action, new_q)
                
                # æ›´æ–°ç»Ÿè®¡
                self.statistics.update_action_value(exp.state, exp.action, G)
    
    def create_greedy_policy(self) -> DeterministicPolicy:
        """
        åˆ›å»ºè´ªå©ªç­–ç•¥
        Create greedy policy
        
        Ï€(s) = argmax_a Q(s,a)
        """
        policy_map = {}
        
        for state in self.env.state_space:
            if not state.is_terminal:
                # æ‰¾æœ€ä¼˜åŠ¨ä½œ
                best_action = None
                best_value = float('-inf')
                
                for action in self.env.action_space:
                    q_value = self.Q.get_value(state, action)
                    if q_value > best_value:
                        best_value = q_value
                        best_action = action
                
                if best_action:
                    policy_map[state] = best_action
        
        return DeterministicPolicy(policy_map)
    
    def evaluate_policy(self, policy: Policy, n_episodes: int = 100) -> float:
        """
        è¯„ä¼°ç­–ç•¥
        Evaluate policy
        
        è¿è¡Œå¤šä¸ªå›åˆè®¡ç®—å¹³å‡å›æŠ¥
        Run multiple episodes to compute average return
        """
        total_return = 0.0
        
        for _ in range(n_episodes):
            episode = self.generate_episode(policy, exploring_starts=False)
            if episode.experiences:
                returns = episode.compute_returns(self.gamma)
                total_return += returns[0] if returns else 0
        
        return total_return / n_episodes
    
    def analyze_learning(self):
        """
        åˆ†æå­¦ä¹ è¿‡ç¨‹
        Analyze learning process
        """
        print("\n" + "="*60)
        print("MCæ§åˆ¶å­¦ä¹ åˆ†æ")
        print("MC Control Learning Analysis")
        print("="*60)
        
        print(f"\næ€»å›åˆæ•°: {len(self.episodes)}")
        print(f"è®¿é—®çš„(s,a)å¯¹: {len(self.sa_visits)}")
        print(f"å¹³å‡è®¿é—®æ¬¡æ•°: {np.mean(list(self.sa_visits.values())):.1f}")
        
        # è®¿é—®é¢‘ç‡åˆ†å¸ƒ
        # Visit frequency distribution
        visits = list(self.sa_visits.values())
        if visits:
            print(f"\nè®¿é—®ç»Ÿè®¡:")
            print(f"  æœ€å°‘: {min(visits)}")
            print(f"  æœ€å¤š: {max(visits)}")
            print(f"  ä¸­ä½æ•°: {np.median(visits):.0f}")
            
            # æ‰¾å‡ºè®¿é—®å¾ˆå°‘çš„(s,a)å¯¹
            # Find rarely visited (s,a) pairs
            rare_pairs = sum(1 for v in visits if v < 5)
            print(f"  è®¿é—®<5æ¬¡çš„å¯¹: {rare_pairs} ({rare_pairs/len(visits):.1%})")


# ================================================================================
# ç¬¬4.3.3èŠ‚ï¼šOn-Policy MCæ§åˆ¶
# Section 4.3.3: On-Policy MC Control
# ================================================================================

class OnPolicyMCControl(MCControl):
    """
    On-Policyè’™ç‰¹å¡æ´›æ§åˆ¶
    On-Policy Monte Carlo Control
    
    è¯„ä¼°å’Œæ”¹è¿›åŒä¸€ä¸ªç­–ç•¥
    Evaluate and improve the same policy
    
    æ ¸å¿ƒæ€æƒ³ï¼šä½¿ç”¨è½¯ç­–ç•¥ï¼ˆå¦‚Îµ-è´ªå©ªï¼‰
    Core idea: Use soft policy (e.g., Îµ-greedy)
    - ç­–ç•¥å¿…é¡»æ¢ç´¢ï¼ˆè½¯ï¼‰
      Policy must explore (soft)
    - è¯„ä¼°è¿™ä¸ªè½¯ç­–ç•¥
      Evaluate this soft policy
    - æ”¹è¿›ä¹Ÿä¿æŒè½¯
      Improvement also stays soft
    
    ç®—æ³•æµç¨‹ï¼š
    Algorithm flow:
    1. åˆå§‹åŒ–Q(s,a)ä»»æ„ï¼ŒÏ€ä¸ºÎµ-è´ªå©ª
       Initialize Q(s,a) arbitrarily, Ï€ as Îµ-greedy
    2. é‡å¤æ¯ä¸ªå›åˆï¼š
       Repeat for each episode:
       a. ç”¨Ï€ç”Ÿæˆå›åˆ
          Generate episode using Ï€
       b. å¯¹å›åˆä¸­æ¯ä¸ª(s,a)ï¼š
          For each (s,a) in episode:
          - è®¡ç®—å›æŠ¥G
            Compute return G
          - æ›´æ–°Q(s,a)å‘G
            Update Q(s,a) toward G
       c. å¯¹å›åˆä¸­æ¯ä¸ªsï¼š
          For each s in episode:
          - æ›´æ–°Ï€(s)ä¸ºå…³äºQçš„Îµ-è´ªå©ª
            Update Ï€(s) to be Îµ-greedy w.r.t. Q
    
    æ”¶æ•›æ€§è´¨ï¼š
    Convergence properties:
    - æ”¶æ•›åˆ°Îµ-è´ªå©ªç­–ç•¥ä¸­çš„æœ€ä¼˜
      Converges to best among Îµ-greedy policies
    - ä¸æ˜¯å…¨å±€æœ€ä¼˜ï¼ˆå› ä¸ºå¿…é¡»ä¿æŒæ¢ç´¢ï¼‰
      Not globally optimal (must maintain exploration)
    - ä½†æ¥è¿‘æœ€ä¼˜å½“Îµå¾ˆå°æ—¶
      But near-optimal when Îµ is small
    
    ä¸ºä»€ä¹ˆå«"On-Policy"ï¼Ÿ
    Why called "On-Policy"?
    å› ä¸ºæ”¹è¿›çš„ç­–ç•¥å°±æ˜¯ç”Ÿæˆæ•°æ®çš„ç­–ç•¥
    Because the policy being improved is the one generating data
    
    ç±»æ¯”ï¼šè‡ªæˆ‘æ”¹è¿›
    Analogy: Self-improvement
    åƒä¸€ä¸ªäººé€šè¿‡å®è·µè‡ªå·±çš„æ–¹æ³•æ¥æ”¹è¿›
    Like a person improving by practicing their own method
    """
    
    def __init__(self,
                 env: MDPEnvironment,
                 gamma: float = 1.0,
                 epsilon: float = 0.1,
                 epsilon_decay: float = 0.995,
                 epsilon_min: float = 0.01,
                 visit_type: str = 'first'):
        """
        åˆå§‹åŒ–On-Policy MCæ§åˆ¶
        Initialize On-Policy MC Control
        
        Args:
            env: ç¯å¢ƒ
            gamma: æŠ˜æ‰£å› å­
            epsilon: æ¢ç´¢ç‡
            epsilon_decay: æ¢ç´¢ç‡è¡°å‡
            epsilon_min: æœ€å°æ¢ç´¢ç‡
            visit_type: è®¿é—®ç±»å‹
        """
        super().__init__(env, gamma, visit_type)
        
        # åˆ›å»ºÎµ-è´ªå©ªç­–ç•¥
        # Create Îµ-greedy policy
        self.policy = EpsilonGreedyPolicy(
            self.Q, epsilon, epsilon_decay, epsilon_min, env.action_space
        )
        
        # è®°å½•ç­–ç•¥æ”¹è¿›å†å²
        # Record policy improvement history
        self.improvement_history = []
        
        logger.info(f"åˆå§‹åŒ–On-Policy MCæ§åˆ¶: Îµ={epsilon}")
    
    def learn(self, n_episodes: int = 1000, 
             verbose: bool = True) -> Policy:
        """
        å­¦ä¹ æœ€ä¼˜ç­–ç•¥
        Learn optimal policy
        
        å®ç°On-Policy MCæ§åˆ¶ç®—æ³•
        Implement On-Policy MC control algorithm
        """
        if verbose:
            print("\n" + "="*60)
            print("On-Policy MCæ§åˆ¶")
            print("On-Policy MC Control")
            print("="*60)
            print(f"  ç¯å¢ƒ: {self.env.name}")
            print(f"  å›åˆæ•°: {n_episodes}")
            print(f"  åˆå§‹Îµ: {self.policy.epsilon}")
            print(f"  è®¿é—®ç±»å‹: {self.visit_type}")
        
        start_time = time.time()
        
        for episode_num in range(n_episodes):
            # ç”Ÿæˆå›åˆï¼ˆä½¿ç”¨å½“å‰Îµ-è´ªå©ªç­–ç•¥ï¼‰
            # Generate episode (using current Îµ-greedy policy)
            episode = self.generate_episode(self.policy)
            self.episodes.append(episode)
            
            # ä»å›åˆä¸­å­¦ä¹ ï¼ˆæ›´æ–°Qï¼‰
            # Learn from episode (update Q)
            self.update_Q(episode)
            
            # ç­–ç•¥å·²ç»é€šè¿‡Qçš„æ›´æ–°è€Œéšå¼æ”¹è¿›
            # Policy is already implicitly improved through Q update
            # ï¼ˆÎµ-è´ªå©ªè‡ªåŠ¨è·ŸéšQçš„å˜åŒ–ï¼‰
            # (Îµ-greedy automatically follows Q changes)
            
            # è¡°å‡æ¢ç´¢ç‡
            # Decay exploration rate
            self.policy.decay_epsilon()
            
            # è®°å½•å­¦ä¹ è¿›åº¦
            # Record learning progress
            if episode.experiences:
                episode_return = episode.compute_returns(self.gamma)[0]
                self.learning_curve.append(episode_return)
            
            # å®šæœŸè¾“å‡ºè¿›åº¦
            # Periodically output progress
            if verbose and (episode_num + 1) % 100 == 0:
                avg_return = np.mean(self.learning_curve[-100:]) if self.learning_curve else 0
                print(f"  Episode {episode_num + 1}/{n_episodes}: "
                      f"å¹³å‡å›æŠ¥={avg_return:.2f}, Îµ={self.policy.epsilon:.4f}")
        
        total_time = time.time() - start_time
        
        if verbose:
            print(f"\nå­¦ä¹ å®Œæˆ:")
            print(f"  æ€»æ—¶é—´: {total_time:.2f}ç§’")
            print(f"  æœ€ç»ˆÎµ: {self.policy.epsilon:.4f}")
            print(f"  è®¿é—®çš„(s,a)å¯¹: {len(self.sa_visits)}")
            
            # æ¢ç´¢ç»Ÿè®¡
            stats = self.policy.get_exploration_stats()
            print(f"  æ€»æ¢ç´¢ç‡: {stats['exploration_ratio']:.2%}")
        
        # è¿”å›æœ€ç»ˆçš„Îµ-è´ªå©ªç­–ç•¥
        # Return final Îµ-greedy policy
        return self.policy
    
    def demonstrate_gpi(self):
        """
        æ¼”ç¤ºå¹¿ä¹‰ç­–ç•¥è¿­ä»£
        Demonstrate Generalized Policy Iteration
        
        å±•ç¤ºè¯„ä¼°å’Œæ”¹è¿›çš„äº¤äº’
        Show interaction of evaluation and improvement
        """
        print("\n" + "="*60)
        print("On-Policy MCä¸­çš„GPI")
        print("GPI in On-Policy MC")
        print("="*60)
        
        print("""
        ğŸ”„ å¹¿ä¹‰ç­–ç•¥è¿­ä»£ Generalized Policy Iteration
        =============================================
        
        åœ¨On-Policy MCä¸­ï¼š
        In On-Policy MC:
        
        è¯„ä¼° Evaluation:           æ”¹è¿› Improvement:
        Q^Ï€ â† Q                    Ï€ â† Îµ-greedy(Q)
             â†˜                    â†™
              â†˜                  â†™
               â†˜                â†™
                Q^Ï€* â†â†’ Ï€*
                (æœ€ä¼˜ç‚¹ Optimal point)
        
        ç‰¹ç‚¹ Characteristics:
        ---------------------
        1. ä¸å®Œå…¨è¯„ä¼°ï¼š
           Incomplete evaluation:
           æ¯ä¸ªå›åˆåªæ›´æ–°è®¿é—®çš„(s,a)
           Each episode only updates visited (s,a)
        
        2. éšå¼æ”¹è¿›ï¼š
           Implicit improvement:
           Îµ-è´ªå©ªè‡ªåŠ¨è·ŸéšQçš„å˜åŒ–
           Îµ-greedy automatically follows Q changes
        
        3. è½¯æ”¶æ•›ï¼š
           Soft convergence:
           æ”¶æ•›åˆ°Îµ-è½¯æœ€ä¼˜ç­–ç•¥
           Converges to Îµ-soft optimal policy
        
        ä¸DPçš„åŒºåˆ« Difference from DP:
        -------------------------------
        DP:  å®Œå…¨è¯„ä¼° â†’ å®Œå…¨æ”¹è¿›
             Complete evaluation â†’ Complete improvement
        
        MC:  éƒ¨åˆ†è¯„ä¼° â†’ éšå¼æ”¹è¿›
             Partial evaluation â†’ Implicit improvement
        
        æ•ˆæœ Effect:
        -----------
        - æ›´é«˜æ•ˆï¼ˆä¸éœ€è¦éå†æ‰€æœ‰çŠ¶æ€ï¼‰
          More efficient (no need to sweep all states)
        - å¯èƒ½æ›´æ…¢æ”¶æ•›ï¼ˆé‡‡æ ·æ–¹å·®ï¼‰
          May converge slower (sampling variance)
        - å®é™…å¯è¡Œï¼ˆä¸éœ€è¦æ¨¡å‹ï¼‰
          Practically feasible (no model needed)
        """)


# ================================================================================
# ç¬¬4.3.4èŠ‚ï¼šOff-Policy MCæ§åˆ¶
# Section 4.3.4: Off-Policy MC Control
# ================================================================================

class OffPolicyMCControl(MCControl):
    """
    Off-Policyè’™ç‰¹å¡æ´›æ§åˆ¶
    Off-Policy Monte Carlo Control
    
    ä½¿ç”¨ä¸åŒçš„ç­–ç•¥æ¥æ¢ç´¢å’Œå­¦ä¹ 
    Use different policies for exploration and learning
    
    ä¸¤ä¸ªç­–ç•¥ï¼š
    Two policies:
    1. è¡Œä¸ºç­–ç•¥b(a|s)ï¼šç”Ÿæˆæ•°æ®ï¼Œå¿…é¡»æ¢ç´¢
       Behavior policy b(a|s): Generates data, must explore
    2. ç›®æ ‡ç­–ç•¥Ï€(a|s)ï¼šè¦å­¦ä¹ çš„ç­–ç•¥ï¼Œå¯ä»¥ç¡®å®šæ€§
       Target policy Ï€(a|s): Policy to learn, can be deterministic
    
    æ ¸å¿ƒæŠ€æœ¯ï¼šé‡è¦æ€§é‡‡æ ·
    Core technique: Importance Sampling
    - ç”¨bçš„æ•°æ®ä¼°è®¡Ï€çš„ä»·å€¼
      Use b's data to estimate Ï€'s value
    - éœ€è¦é‡è¦æ€§é‡‡æ ·æ¯”ç‡
      Need importance sampling ratio
    
    ç®—æ³•æµç¨‹ï¼š
    Algorithm flow:
    1. åˆå§‹åŒ–Q(s,a)ï¼ŒÏ€ä¸ºè´ªå©ª
       Initialize Q(s,a), Ï€ as greedy
    2. é‡å¤ï¼š
       Repeat:
       a. ç”¨bç”Ÿæˆå›åˆ
          Generate episode using b
       b. è®¡ç®—é‡è¦æ€§é‡‡æ ·æ¯”ç‡
          Compute importance sampling ratio
       c. ç”¨åŠ æƒå›æŠ¥æ›´æ–°Q
          Update Q with weighted returns
       d. æ›´æ–°Ï€ä¸ºå…³äºQè´ªå©ª
          Update Ï€ to be greedy w.r.t. Q
    
    ä¼˜åŠ¿ï¼š
    Advantages:
    - å¯ä»¥å­¦ä¹ æœ€ä¼˜ç¡®å®šæ€§ç­–ç•¥
      Can learn optimal deterministic policy
    - å¯ä»¥é‡ç”¨ä»»ä½•ç­–ç•¥çš„æ•°æ®
      Can reuse data from any policy
    - æ›´çµæ´»çš„æ¢ç´¢ç­–ç•¥
      More flexible exploration strategy
    
    åŠ£åŠ¿ï¼š
    Disadvantages:
    - é«˜æ–¹å·®ï¼ˆé‡è¦æ€§é‡‡æ ·ï¼‰
      High variance (importance sampling)
    - éœ€è¦b(a|s) > 0å½“Ï€(a|s) > 0
      Need b(a|s) > 0 when Ï€(a|s) > 0
    - æ”¶æ•›å¯èƒ½å¾ˆæ…¢
      Convergence can be slow
    
    ä¸ºä»€ä¹ˆå«"Off-Policy"ï¼Ÿ
    Why called "Off-Policy"?
    å› ä¸ºç›®æ ‡ç­–ç•¥"ç¦»çº¿"å­¦ä¹ ï¼Œä¸ç›´æ¥ç”Ÿæˆæ•°æ®
    Because target policy learns "offline", not directly generating data
    
    ç±»æ¯”ï¼šè§‚å¯Ÿå­¦ä¹ 
    Analogy: Observational learning
    åƒé€šè¿‡è§‚å¯Ÿåˆ«äººæ¥å­¦ä¹ æœ€ä¼˜è¡Œä¸º
    Like learning optimal behavior by observing others
    """
    
    def __init__(self,
                 env: MDPEnvironment,
                 gamma: float = 1.0,
                 behavior_epsilon: float = 0.3,
                 visit_type: str = 'first'):
        """
        åˆå§‹åŒ–Off-Policy MCæ§åˆ¶
        Initialize Off-Policy MC Control
        
        Args:
            env: ç¯å¢ƒ
            gamma: æŠ˜æ‰£å› å­
            behavior_epsilon: è¡Œä¸ºç­–ç•¥çš„æ¢ç´¢ç‡
                            Exploration rate for behavior policy
            visit_type: è®¿é—®ç±»å‹
        """
        super().__init__(env, gamma, visit_type)
        
        # è¡Œä¸ºç­–ç•¥ï¼ˆÎµ-è´ªå©ªï¼Œæ¢ç´¢ï¼‰
        # Behavior policy (Îµ-greedy, explores)
        self.behavior_policy = EpsilonGreedyPolicy(
            self.Q, 
            epsilon=behavior_epsilon,
            epsilon_decay=1.0,  # ä¸è¡°å‡ï¼Œä¿æŒæ¢ç´¢
            epsilon_min=behavior_epsilon,
            action_space=env.action_space
        )
        
        # ç›®æ ‡ç­–ç•¥ï¼ˆè´ªå©ªï¼Œç¡®å®šæ€§ï¼‰
        # Target policy (greedy, deterministic)
        self.target_policy = self.create_greedy_policy()
        
        # ç´¯ç§¯åˆ†æ¯ï¼ˆç”¨äºåŠ æƒé‡è¦æ€§é‡‡æ ·ï¼‰
        # Cumulative denominator (for weighted importance sampling)
        self.C = defaultdict(float)
        
        # è®°å½•é‡è¦æ€§é‡‡æ ·æ¯”ç‡
        # Record importance sampling ratios
        self.importance_ratios = []
        
        logger.info(f"åˆå§‹åŒ–Off-Policy MCæ§åˆ¶: è¡Œä¸ºÎµ={behavior_epsilon}")
    
    def compute_importance_ratio(self, episode: Episode, t: int) -> float:
        """
        è®¡ç®—é‡è¦æ€§é‡‡æ ·æ¯”ç‡
        Compute importance sampling ratio
        
        Ï_{t:T-1} = âˆ_{k=t}^{T-1} [Ï€(A_k|S_k) / b(A_k|S_k)]
        
        è¿™æ˜¯off-policyçš„å…³é”®ï¼
        This is the key to off-policy!
        
        Args:
            episode: å›åˆ
            t: èµ·å§‹æ—¶é—´æ­¥
        
        Returns:
            é‡è¦æ€§é‡‡æ ·æ¯”ç‡
            Importance sampling ratio
        
        æ•°å­¦åŸç†ï¼š
        Mathematical principle:
        - æœŸæœ›çš„å˜æ¢
          Transformation of expectation
        - E_b[Ï Ã— G] = E_Ï€[G]
        - ä½¿bçš„æ•°æ®æ— åä¼°è®¡Ï€
          Makes b's data unbiased for Ï€
        """
        ratio = 1.0
        
        for k in range(t, len(episode.experiences)):
            exp = episode.experiences[k]
            
            # è·å–ç›®æ ‡ç­–ç•¥æ¦‚ç‡
            # Get target policy probability
            if isinstance(self.target_policy, DeterministicPolicy):
                if exp.state in self.target_policy.policy_map:
                    target_action = self.target_policy.policy_map[exp.state]
                    target_prob = 1.0 if exp.action.id == target_action.id else 0.0
                else:
                    target_prob = 0.0
            else:
                target_probs = self.target_policy.get_action_probabilities(
                    exp.state
                )
                target_prob = target_probs.get(exp.action, 0.0)
            
            # è·å–è¡Œä¸ºç­–ç•¥æ¦‚ç‡
            # Get behavior policy probability
            behavior_probs = self.behavior_policy.get_action_probabilities(
                exp.state
            )
            behavior_prob = behavior_probs.get(exp.action, 1e-10)  # é¿å…é™¤é›¶
            
            # ç´¯ç§¯æ¯”ç‡
            # Accumulate ratio
            ratio *= target_prob / behavior_prob
            
            # å¦‚æœæ¯”ç‡ä¸º0ï¼Œåç»­éƒ½æ˜¯0
            # If ratio is 0, all subsequent are 0
            if ratio == 0:
                break
        
        return ratio
    
    def learn(self, n_episodes: int = 1000,
             verbose: bool = True) -> Policy:
        """
        å­¦ä¹ æœ€ä¼˜ç­–ç•¥
        Learn optimal policy
        
        å®ç°Off-Policy MCæ§åˆ¶ï¼ˆåŠ æƒé‡è¦æ€§é‡‡æ ·ï¼‰
        Implement Off-Policy MC Control (weighted importance sampling)
        """
        if verbose:
            print("\n" + "="*60)
            print("Off-Policy MCæ§åˆ¶")
            print("Off-Policy MC Control")
            print("="*60)
            print(f"  ç¯å¢ƒ: {self.env.name}")
            print(f"  å›åˆæ•°: {n_episodes}")
            print(f"  è¡Œä¸ºç­–ç•¥Îµ: {self.behavior_policy.epsilon}")
            print(f"  ç›®æ ‡ç­–ç•¥: è´ªå©ªï¼ˆç¡®å®šæ€§ï¼‰")
        
        start_time = time.time()
        
        for episode_num in range(n_episodes):
            # ç”¨è¡Œä¸ºç­–ç•¥ç”Ÿæˆå›åˆ
            # Generate episode using behavior policy
            episode = self.generate_episode(self.behavior_policy)
            self.episodes.append(episode)
            
            # è®¡ç®—å›æŠ¥
            # Compute returns
            returns = episode.compute_returns(self.gamma)
            
            # åå‘å¤„ç†å›åˆï¼ˆä¸ºäº†ç´¯ç§¯é‡è¦æ€§æ¯”ç‡ï¼‰
            # Process episode backward (to accumulate importance ratio)
            W = 1.0  # ç´¯ç§¯é‡è¦æ€§æ¯”ç‡
            
            for t in reversed(range(len(episode.experiences))):
                exp = episode.experiences[t]
                sa_pair = (exp.state.id, exp.action.id)
                G = returns[t]
                
                # æ›´æ–°ç´¯ç§¯åˆ†æ¯
                # Update cumulative denominator
                self.C[sa_pair] += W
                
                # åŠ æƒæ›´æ–°Q
                # Weighted Q update
                if self.C[sa_pair] > 0:
                    old_q = self.Q.get_value(exp.state, exp.action)
                    # åŠ æƒå¢é‡æ›´æ–°
                    # Weighted incremental update
                    new_q = old_q + (W / self.C[sa_pair]) * (G - old_q)
                    self.Q.set_value(exp.state, exp.action, new_q)
                
                # æ›´æ–°ç›®æ ‡ç­–ç•¥ï¼ˆè´ªå©ªï¼‰
                # Update target policy (greedy)
                self.target_policy = self.create_greedy_policy()
                
                # å¦‚æœåŠ¨ä½œä¸æ˜¯ç›®æ ‡ç­–ç•¥ä¼šé€‰çš„ï¼Œç»ˆæ­¢
                # If action is not what target would choose, terminate
                if isinstance(self.target_policy, DeterministicPolicy):
                    if exp.state in self.target_policy.policy_map:
                        target_action = self.target_policy.policy_map[exp.state]
                        if exp.action.id != target_action.id:
                            break  # é‡è¦æ€§æ¯”ç‡åç»­ä¸º0
                
                # æ›´æ–°Wï¼ˆé‡è¦æ€§æ¯”ç‡ï¼‰
                # Update W (importance ratio)
                behavior_probs = self.behavior_policy.get_action_probabilities(
                    exp.state, self.env.action_space
                )
                behavior_prob = behavior_probs.get(exp.action, 1e-10)
                
                W = W / behavior_prob  # ç›®æ ‡ç­–ç•¥æ˜¯ç¡®å®šæ€§çš„ï¼Œåˆ†å­æ˜¯1
                
                # è®°å½•æ¯”ç‡
                # Record ratio
                self.importance_ratios.append(W)
            
            # è®°å½•å­¦ä¹ è¿›åº¦
            # Record learning progress
            if episode.experiences:
                episode_return = returns[0] if returns else 0
                self.learning_curve.append(episode_return)
            
            # å®šæœŸè¾“å‡ºè¿›åº¦
            # Periodically output progress
            if verbose and (episode_num + 1) % 100 == 0:
                avg_return = np.mean(self.learning_curve[-100:]) if self.learning_curve else 0
                avg_ratio = np.mean(self.importance_ratios[-1000:]) if self.importance_ratios else 0
                
                print(f"  Episode {episode_num + 1}/{n_episodes}: "
                      f"å¹³å‡å›æŠ¥={avg_return:.2f}, "
                      f"å¹³å‡ISæ¯”ç‡={avg_ratio:.2f}")
        
        total_time = time.time() - start_time
        
        if verbose:
            print(f"\nå­¦ä¹ å®Œæˆ:")
            print(f"  æ€»æ—¶é—´: {total_time:.2f}ç§’")
            print(f"  è®¿é—®çš„(s,a)å¯¹: {len(self.sa_visits)}")
            print(f"  å¹³å‡ISæ¯”ç‡: {np.mean(self.importance_ratios):.2f}")
            print(f"  ISæ¯”ç‡æ ‡å‡†å·®: {np.std(self.importance_ratios):.2f}")
        
        # è¿”å›å­¦ä¹ åˆ°çš„ç›®æ ‡ç­–ç•¥
        # Return learned target policy
        return self.target_policy
    
    def analyze_importance_sampling(self):
        """
        åˆ†æé‡è¦æ€§é‡‡æ ·
        Analyze importance sampling
        
        å±•ç¤ºoff-policyçš„æŒ‘æˆ˜
        Show challenges of off-policy
        """
        print("\n" + "="*60)
        print("é‡è¦æ€§é‡‡æ ·åˆ†æ")
        print("Importance Sampling Analysis")
        print("="*60)
        
        if not self.importance_ratios:
            print("æ²¡æœ‰é‡è¦æ€§é‡‡æ ·æ•°æ®")
            return
        
        ratios = np.array(self.importance_ratios)
        
        print(f"\né‡è¦æ€§æ¯”ç‡ç»Ÿè®¡:")
        print(f"  æ ·æœ¬æ•°: {len(ratios)}")
        print(f"  å‡å€¼: {np.mean(ratios):.2f}")
        print(f"  æ ‡å‡†å·®: {np.std(ratios):.2f}")
        print(f"  æœ€å°å€¼: {np.min(ratios):.4f}")
        print(f"  æœ€å¤§å€¼: {np.max(ratios):.2f}")
        print(f"  ä¸­ä½æ•°: {np.median(ratios):.2f}")
        
        # åˆ†ææç«¯å€¼
        # Analyze extreme values
        extreme_threshold = 10.0
        extreme_count = np.sum(ratios > extreme_threshold)
        print(f"\næç«¯å€¼ (>{extreme_threshold}): {extreme_count} ({extreme_count/len(ratios):.1%})")
        
        # æœ‰æ•ˆæ ·æœ¬å¤§å°
        # Effective sample size
        if len(ratios) > 0:
            # ESS = (Î£w)Â² / Î£wÂ²
            sum_w = np.sum(ratios)
            sum_w2 = np.sum(ratios ** 2)
            if sum_w2 > 0:
                ess = (sum_w ** 2) / sum_w2
                print(f"\næœ‰æ•ˆæ ·æœ¬å¤§å° (ESS): {ess:.1f} / {len(ratios)}")
                print(f"æ•ˆç‡: {ess/len(ratios):.1%}")
        
        print("\n" + "="*40)
        print("é—®é¢˜è¯Šæ–­:")
        print("Problem Diagnosis:")
        print("="*40)
        
        if np.std(ratios) > np.mean(ratios):
            print("âš ï¸ é«˜æ–¹å·®é—®é¢˜ï¼šæ ‡å‡†å·® > å‡å€¼")
            print("   High variance: std > mean")
            print("   å»ºè®®ï¼šå‡å°è¡Œä¸ºç­–ç•¥çš„Îµ")
            print("   Suggestion: Reduce behavior policy Îµ")
        
        if extreme_count / len(ratios) > 0.1:
            print("âš ï¸ è¿‡å¤šæç«¯å€¼")
            print("   Too many extreme values")
            print("   å»ºè®®ï¼šä½¿ç”¨æ›´ç›¸ä¼¼çš„è¡Œä¸ºç­–ç•¥")
            print("   Suggestion: Use more similar behavior policy")
        
        print("""
        ç†è®ºèƒŒæ™¯ Theoretical Background:
        ================================
        
        é‡è¦æ€§é‡‡æ ·çš„æ–¹å·®ï¼š
        Variance of importance sampling:
        Var[ÏG] = E_b[(ÏG)Â²] - (E_b[ÏG])Â²
        
        å½“bå’ŒÏ€å·®å¼‚å¤§æ—¶ï¼š
        When b and Ï€ differ greatly:
        - Ïçš„æ–¹å·®çˆ†ç‚¸
          Variance of Ï explodes
        - ä¼°è®¡å˜å¾—ä¸å¯é 
          Estimates become unreliable
        - æ”¶æ•›ææ…¢
          Convergence extremely slow
        
        è§£å†³æ–¹æ¡ˆï¼š
        Solutions:
        1. åŠ æƒé‡è¦æ€§é‡‡æ ·ï¼ˆå·²ä½¿ç”¨ï¼‰
           Weighted importance sampling (already used)
        2. æˆªæ–­é‡è¦æ€§æ¯”ç‡
           Truncate importance ratios
        3. ä½¿ç”¨æ›´æ¥è¿‘çš„è¡Œä¸ºç­–ç•¥
           Use closer behavior policy
        """)


# ================================================================================
# ç¬¬4.3.5èŠ‚ï¼šæ¢ç´¢æ€§èµ·å§‹
# Section 4.3.5: Exploring Starts
# ================================================================================

class ExploringStarts:
    """
    æ¢ç´¢æ€§èµ·å§‹æ–¹æ³•
    Exploring Starts Method
    
    ä¿è¯æ¢ç´¢çš„å¦ä¸€ç§æ–¹å¼
    Another way to ensure exploration
    
    æ ¸å¿ƒæ€æƒ³ï¼š
    Core idea:
    - æ¯ä¸ªå›åˆä»éšæœºçš„(s,a)å¯¹å¼€å§‹
      Each episode starts from random (s,a) pair
    - ä¹‹åå¯ä»¥ç”¨ç¡®å®šæ€§ç­–ç•¥
      Can use deterministic policy afterwards
    - ä¿è¯æ‰€æœ‰(s,a)å¯¹è¢«æ— é™è®¿é—®
      Ensures all (s,a) pairs visited infinitely
    
    ç®—æ³•ï¼ˆMC ESï¼‰ï¼š
    Algorithm (MC ES):
    1. åˆå§‹åŒ–Q(s,a)å’ŒÏ€(s)ä»»æ„
       Initialize Q(s,a) and Ï€(s) arbitrarily
    2. é‡å¤ï¼š
       Repeat:
       a. é€‰æ‹©éšæœºSâ‚€âˆˆS, Aâ‚€âˆˆA(Sâ‚€)
          Choose random Sâ‚€âˆˆS, Aâ‚€âˆˆA(Sâ‚€)
       b. ä»Sâ‚€,Aâ‚€å¼€å§‹ç”Ÿæˆå›åˆ
          Generate episode starting from Sâ‚€,Aâ‚€
       c. æ›´æ–°Qä½¿ç”¨å›åˆ
          Update Q using episode
       d. å¯¹æ¯ä¸ªsï¼ŒÏ€(s) â† argmax_a Q(s,a)
          For each s, Ï€(s) â† argmax_a Q(s,a)
    
    ä¼˜åŠ¿ï¼š
    Advantages:
    - å¯ä»¥å­¦ä¹ ç¡®å®šæ€§æœ€ä¼˜ç­–ç•¥
      Can learn deterministic optimal policy
    - ä¸éœ€è¦Îµ-è´ªå©ªçš„æ¬¡ä¼˜æ€§
      No suboptimality of Îµ-greedy
    
    åŠ£åŠ¿ï¼š
    Disadvantages:
    - éœ€è¦èƒ½æŒ‡å®šèµ·å§‹çŠ¶æ€
      Need ability to specify starting state
    - å®è·µä¸­å¸¸å¸¸ä¸å¯è¡Œ
      Often infeasible in practice
    - ä¸é€‚ç”¨äºç»§ç»­æ€§ä»»åŠ¡
      Not applicable to continuing tasks
    
    è¿™æ˜¯ç†è®ºä¸Šä¼˜é›…ä½†å®è·µå—é™çš„æ–¹æ³•
    This is theoretically elegant but practically limited
    """
    
    @staticmethod
    def demonstrate(env: MDPEnvironment, 
                   n_episodes: int = 1000,
                   gamma: float = 1.0):
        """
        æ¼”ç¤ºæ¢ç´¢æ€§èµ·å§‹
        Demonstrate exploring starts
        """
        print("\n" + "="*60)
        print("æ¢ç´¢æ€§èµ·å§‹MCæ§åˆ¶ (MC ES)")
        print("Exploring Starts MC Control (MC ES)")
        print("="*60)
        
        # åˆå§‹åŒ–
        Q = ActionValueFunction(env.state_space, env.action_space, initial_value=0.0)
        sa_visits = defaultdict(int)
        
        # åˆ›å»ºåˆå§‹ç­–ç•¥ï¼ˆéšæœºï¼‰
        # Create initial policy (random)
        policy_map = {}
        for state in env.state_space:
            if not state.is_terminal:
                policy_map[state] = np.random.choice(env.action_space)
        
        policy = DeterministicPolicy(policy_map)
        
        print(f"è¿è¡Œ{n_episodes}ä¸ªå›åˆ...")
        
        for episode_num in range(n_episodes):
            # æ¢ç´¢æ€§èµ·å§‹ï¼šéšæœºé€‰æ‹©èµ·å§‹(s,a)
            # Exploring start: randomly choose starting (s,a)
            non_terminal_states = [s for s in env.state_space if not s.is_terminal]
            if not non_terminal_states:
                break
            
            start_state = np.random.choice(non_terminal_states)
            start_action = np.random.choice(env.action_space)
            
            # ç”Ÿæˆå›åˆ
            # Generate episode
            episode = Episode()
            
            # å¼ºåˆ¶ç¬¬ä¸€æ­¥
            # Force first step
            env.current_state = start_state
            next_state, reward, done, _ = env.step(start_action)
            
            exp = Experience(start_state, start_action, reward, next_state, done)
            episode.add_experience(exp)
            
            # ç»§ç»­å›åˆï¼ˆç”¨å½“å‰ç­–ç•¥ï¼‰
            # Continue episode (using current policy)
            state = next_state
            while not done:
                if state in policy.policy_map:
                    action = policy.policy_map[state]
                else:
                    action = np.random.choice(env.action_space)
                
                next_state, reward, done, _ = env.step(action)
                exp = Experience(state, action, reward, next_state, done)
                episode.add_experience(exp)
                state = next_state
            
            # æ›´æ–°Qï¼ˆfirst-visitï¼‰
            # Update Q (first-visit)
            returns = episode.compute_returns(gamma)
            sa_pairs_seen = set()
            
            for t, exp in enumerate(episode.experiences):
                sa_pair = (exp.state.id, exp.action.id)
                
                if sa_pair not in sa_pairs_seen:
                    sa_pairs_seen.add(sa_pair)
                    G = returns[t]
                    
                    sa_visits[sa_pair] += 1
                    n = sa_visits[sa_pair]
                    
                    old_q = Q.get_value(exp.state, exp.action)
                    new_q = old_q + (G - old_q) / n
                    Q.set_value(exp.state, exp.action, new_q)
            
            # æ”¹è¿›ç­–ç•¥ï¼ˆè´ªå©ªï¼‰
            # Improve policy (greedy)
            for state in env.state_space:
                if not state.is_terminal:
                    best_action = None
                    best_value = float('-inf')
                    
                    for action in env.action_space:
                        q_value = Q.get_value(state, action)
                        if q_value > best_value:
                            best_value = q_value
                            best_action = action
                    
                    if best_action:
                        policy.policy_map[state] = best_action
            
            if (episode_num + 1) % 100 == 0:
                print(f"  Episode {episode_num + 1}: è®¿é—®{len(sa_visits)}ä¸ª(s,a)å¯¹")
        
        print(f"\nç»“æœ:")
        print(f"  æ€»è®¿é—®(s,a)å¯¹: {len(sa_visits)}")
        print(f"  å¹³å‡è®¿é—®æ¬¡æ•°: {np.mean(list(sa_visits.values())):.1f}")
        
        # åˆ†æè¦†ç›–ç‡
        # Analyze coverage
        total_sa_pairs = sum(1 for s in env.state_space 
                           for a in env.action_space 
                           if not s.is_terminal)
        coverage = len(sa_visits) / total_sa_pairs if total_sa_pairs > 0 else 0
        
        print(f"  (s,a)å¯¹è¦†ç›–ç‡: {coverage:.1%}")
        
        print("\n" + "="*40)
        print("æ¢ç´¢æ€§èµ·å§‹çš„ç‰¹ç‚¹:")
        print("Characteristics of Exploring Starts:")
        print("="*40)
        print("""
        âœ“ ä¼˜ç‚¹ Advantages:
        ------------------
        1. ç†è®ºä¿è¯ï¼š
           Theoretical guarantee:
           æ‰€æœ‰(s,a)å¯¹éƒ½è¢«è®¿é—® â†’ æ”¶æ•›åˆ°æœ€ä¼˜
           All (s,a) pairs visited â†’ Converges to optimal
        
        2. æ— éœ€è½¯ç­–ç•¥ï¼š
           No need for soft policy:
           å¯ä»¥å­¦ä¹ ç¡®å®šæ€§æœ€ä¼˜ç­–ç•¥
           Can learn deterministic optimal policy
        
        3. ç®€å•æ¸…æ™°ï¼š
           Simple and clear:
           ç®—æ³•é€»è¾‘ç›´æ¥
           Algorithm logic is straightforward
        
        âœ— ç¼ºç‚¹ Disadvantages:
        ---------------------
        1. å®è·µé™åˆ¶ï¼š
           Practical limitations:
           å¾ˆå¤šç¯å¢ƒä¸èƒ½ä»»æ„è®¾ç½®èµ·å§‹çŠ¶æ€
           Many environments can't set arbitrary start state
        
        2. è¦†ç›–å›°éš¾ï¼š
           Coverage difficulty:
           çŠ¶æ€ç©ºé—´å¤§æ—¶éš¾ä»¥è¦†ç›–æ‰€æœ‰(s,a)
           Hard to cover all (s,a) when state space is large
        
        3. ä¸è‡ªç„¶ï¼š
           Unnatural:
           éšæœºèµ·å§‹å¯èƒ½ä¸ç¬¦åˆé—®é¢˜è®¾å®š
           Random starts may not fit problem setting
        
        å› æ­¤å®è·µä¸­æ›´å¸¸ç”¨Îµ-è´ªå©ªæˆ–off-policyæ–¹æ³•
        Therefore Îµ-greedy or off-policy more common in practice
        """)
        
        return policy, Q


# ================================================================================
# ç¬¬4.3.6èŠ‚ï¼šMCæ§åˆ¶å¯è§†åŒ–å™¨
# Section 4.3.6: MC Control Visualizer
# ================================================================================

class MCControlVisualizer:
    """
    MCæ§åˆ¶å¯è§†åŒ–å™¨
    MC Control Visualizer
    
    æä¾›ä¸°å¯Œçš„å¯è§†åŒ–æ¥ç†è§£MCæ§åˆ¶
    Provides rich visualizations to understand MC control
    """
    
    @staticmethod
    def plot_learning_curves(controllers: Dict[str, MCControl]):
        """
        ç»˜åˆ¶å­¦ä¹ æ›²çº¿æ¯”è¾ƒ
        Plot learning curves comparison
        """
        fig, axes = plt.subplots(2, 2, figsize=(14, 10))
        
        colors = {'On-Policy': 'blue', 'Off-Policy': 'red', 'MC ES': 'green'}
        
        # å›¾1ï¼šå›æŠ¥æ›²çº¿
        # Plot 1: Return curves
        ax1 = axes[0, 0]
        ax1.set_title('Learning Curves (Episode Returns)')
        ax1.set_xlabel('Episodes')
        ax1.set_ylabel('Episode Return')
        
        for name, controller in controllers.items():
            if controller.learning_curve:
                # å¹³æ»‘æ›²çº¿
                # Smooth curve
                window = 50
                if len(controller.learning_curve) >= window:
                    smoothed = np.convolve(controller.learning_curve, 
                                          np.ones(window)/window, 
                                          mode='valid')
                    x = np.arange(len(smoothed))
                    ax1.plot(x, smoothed, color=colors.get(name, 'gray'),
                           label=name, linewidth=2, alpha=0.7)
        
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # å›¾2ï¼šQå€¼æ¼”åŒ–
        # Plot 2: Q-value evolution
        ax2 = axes[0, 1]
        ax2.set_title('Q-value Statistics')
        
        for idx, (name, controller) in enumerate(controllers.items()):
            # è·å–æ‰€æœ‰Qå€¼
            # Get all Q values
            q_values = []
            for state in controller.env.state_space:
                if not state.is_terminal:
                    for action in controller.env.action_space:
                        q_values.append(controller.Q.get_value(state, action))
            
            if q_values:
                # ç®±çº¿å›¾
                # Box plot
                bp = ax2.boxplot(q_values, positions=[idx], widths=0.6,
                                patch_artist=True, labels=[name])
                bp['boxes'][0].set_facecolor(colors.get(name, 'gray'))
                bp['boxes'][0].set_alpha(0.5)
        
        ax2.set_ylabel('Q-values')
        ax2.grid(True, alpha=0.3, axis='y')
        
        # å›¾3ï¼šæ¢ç´¢ç»Ÿè®¡ï¼ˆä»…On-Policyï¼‰
        # Plot 3: Exploration statistics (On-Policy only)
        ax3 = axes[1, 0]
        ax3.set_title('Exploration vs Exploitation (On-Policy)')
        
        for name, controller in controllers.items():
            if hasattr(controller, 'policy') and isinstance(controller.policy, EpsilonGreedyPolicy):
                stats = controller.policy.get_exploration_stats()
                
                # é¥¼å›¾
                # Pie chart
                sizes = [stats['exploration_count'], stats['exploitation_count']]
                labels = ['Exploration', 'Exploitation']
                colors_pie = ['lightcoral', 'lightblue']
                
                if sum(sizes) > 0:
                    ax3.pie(sizes, labels=labels, colors=colors_pie,
                           autopct='%1.1f%%', startangle=90)
                    ax3.set_title(f'{name}: Îµ={controller.policy.epsilon:.3f}')
                    break  # åªæ˜¾ç¤ºä¸€ä¸ª
        
        # å›¾4ï¼šè®¿é—®é¢‘ç‡åˆ†å¸ƒ
        # Plot 4: Visit frequency distribution
        ax4 = axes[1, 1]
        ax4.set_title('State-Action Visit Frequencies')
        ax4.set_xlabel('Visit Count')
        ax4.set_ylabel('Number of (s,a) pairs')
        
        for name, controller in controllers.items():
            if controller.sa_visits:
                visits = list(controller.sa_visits.values())
                ax4.hist(visits, bins=30, alpha=0.5, 
                        label=name, color=colors.get(name, 'gray'))
        
        ax4.legend()
        ax4.grid(True, alpha=0.3)
        
        plt.suptitle('MC Control Methods Comparison', fontsize=14, fontweight='bold')
        plt.tight_layout()
        
        return fig
    
    @staticmethod
    def plot_policy_evolution(controller: MCControl, 
                            sample_states: Optional[List[State]] = None):
        """
        ç»˜åˆ¶ç­–ç•¥æ¼”åŒ–
        Plot policy evolution
        
        å±•ç¤ºç­–ç•¥å¦‚ä½•éšå­¦ä¹ æ”¹å˜
        Show how policy changes with learning
        """
        if not hasattr(controller, 'policy'):
            print("æ§åˆ¶å™¨æ²¡æœ‰ç­–ç•¥å±æ€§")
            return None
        
        policy = controller.policy
        
        # å¦‚æœæ²¡æŒ‡å®šï¼Œé€‰æ‹©ä¸€äº›çŠ¶æ€
        # If not specified, select some states
        if sample_states is None:
            non_terminal = [s for s in controller.env.state_space 
                          if not s.is_terminal]
            sample_states = non_terminal[:min(4, len(non_terminal))]
        
        if not sample_states:
            print("æ²¡æœ‰å¯æ˜¾ç¤ºçš„çŠ¶æ€")
            return None
        
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        axes = axes.flatten()
        
        for idx, state in enumerate(sample_states):
            if idx >= 4:
                break
            
            ax = axes[idx]
            
            # è·å–åŠ¨ä½œæ¦‚ç‡
            # Get action probabilities
            if isinstance(policy, StochasticPolicy):
                probs = policy.get_action_probabilities(state)
                
                # æ¡å½¢å›¾
                # Bar plot
                actions = list(probs.keys())
                probabilities = list(probs.values())
                
                x = np.arange(len(actions))
                bars = ax.bar(x, probabilities, alpha=0.7, color='steelblue')
                
                # é«˜äº®æœ€ä¼˜åŠ¨ä½œ
                # Highlight best action
                best_idx = np.argmax(probabilities)
                bars[best_idx].set_color('red')
                bars[best_idx].set_alpha(1.0)
                
                ax.set_xticks(x)
                ax.set_xticklabels([a.id for a in actions], rotation=45)
                ax.set_ylabel('Probability')
                ax.set_ylim([0, 1])
                ax.set_title(f'State: {state.id}')
                
                # æ·»åŠ Qå€¼ä½œä¸ºå‚è€ƒ
                # Add Q-values as reference
                q_values = [controller.Q.get_value(state, a) for a in actions]
                ax2 = ax.twinx()
                ax2.plot(x, q_values, 'go-', linewidth=2, markersize=8, alpha=0.5)
                ax2.set_ylabel('Q-value', color='g')
                ax2.tick_params(axis='y', labelcolor='g')
                
            elif isinstance(policy, DeterministicPolicy):
                # å¯¹äºç¡®å®šæ€§ç­–ç•¥ï¼Œæ˜¾ç¤ºQå€¼
                # For deterministic policy, show Q-values
                q_values = []
                action_labels = []
                
                for action in controller.env.action_space:
                    q_values.append(controller.Q.get_value(state, action))
                    action_labels.append(action.id)
                
                x = np.arange(len(q_values))
                bars = ax.bar(x, q_values, alpha=0.7, color='steelblue')
                
                # é«˜äº®é€‰æ‹©çš„åŠ¨ä½œ
                # Highlight selected action
                if state in policy.policy_map:
                    selected_action = policy.policy_map[state]
                    for i, action in enumerate(controller.env.action_space):
                        if action.id == selected_action.id:
                            bars[i].set_color('red')
                            bars[i].set_alpha(1.0)
                            break
                
                ax.set_xticks(x)
                ax.set_xticklabels(action_labels, rotation=45)
                ax.set_ylabel('Q-value')
                ax.set_title(f'State: {state.id}')
        
        plt.suptitle('Policy and Q-values by State', fontsize=14, fontweight='bold')
        plt.tight_layout()
        
        return fig
    
    @staticmethod
    def plot_importance_sampling_analysis(off_policy_controller: OffPolicyMCControl):
        """
        ç»˜åˆ¶é‡è¦æ€§é‡‡æ ·åˆ†æ
        Plot importance sampling analysis
        """
        if not off_policy_controller.importance_ratios:
            print("æ²¡æœ‰é‡è¦æ€§é‡‡æ ·æ•°æ®")
            return None
        
        fig, axes = plt.subplots(2, 2, figsize=(12, 8))
        
        ratios = np.array(off_policy_controller.importance_ratios)
        
        # å›¾1ï¼šæ¯”ç‡åˆ†å¸ƒ
        # Plot 1: Ratio distribution
        ax1 = axes[0, 0]
        ax1.hist(np.clip(ratios, 0, 20), bins=50, alpha=0.7, color='blue')
        ax1.set_xlabel('Importance Ratio (clipped at 20)')
        ax1.set_ylabel('Frequency')
        ax1.set_title('Distribution of Importance Ratios')
        ax1.axvline(x=1.0, color='red', linestyle='--', label='Ratio=1')
        ax1.legend()
        
        # å›¾2ï¼šæ¯”ç‡éšæ—¶é—´å˜åŒ–
        # Plot 2: Ratios over time
        ax2 = axes[0, 1]
        window = min(100, len(ratios) // 10)
        if len(ratios) >= window:
            smoothed = np.convolve(ratios, np.ones(window)/window, mode='valid')
            ax2.plot(smoothed, alpha=0.7, color='green')
            ax2.set_xlabel('Sample Index')
            ax2.set_ylabel('Importance Ratio (smoothed)')
            ax2.set_title(f'IS Ratios Over Time (window={window})')
            ax2.axhline(y=1.0, color='red', linestyle='--', alpha=0.5)
        
        # å›¾3ï¼šæœ‰æ•ˆæ ·æœ¬å¤§å°
        # Plot 3: Effective sample size
        ax3 = axes[1, 0]
        
        # è®¡ç®—ç´¯ç§¯ESS
        # Compute cumulative ESS
        ess_history = []
        for i in range(100, len(ratios), 100):
            batch = ratios[:i]
            sum_w = np.sum(batch)
            sum_w2 = np.sum(batch ** 2)
            if sum_w2 > 0:
                ess = (sum_w ** 2) / sum_w2
                ess_history.append(ess / i)  # å½’ä¸€åŒ–
        
        if ess_history:
            ax3.plot(np.arange(100, len(ratios), 100), ess_history, 'bo-')
            ax3.set_xlabel('Number of Samples')
            ax3.set_ylabel('ESS / n (Efficiency)')
            ax3.set_title('Effective Sample Size Efficiency')
            ax3.axhline(y=1.0, color='red', linestyle='--', label='Perfect efficiency')
            ax3.axhline(y=0.5, color='orange', linestyle='--', label='50% efficiency')
            ax3.legend()
            ax3.grid(True, alpha=0.3)
        
        # å›¾4ï¼šæç«¯å€¼åˆ†æ
        # Plot 4: Extreme values analysis
        ax4 = axes[1, 1]
        
        thresholds = [1, 2, 5, 10, 20, 50]
        extreme_counts = [np.sum(ratios > t) / len(ratios) * 100 for t in thresholds]
        
        ax4.bar(range(len(thresholds)), extreme_counts, alpha=0.7, color='coral')
        ax4.set_xticks(range(len(thresholds)))
        ax4.set_xticklabels([str(t) for t in thresholds])
        ax4.set_xlabel('Threshold')
        ax4.set_ylabel('Percentage of Samples (%)')
        ax4.set_title('Percentage of Samples Exceeding Threshold')
        ax4.grid(True, alpha=0.3, axis='y')
        
        plt.suptitle('Importance Sampling Analysis', fontsize=14, fontweight='bold')
        plt.tight_layout()
        
        return fig


# ================================================================================
# ç¬¬4.3.7èŠ‚ï¼šMCæ§åˆ¶ç»¼åˆæ¼”ç¤º
# Section 4.3.7: MC Control Comprehensive Demo
# ================================================================================

def demonstrate_mc_control():
    """
    ç»¼åˆæ¼”ç¤ºMCæ§åˆ¶æ–¹æ³•
    Comprehensive demonstration of MC control methods
    """
    print("\n" + "="*80)
    print("è’™ç‰¹å¡æ´›æ§åˆ¶æ–¹æ³•ç»¼åˆæ¼”ç¤º")
    print("Monte Carlo Control Methods Comprehensive Demo")
    print("="*80)
    
    # åˆ›å»ºæµ‹è¯•ç¯å¢ƒ
    # Create test environment
    from ch02_mdp.gridworld import GridWorld
    
    env = GridWorld(rows=4, cols=4,
                   start_pos=(0,0),
                   goal_pos=(3,3),
                   obstacles={(1,1), (2,2)})
    
    print(f"\næµ‹è¯•ç¯å¢ƒ: {env.name}")
    print(f"  çŠ¶æ€æ•°: {len(env.state_space)}")
    print(f"  åŠ¨ä½œæ•°: {len(env.action_space)}")
    print(f"  èµ·ç‚¹: (0,0), ç»ˆç‚¹: (3,3)")
    print(f"  éšœç¢ç‰©: (1,1), (2,2)")
    
    # è®­ç»ƒå›åˆæ•°
    # Number of training episodes
    n_episodes = 500
    
    # 1. On-Policy MCæ§åˆ¶
    # 1. On-Policy MC Control
    print("\n" + "="*60)
    print("1. On-Policy MCæ§åˆ¶")
    on_policy = OnPolicyMCControl(
        env, gamma=0.9,
        epsilon=0.2,
        epsilon_decay=0.995,
        epsilon_min=0.01
    )
    
    on_policy_result = on_policy.learn(n_episodes, verbose=True)
    on_policy.analyze_learning()
    on_policy.demonstrate_gpi()
    on_policy.policy.analyze_exploration_pattern()
    
    # 2. Off-Policy MCæ§åˆ¶
    # 2. Off-Policy MC Control
    print("\n" + "="*60)
    print("2. Off-Policy MCæ§åˆ¶")
    off_policy = OffPolicyMCControl(
        env, gamma=0.9,
        behavior_epsilon=0.3
    )
    
    off_policy_result = off_policy.learn(n_episodes, verbose=True)
    off_policy.analyze_learning()
    off_policy.analyze_importance_sampling()
    
    # 3. æ¢ç´¢æ€§èµ·å§‹
    # 3. Exploring Starts
    print("\n" + "="*60)
    print("3. æ¢ç´¢æ€§èµ·å§‹")
    es_policy, es_Q = ExploringStarts.demonstrate(env, n_episodes=500, gamma=0.9)
    
    # æ¯”è¾ƒç»“æœ
    # Compare results
    print("\n" + "="*80)
    print("æ–¹æ³•æ¯”è¾ƒ")
    print("Method Comparison")
    print("="*80)
    
    # è¯„ä¼°æœ€ç»ˆç­–ç•¥
    # Evaluate final policies
    print("\næœ€ç»ˆç­–ç•¥è¯„ä¼° (100å›åˆå¹³å‡):")
    print("Final Policy Evaluation (100 episode average):")
    
    on_return = on_policy.evaluate_policy(on_policy_result, 100)
    off_return = off_policy.evaluate_policy(off_policy_result, 100)
    
    # ä¸ºESåˆ›å»ºä¸´æ—¶æ§åˆ¶å™¨æ¥è¯„ä¼°
    # Create temporary controller for ES evaluation
    es_controller = MCControl(env, gamma=0.9)
    es_return = es_controller.evaluate_policy(es_policy, 100)
    
    print(f"  On-Policy: {on_return:.2f}")
    print(f"  Off-Policy: {off_return:.2f}")
    print(f"  Exploring Starts: {es_return:.2f}")
    
    # åˆ†æè¦†ç›–ç‡
    # Analyze coverage
    print("\nçŠ¶æ€-åŠ¨ä½œå¯¹è¦†ç›–ç‡:")
    print("State-Action Pair Coverage:")
    
    total_sa = sum(1 for s in env.state_space 
                  for a in env.action_space 
                  if not s.is_terminal)
    
    on_coverage = len(on_policy.sa_visits) / total_sa * 100
    off_coverage = len(off_policy.sa_visits) / total_sa * 100
    
    print(f"  On-Policy: {on_coverage:.1f}%")
    print(f"  Off-Policy: {off_coverage:.1f}%")
    
    # å¯è§†åŒ–
    # Visualization
    print("\nç”Ÿæˆå¯è§†åŒ–...")
    
    controllers = {
        'On-Policy': on_policy,
        'Off-Policy': off_policy
    }
    
    # å­¦ä¹ æ›²çº¿æ¯”è¾ƒ
    # Learning curves comparison
    fig1 = MCControlVisualizer.plot_learning_curves(controllers)
    
    # ç­–ç•¥å¯è§†åŒ–
    # Policy visualization
    fig2 = MCControlVisualizer.plot_policy_evolution(on_policy)
    
    # é‡è¦æ€§é‡‡æ ·åˆ†æ
    # Importance sampling analysis
    fig3 = MCControlVisualizer.plot_importance_sampling_analysis(off_policy)
    
    # æ€»ç»“
    # Summary
    print("\n" + "="*80)
    print("å…³é”®è¦ç‚¹")
    print("Key Takeaways")
    print("="*80)
    print("""
    1. On-Policy MCæ§åˆ¶:
       - ç®€å•ç›´æ¥ï¼Œå®¹æ˜“å®ç°
         Simple and straightforward
       - Îµ-è´ªå©ªå¹³è¡¡æ¢ç´¢å’Œåˆ©ç”¨
         Îµ-greedy balances exploration and exploitation
       - æ”¶æ•›åˆ°Îµ-è½¯æœ€ä¼˜ç­–ç•¥
         Converges to Îµ-soft optimal policy
    
    2. Off-Policy MCæ§åˆ¶:
       - å¯ä»¥å­¦ä¹ æœ€ä¼˜ç¡®å®šæ€§ç­–ç•¥
         Can learn optimal deterministic policy
       - é‡è¦æ€§é‡‡æ ·å¸¦æ¥é«˜æ–¹å·®
         Importance sampling brings high variance
       - æ•°æ®æ•ˆç‡å¯èƒ½æ›´é«˜
         May be more data efficient
    
    3. æ¢ç´¢æ€§èµ·å§‹:
       - ç†è®ºä¼˜é›…ä½†å®è·µå—é™
         Theoretically elegant but practically limited
       - ä¿è¯æ‰€æœ‰(s,a)å¯¹è¢«è®¿é—®
         Ensures all (s,a) pairs are visited
       - éœ€è¦ç¯å¢ƒæ”¯æŒä»»æ„èµ·å§‹
         Needs environment support for arbitrary starts
    
    4. å…±åŒç‰¹ç‚¹:
       - éƒ½æ˜¯æ— æ¨¡å‹æ–¹æ³•
         All are model-free methods
       - éƒ½éœ€è¦å®Œæ•´å›åˆ
         All need complete episodes
       - éƒ½åŸºäºGPIæ¡†æ¶
         All based on GPI framework
    
    5. å‘TDæ–¹æ³•çš„è¿‡æ¸¡:
       - MCçš„é«˜æ–¹å·®æ¿€å‘äº†TDæ–¹æ³•
         High variance of MC motivated TD methods
       - TDç»“åˆäº†MCå’ŒDPçš„ä¼˜ç‚¹
         TD combines advantages of MC and DP
       - ä¸‹ä¸€æ­¥ï¼šå­¦ä¹ TDæ§åˆ¶ï¼ˆQ-learning, SARSAï¼‰
         Next: Learn TD control (Q-learning, SARSA)
    """)
    print("="*80)
    
    plt.show()


# ================================================================================
# ä¸»å‡½æ•°
# Main Function  
# ================================================================================

def main():
    """
    è¿è¡ŒMCæ§åˆ¶æ¼”ç¤º
    Run MC Control Demo
    """
    demonstrate_mc_control()


if __name__ == "__main__":
    main()