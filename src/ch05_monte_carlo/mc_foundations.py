"""
================================================================================
ç¬¬4.1èŠ‚ï¼šè’™ç‰¹å¡æ´›æ–¹æ³•åŸºç¡€ - ä»ç»éªŒä¸­å­¦ä¹ 
Section 4.1: Monte Carlo Foundations - Learning from Experience
================================================================================

è’™ç‰¹å¡æ´›æ–¹æ³•æ˜¯å¼ºåŒ–å­¦ä¹ çš„é‡è¦è½¬æŠ˜ç‚¹ï¼šä»éœ€è¦æ¨¡å‹åˆ°ä¸éœ€è¦æ¨¡å‹ã€‚
Monte Carlo methods are a turning point in RL: from model-based to model-free.

æ ¸å¿ƒæ´å¯Ÿï¼šæˆ‘ä»¬å¯ä»¥é€šè¿‡å¹³å‡å®é™…å›æŠ¥æ¥ä¼°è®¡æœŸæœ›å›æŠ¥
Core insight: We can estimate expected return by averaging actual returns

è¿™åŸºäºå¤§æ•°å®šå¾‹ï¼š
This is based on the Law of Large Numbers:
éšç€æ ·æœ¬å¢åŠ ï¼Œæ ·æœ¬å‡å€¼æ”¶æ•›åˆ°æœŸæœ›å€¼
As samples increase, sample mean converges to expected value

æ¯”å–»ï¼šå°±åƒé€šè¿‡å¤šæ¬¡æŠ•æ·æ¥ä¼°è®¡ç¡¬å¸æ­£é¢çš„æ¦‚ç‡
Analogy: Like estimating coin's probability by many tosses

MCçš„ä¼˜åŠ¿ï¼š
Advantages of MC:
1. ä¸éœ€è¦ç¯å¢ƒæ¨¡å‹ï¼ˆåªéœ€è¦èƒ½é‡‡æ ·ï¼‰
   No need for environment model (only need sampling)
2. å¯ä»¥ä»å®é™…æˆ–æ¨¡æ‹Ÿç»éªŒä¸­å­¦ä¹ 
   Can learn from actual or simulated experience
3. ä¸å—é©¬å°”å¯å¤«æ€§é™åˆ¶
   Not restricted by Markov property
4. å¯ä»¥ä¸“æ³¨äºæ„Ÿå…´è¶£çš„çŠ¶æ€å­é›†
   Can focus on subset of states of interest

MCçš„åŠ£åŠ¿ï¼š
Disadvantages of MC:
1. åªé€‚ç”¨äºå›åˆå¼ä»»åŠ¡
   Only works for episodic tasks
2. éœ€è¦ç­‰åˆ°å›åˆç»“æŸæ‰èƒ½æ›´æ–°
   Must wait until episode ends to update
3. é«˜æ–¹å·®ï¼ˆä½†æ— åï¼‰
   High variance (but unbiased)
4. æ”¶æ•›å¯èƒ½å¾ˆæ…¢
   Convergence can be slow
"""

import numpy as np
from typing import List, Tuple, Dict, Optional, Any, Union, Callable
from dataclasses import dataclass, field
from collections import defaultdict, deque
import logging
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
import time

# å¯¼å…¥åŸºç¡€ç»„ä»¶
import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent))

from src.ch03_finite_mdp.mdp_framework import State, Action, MDPEnvironment
from src.ch03_finite_mdp.policies_and_values import (
    Policy, StateValueFunction, ActionValueFunction
)

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


# ================================================================================
# ç¬¬4.1.1èŠ‚ï¼šå›åˆä¸å›æŠ¥
# Section 4.1.1: Episodes and Returns
# ================================================================================

@dataclass
class Experience:
    """
    å•æ­¥ç»éªŒ
    Single-step experience
    
    è¿™æ˜¯MCæ–¹æ³•çš„åŸå­å•ä½
    This is the atomic unit of MC methods
    """
    state: State
    action: Action
    reward: float
    next_state: State
    done: bool
    
    def __repr__(self):
        return f"({self.state.id}, {self.action.id}, {self.reward:.1f})"


@dataclass
class Episode:
    """
    å®Œæ•´çš„å›åˆï¼ˆè½¨è¿¹ï¼‰
    Complete episode (trajectory)
    
    MCçš„å…³é”®ï¼šéœ€è¦å®Œæ•´çš„å›åˆæ¥è®¡ç®—å›æŠ¥
    Key to MC: Need complete episodes to compute returns
    
    ä¸€ä¸ªå›åˆæ˜¯ä»åˆå§‹çŠ¶æ€åˆ°ç»ˆæ­¢çŠ¶æ€çš„å®Œæ•´åºåˆ—ï¼š
    An episode is a complete sequence from initial to terminal state:
    Sâ‚€, Aâ‚€, Râ‚, Sâ‚, Aâ‚, Râ‚‚, ..., Sâ‚œ (terminal)
    
    æ•°å­¦è¡¨ç¤ºï¼š
    Mathematical representation:
    Ï„ = (Sâ‚€, Aâ‚€, Râ‚, Sâ‚, ..., Sâ‚œ)
    
    ä¸ºä»€ä¹ˆéœ€è¦å®Œæ•´å›åˆï¼Ÿ
    Why need complete episodes?
    å› ä¸ºéœ€è¦çŸ¥é“æœªæ¥æ‰€æœ‰å¥–åŠ±æ‰èƒ½è®¡ç®—å‡†ç¡®çš„å›æŠ¥
    Because we need all future rewards to compute accurate returns
    """
    experiences: List[Experience] = field(default_factory=list)
    
    def add_experience(self, exp: Experience):
        """
        æ·»åŠ ä¸€æ­¥ç»éªŒ
        Add one-step experience
        """
        self.experiences.append(exp)
    
    def length(self) -> int:
        """
        å›åˆé•¿åº¦
        Episode length
        """
        return len(self.experiences)
    
    def is_complete(self) -> bool:
        """
        æ£€æŸ¥å›åˆæ˜¯å¦ç»“æŸ
        Check if episode is complete
        """
        if not self.experiences:
            return False
        return self.experiences[-1].done
    
    def get_states(self) -> List[State]:
        """
        è·å–æ‰€æœ‰è®¿é—®çš„çŠ¶æ€
        Get all visited states
        """
        states = [exp.state for exp in self.experiences]
        if self.experiences and not self.experiences[-1].done:
            states.append(self.experiences[-1].next_state)
        return states
    
    def get_state_action_pairs(self) -> List[Tuple[State, Action]]:
        """
        è·å–æ‰€æœ‰(çŠ¶æ€,åŠ¨ä½œ)å¯¹
        Get all (state, action) pairs
        
        è¿™å¯¹äºQå‡½æ•°ä¼°è®¡å¾ˆé‡è¦
        This is important for Q-function estimation
        """
        return [(exp.state, exp.action) for exp in self.experiences]
    
    def compute_returns(self, gamma: float = 1.0) -> List[float]:
        """
        è®¡ç®—æ¯ä¸€æ­¥çš„å›æŠ¥
        Compute return for each step
        
        å›æŠ¥å®šä¹‰ï¼š
        Return definition:
        G_t = R_{t+1} + Î³R_{t+2} + Î³Â²R_{t+3} + ... = Î£_{k=0}^{T-t-1} Î³^k R_{t+k+1}
        
        è¿™æ˜¯MCçš„æ ¸å¿ƒè®¡ç®—ï¼
        This is the core computation of MC!
        
        Args:
            gamma: æŠ˜æ‰£å› å­
                  Discount factor
        
        Returns:
            æ¯ä¸€æ­¥çš„å›æŠ¥åˆ—è¡¨
            List of returns for each step
        
        ç®—æ³•ï¼šåå‘è®¡ç®—æ›´é«˜æ•ˆ
        Algorithm: Backward computation is more efficient
        G_{T-1} = R_T
        G_{t} = R_{t+1} + Î³G_{t+1}
        """
        if not self.is_complete():
            logger.warning("è®¡ç®—æœªå®Œæˆå›åˆçš„å›æŠ¥")
        
        returns = []
        G = 0  # åˆå§‹åŒ–ä¸º0ï¼ˆç»ˆæ­¢çŠ¶æ€ä¹‹åï¼‰
        
        # åå‘éå†è®¡ç®—å›æŠ¥
        for exp in reversed(self.experiences):
            G = exp.reward + gamma * G
            returns.append(G)
        
        # åè½¬å¾—åˆ°æ­£ç¡®é¡ºåº
        returns.reverse()
        
        return returns
    
    def first_visit_indices(self, state: State) -> List[int]:
        """
        æ‰¾åˆ°çŠ¶æ€çš„é¦–æ¬¡è®¿é—®ç´¢å¼•
        Find first-visit indices for a state
        
        First-visit MCåªä½¿ç”¨æ¯ä¸ªçŠ¶æ€çš„ç¬¬ä¸€æ¬¡è®¿é—®
        First-visit MC only uses first visit to each state
        """
        indices = []
        visited = set()
        
        for i, exp in enumerate(self.experiences):
            if exp.state.id not in visited:
                if exp.state.id == state.id:
                    indices.append(i)
                visited.add(exp.state.id)
        
        return indices
    
    def every_visit_indices(self, state: State) -> List[int]:
        """
        æ‰¾åˆ°çŠ¶æ€çš„æ‰€æœ‰è®¿é—®ç´¢å¼•
        Find all visit indices for a state
        
        Every-visit MCä½¿ç”¨æ¯ä¸ªçŠ¶æ€çš„æ‰€æœ‰è®¿é—®
        Every-visit MC uses all visits to each state
        """
        indices = []
        for i, exp in enumerate(self.experiences):
            if exp.state.id == state.id:
                indices.append(i)
        return indices


class Return:
    """
    å›æŠ¥ç»Ÿè®¡
    Return Statistics
    
    ç®¡ç†å’Œæ›´æ–°å›æŠ¥çš„ç»Ÿè®¡ä¿¡æ¯
    Manage and update return statistics
    
    è¿™æ˜¯å®ç°å¢é‡MCæ›´æ–°çš„å…³é”®
    This is key to implementing incremental MC updates
    """
    
    def __init__(self):
        """
        åˆå§‹åŒ–å›æŠ¥ç»Ÿè®¡
        Initialize return statistics
        """
        self.returns = []  # æ‰€æœ‰è§‚å¯Ÿåˆ°çš„å›æŠ¥
        self.count = 0     # è§‚å¯Ÿæ¬¡æ•°
        self.mean = 0.0    # å¹³å‡å›æŠ¥
        self.variance = 0.0  # æ–¹å·®
        self.std = 0.0     # æ ‡å‡†å·®
    
    def add_return(self, G: float):
        """
        æ·»åŠ ä¸€ä¸ªå›æŠ¥è§‚å¯Ÿ
        Add a return observation
        
        ä½¿ç”¨å¢é‡æ›´æ–°å…¬å¼ï¼š
        Using incremental update formula:
        Î¼_n = Î¼_{n-1} + (1/n)(G_n - Î¼_{n-1})
        
        è¿™é¿å…äº†å­˜å‚¨æ‰€æœ‰å›æŠ¥
        This avoids storing all returns
        """
        self.returns.append(G)
        self.count += 1
        
        # å¢é‡æ›´æ–°å‡å€¼
        old_mean = self.mean
        self.mean += (G - self.mean) / self.count
        
        # å¢é‡æ›´æ–°æ–¹å·®ï¼ˆWelford's algorithmï¼‰
        if self.count > 1:
            self.variance += (G - old_mean) * (G - self.mean)
            self.std = np.sqrt(self.variance / (self.count - 1))
    
    def get_statistics(self) -> Dict[str, float]:
        """
        è·å–ç»Ÿè®¡ä¿¡æ¯
        Get statistics
        """
        return {
            'mean': self.mean,
            'std': self.std,
            'count': self.count,
            'min': min(self.returns) if self.returns else 0,
            'max': max(self.returns) if self.returns else 0
        }
    
    def confidence_interval(self, confidence: float = 0.95) -> Tuple[float, float]:
        """
        è®¡ç®—ç½®ä¿¡åŒºé—´
        Compute confidence interval
        
        åŸºäºä¸­å¿ƒæé™å®šç†
        Based on Central Limit Theorem
        
        Args:
            confidence: ç½®ä¿¡æ°´å¹³
                       Confidence level
        
        Returns:
            (ä¸‹ç•Œ, ä¸Šç•Œ)
            (lower bound, upper bound)
        """
        if self.count < 2:
            return (self.mean, self.mean)
        
        # tåˆ†å¸ƒçš„ä¸´ç•Œå€¼
        alpha = 1 - confidence
        df = self.count - 1
        t_critical = stats.t.ppf(1 - alpha/2, df)
        
        # æ ‡å‡†è¯¯å·®
        se = self.std / np.sqrt(self.count)
        
        # ç½®ä¿¡åŒºé—´
        margin = t_critical * se
        return (self.mean - margin, self.mean + margin)


# ================================================================================
# ç¬¬4.1.2èŠ‚ï¼šå¤§æ•°å®šå¾‹ä¸MCæ”¶æ•›
# Section 4.1.2: Law of Large Numbers and MC Convergence
# ================================================================================

class LawOfLargeNumbers:
    """
    å¤§æ•°å®šå¾‹æ¼”ç¤º
    Law of Large Numbers Demonstration
    
    è¿™æ˜¯MCæ–¹æ³•çš„ç†è®ºåŸºç¡€
    This is the theoretical foundation of MC methods
    
    å¤§æ•°å®šå¾‹è¯´ï¼š
    Law of Large Numbers states:
    lim_{nâ†’âˆ} (1/n)Î£áµ¢ Xáµ¢ = E[X]  (å‡ ä¹å¿…ç„¶)
    
    å¯¹äºMCï¼š
    For MC:
    v_Ï€(s) = E[G_t | S_t = s] â‰ˆ (1/n)Î£áµ¢ G_i(s)
    
    å…³é”®æ€§è´¨ï¼š
    Key properties:
    1. æ— åæ€§ï¼šE[estimate] = true_value
       Unbiasedness: E[estimate] = true_value
    2. ä¸€è‡´æ€§ï¼šéšnå¢åŠ ï¼Œestimate â†’ true_value
       Consistency: As n increases, estimate â†’ true_value
    3. æ”¶æ•›é€Ÿåº¦ï¼šO(1/âˆšn)
       Convergence rate: O(1/âˆšn)
    """
    
    @staticmethod
    def demonstrate_convergence(true_value: float = 5.0,
                              std_dev: float = 2.0,
                              n_samples: int = 1000):
        """
        æ¼”ç¤ºå¤§æ•°å®šå¾‹æ”¶æ•›
        Demonstrate Law of Large Numbers convergence
        
        é€šè¿‡æ¨¡æ‹Ÿå±•ç¤ºæ ·æœ¬å‡å€¼å¦‚ä½•æ”¶æ•›åˆ°çœŸå®æœŸæœ›
        Show how sample mean converges to true expectation through simulation
        
        Args:
            true_value: çœŸå®æœŸæœ›å€¼
                       True expected value
            std_dev: æ ‡å‡†å·®
                    Standard deviation
            n_samples: æ ·æœ¬æ•°é‡
                      Number of samples
        """
        print("\n" + "="*60)
        print("å¤§æ•°å®šå¾‹æ¼”ç¤º")
        print("Law of Large Numbers Demonstration")
        print("="*60)
        
        # ç”Ÿæˆæ ·æœ¬
        np.random.seed(42)
        samples = np.random.normal(true_value, std_dev, n_samples)
        
        # è®¡ç®—ç´¯ç§¯å¹³å‡
        cumulative_means = np.cumsum(samples) / np.arange(1, n_samples + 1)
        
        # è®¡ç®—æ ‡å‡†è¯¯å·®
        standard_errors = std_dev / np.sqrt(np.arange(1, n_samples + 1))
        
        # å¯è§†åŒ–
        fig, axes = plt.subplots(2, 2, figsize=(12, 8))
        
        # å›¾1ï¼šæ”¶æ•›è¿‡ç¨‹
        ax1 = axes[0, 0]
        ax1.plot(cumulative_means, 'b-', alpha=0.7, label='Sample Mean')
        ax1.axhline(y=true_value, color='r', linestyle='--', label=f'True Value = {true_value}')
        
        # æ·»åŠ ç½®ä¿¡å¸¦
        confidence_band = 1.96 * standard_errors  # 95% confidence
        ax1.fill_between(range(n_samples), 
                         true_value - confidence_band,
                         true_value + confidence_band,
                         alpha=0.2, color='gray', label='95% CI')
        
        ax1.set_xlabel('Number of Samples')
        ax1.set_ylabel('Estimate')
        ax1.set_title('Convergence of Sample Mean')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # å›¾2ï¼šè¯¯å·®å‡å°
        ax2 = axes[0, 1]
        errors = np.abs(cumulative_means - true_value)
        ax2.loglog(range(1, n_samples + 1), errors, 'b-', alpha=0.7, label='|Error|')
        
        # ç†è®ºè¯¯å·®ç•Œé™ O(1/âˆšn)
        theoretical_bound = 3 * std_dev / np.sqrt(np.arange(1, n_samples + 1))
        ax2.loglog(range(1, n_samples + 1), theoretical_bound, 'r--', 
                  label='O(1/âˆšn) bound')
        
        ax2.set_xlabel('Number of Samples (log)')
        ax2.set_ylabel('Absolute Error (log)')
        ax2.set_title('Error Decay Rate')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # å›¾3ï¼šæ–¹å·®å‡å°
        ax3 = axes[1, 0]
        
        # è®¡ç®—æ»‘åŠ¨çª—å£æ–¹å·®
        window_size = 50
        variances = []
        for i in range(window_size, n_samples):
            window_samples = samples[i-window_size:i]
            variances.append(np.var(window_samples))
        
        ax3.plot(range(window_size, n_samples), variances, 'g-', alpha=0.7)
        ax3.axhline(y=std_dev**2, color='r', linestyle='--', 
                   label=f'True Variance = {std_dev**2:.1f}')
        ax3.set_xlabel('Sample Index')
        ax3.set_ylabel('Variance Estimate')
        ax3.set_title('Variance Estimation')
        ax3.legend()
        ax3.grid(True, alpha=0.3)
        
        # å›¾4ï¼šç›´æ–¹å›¾
        ax4 = axes[1, 1]
        
        # å¤šä¸ªé˜¶æ®µçš„ä¼°è®¡åˆ†å¸ƒ
        stages = [10, 100, 1000]
        colors = ['red', 'blue', 'green']
        
        for stage, color in zip(stages, colors):
            if stage <= n_samples:
                # å¤šæ¬¡è¿è¡Œè·å¾—ä¼°è®¡çš„åˆ†å¸ƒ
                estimates = []
                for _ in range(1000):
                    stage_samples = np.random.normal(true_value, std_dev, stage)
                    estimates.append(np.mean(stage_samples))
                
                ax4.hist(estimates, bins=30, alpha=0.3, color=color, 
                        label=f'n={stage}', density=True)
        
        ax4.axvline(x=true_value, color='r', linestyle='--', linewidth=2)
        ax4.set_xlabel('Estimate Value')
        ax4.set_ylabel('Density')
        ax4.set_title('Distribution of Estimates')
        ax4.legend()
        
        plt.suptitle('Law of Large Numbers in Monte Carlo Methods', fontsize=14, fontweight='bold')
        plt.tight_layout()
        
        # æ‰“å°ç»Ÿè®¡
        print(f"\nç»Ÿè®¡ç»“æœ (n={n_samples}):")
        print(f"  çœŸå®å€¼: {true_value:.3f}")
        print(f"  æœ€ç»ˆä¼°è®¡: {cumulative_means[-1]:.3f}")
        print(f"  æœ€ç»ˆè¯¯å·®: {errors[-1]:.3f}")
        print(f"  ç†è®ºæ ‡å‡†è¯¯å·®: {standard_errors[-1]:.3f}")
        
        # æ”¶æ•›é€Ÿåº¦åˆ†æ
        convergence_point = None
        tolerance = 0.1
        for i, error in enumerate(errors):
            if error < tolerance:
                convergence_point = i + 1
                break
        
        if convergence_point:
            print(f"  æ”¶æ•›åˆ°Â±{tolerance}éœ€è¦: {convergence_point}ä¸ªæ ·æœ¬")
        
        return fig
    
    @staticmethod
    def analyze_bias_variance_tradeoff():
        """
        åˆ†æåå·®-æ–¹å·®æƒè¡¡
        Analyze Bias-Variance Tradeoff
        
        MCä¼°è®¡çš„å…³é”®ç‰¹æ€§ï¼š
        Key properties of MC estimates:
        - æ— åï¼šE[estimate] = true_value
          Unbiased: E[estimate] = true_value
        - é«˜æ–¹å·®ï¼šVar[estimate] = ÏƒÂ²/n
          High variance: Var[estimate] = ÏƒÂ²/n
        
        ä¸TDæ–¹æ³•å¯¹æ¯”ï¼ˆåç»­ç« èŠ‚ï¼‰ï¼š
        Contrast with TD methods (later chapters):
        - TDæœ‰åä½†ä½æ–¹å·®
          TD is biased but low variance
        - MCæ— åä½†é«˜æ–¹å·®
          MC is unbiased but high variance
        """
        print("\n" + "="*60)
        print("MCä¼°è®¡çš„åå·®-æ–¹å·®åˆ†æ")
        print("Bias-Variance Analysis of MC Estimates")
        print("="*60)
        
        print("""
        ğŸ“Š MCä¼°è®¡çš„æ•°å­¦æ€§è´¨
        Mathematical Properties of MC Estimates
        ================================
        
        1. æ— åæ€§ Unbiasedness:
           E[Äœ] = E[G] = v_Ï€(s)
           
           è¯æ˜ï¼š
           Proof:
           MCä¼°è®¡æ˜¯å›æŠ¥çš„æ ·æœ¬å¹³å‡
           MC estimate is sample average of returns
           E[(1/n)Î£ G_i] = (1/n)Î£ E[G_i] = E[G] = v_Ï€(s)
        
        2. æ–¹å·® Variance:
           Var[Äœ] = Var[G]/n = ÏƒÂ²/n
           
           å«ä¹‰ï¼š
           Implication:
           - æ–¹å·®éšæ ·æœ¬æ•°çº¿æ€§å‡å°
             Variance decreases linearly with samples
           - æ ‡å‡†è¯¯å·®æŒ‰âˆšnå‡å°
             Standard error decreases as âˆšn
        
        3. å‡æ–¹è¯¯å·® Mean Squared Error:
           MSE = BiasÂ² + Variance = 0 + ÏƒÂ²/n = ÏƒÂ²/n
           
           å› ä¸ºMCæ— åï¼ŒMSEå®Œå…¨ç”±æ–¹å·®å†³å®š
           Since MC is unbiased, MSE is entirely variance
        
        4. æ”¶æ•›é€Ÿåº¦ Convergence Rate:
           P(|Äœ - v_Ï€(s)| > Îµ) â‰¤ 2exp(-2nÎµÂ²/BÂ²)
           
           è¿™æ˜¯Hoeffdingä¸ç­‰å¼
           This is Hoeffding's inequality
           
           å®è·µå«ä¹‰ï¼š
           Practical implication:
           - è¯¯å·®ä»¥æŒ‡æ•°é€Ÿåº¦å‡å°
             Error decreases exponentially
           - ä½†å¸¸æ•°å¯èƒ½å¾ˆå¤§
             But constant can be large
        
        5. ä¸­å¿ƒæé™å®šç† Central Limit Theorem:
           âˆšn(Äœ - v_Ï€(s)) â†’ N(0, ÏƒÂ²)
           
           å¤§æ ·æœ¬ä¸‹ï¼Œä¼°è®¡è¿‘ä¼¼æ­£æ€åˆ†å¸ƒ
           For large samples, estimate is approximately normal
           
           åº”ç”¨ï¼š
           Application:
           - å¯ä»¥æ„é€ ç½®ä¿¡åŒºé—´
             Can construct confidence intervals
           - å¯ä»¥åšå‡è®¾æ£€éªŒ
             Can do hypothesis testing
        """)


# ================================================================================
# ç¬¬4.1.3èŠ‚ï¼šMCç»Ÿè®¡ä¸æ”¶æ•›åˆ†æ
# Section 4.1.3: MC Statistics and Convergence Analysis
# ================================================================================

class MCStatistics:
    """
    MCæ–¹æ³•çš„ç»Ÿè®¡åˆ†æ
    Statistical Analysis for MC Methods
    
    æä¾›MCä¼°è®¡çš„å„ç§ç»Ÿè®¡å·¥å…·
    Provides various statistical tools for MC estimates
    """
    
    def __init__(self):
        """
        åˆå§‹åŒ–ç»Ÿè®¡æ”¶é›†å™¨
        Initialize statistics collector
        """
        # å­˜å‚¨æ¯ä¸ªçŠ¶æ€çš„å›æŠ¥
        self.state_returns: Dict[str, Return] = defaultdict(Return)
        
        # å­˜å‚¨æ¯ä¸ª(çŠ¶æ€,åŠ¨ä½œ)å¯¹çš„å›æŠ¥
        self.state_action_returns: Dict[Tuple[str, str], Return] = defaultdict(Return)
        
        # è®°å½•è®¿é—®æ¬¡æ•°
        self.state_visits: Dict[str, int] = defaultdict(int)
        self.state_action_visits: Dict[Tuple[str, str], int] = defaultdict(int)
        
        # æ”¶æ•›å†å²
        self.convergence_history = []
        
        logger.info("åˆå§‹åŒ–MCç»Ÿè®¡æ”¶é›†å™¨")
    
    def update_state_value(self, state: State, G: float):
        """
        æ›´æ–°çŠ¶æ€ä»·å€¼ç»Ÿè®¡
        Update state value statistics
        
        Args:
            state: çŠ¶æ€
            G: è§‚å¯Ÿåˆ°çš„å›æŠ¥
        """
        state_id = state.id
        self.state_returns[state_id].add_return(G)
        self.state_visits[state_id] += 1
    
    def update_action_value(self, state: State, action: Action, G: float):
        """
        æ›´æ–°åŠ¨ä½œä»·å€¼ç»Ÿè®¡
        Update action value statistics
        
        Args:
            state: çŠ¶æ€
            action: åŠ¨ä½œ
            G: è§‚å¯Ÿåˆ°çš„å›æŠ¥
        """
        sa_pair = (state.id, action.id)
        self.state_action_returns[sa_pair].add_return(G)
        self.state_action_visits[sa_pair] += 1
    
    def get_state_value_estimate(self, state: State) -> float:
        """
        è·å–çŠ¶æ€ä»·å€¼ä¼°è®¡
        Get state value estimate
        
        Returns:
            ä¼°è®¡çš„çŠ¶æ€ä»·å€¼
            Estimated state value
        """
        state_id = state.id
        if state_id in self.state_returns:
            return self.state_returns[state_id].mean
        return 0.0
    
    def get_action_value_estimate(self, state: State, action: Action) -> float:
        """
        è·å–åŠ¨ä½œä»·å€¼ä¼°è®¡
        Get action value estimate
        
        Returns:
            ä¼°è®¡çš„åŠ¨ä½œä»·å€¼
            Estimated action value
        """
        sa_pair = (state.id, action.id)
        if sa_pair in self.state_action_returns:
            return self.state_action_returns[sa_pair].mean
        return 0.0
    
    def get_confidence_intervals(self, confidence: float = 0.95) -> Dict[str, Tuple[float, float]]:
        """
        è·å–æ‰€æœ‰çŠ¶æ€çš„ç½®ä¿¡åŒºé—´
        Get confidence intervals for all states
        
        Args:
            confidence: ç½®ä¿¡æ°´å¹³
                       Confidence level
        
        Returns:
            çŠ¶æ€IDåˆ°ç½®ä¿¡åŒºé—´çš„æ˜ å°„
            Mapping from state ID to confidence interval
        """
        intervals = {}
        for state_id, returns in self.state_returns.items():
            if returns.count >= 2:
                intervals[state_id] = returns.confidence_interval(confidence)
            else:
                intervals[state_id] = (returns.mean, returns.mean)
        return intervals
    
    def analyze_convergence(self, true_values: Optional[Dict[str, float]] = None):
        """
        åˆ†ææ”¶æ•›æ€§
        Analyze convergence
        
        Args:
            true_values: çœŸå®ä»·å€¼ï¼ˆå¦‚æœå·²çŸ¥ï¼‰
                        True values (if known)
        """
        print("\n" + "="*60)
        print("MCæ”¶æ•›åˆ†æ")
        print("MC Convergence Analysis")
        print("="*60)
        
        # ç»Ÿè®¡ä¿¡æ¯
        print(f"\nè®¿é—®ç»Ÿè®¡:")
        print(f"  çŠ¶æ€æ•°: {len(self.state_visits)}")
        print(f"  æ€»è®¿é—®æ¬¡æ•°: {sum(self.state_visits.values())}")
        
        # è®¿é—®é¢‘ç‡åˆ†æ
        if self.state_visits:
            visits = list(self.state_visits.values())
            print(f"  å¹³å‡è®¿é—®: {np.mean(visits):.1f}")
            print(f"  æœ€å°‘è®¿é—®: {min(visits)}")
            print(f"  æœ€å¤šè®¿é—®: {max(visits)}")
        
        # ä¼°è®¡ç²¾åº¦
        print(f"\nä¼°è®¡ç²¾åº¦:")
        for state_id, returns in self.state_returns.items():
            if returns.count > 0:
                ci = returns.confidence_interval(0.95)
                print(f"  {state_id}: {returns.mean:.3f} Â± {(ci[1]-ci[0])/2:.3f} "
                      f"(n={returns.count})")
        
        # å¦‚æœæœ‰çœŸå®å€¼ï¼Œè®¡ç®—è¯¯å·®
        if true_values:
            print(f"\nè¯¯å·®åˆ†æ:")
            errors = []
            for state_id, true_value in true_values.items():
                if state_id in self.state_returns:
                    estimate = self.state_returns[state_id].mean
                    error = abs(estimate - true_value)
                    errors.append(error)
                    print(f"  {state_id}: è¯¯å·®={error:.3f}")
            
            if errors:
                print(f"\n  å¹³å‡è¯¯å·®: {np.mean(errors):.3f}")
                print(f"  æœ€å¤§è¯¯å·®: {max(errors):.3f}")
                print(f"  RMSE: {np.sqrt(np.mean(np.square(errors))):.3f}")
    
    def plot_convergence(self, state_ids: Optional[List[str]] = None):
        """
        ç»˜åˆ¶æ”¶æ•›æ›²çº¿
        Plot convergence curves
        
        Args:
            state_ids: è¦ç»˜åˆ¶çš„çŠ¶æ€IDåˆ—è¡¨
                      List of state IDs to plot
        """
        if not self.convergence_history:
            logger.warning("æ²¡æœ‰æ”¶æ•›å†å²å¯ç»˜åˆ¶")
            return None
        
        fig, axes = plt.subplots(2, 2, figsize=(12, 8))
        
        # é€‰æ‹©è¦ç»˜åˆ¶çš„çŠ¶æ€
        if state_ids is None:
            state_ids = list(self.state_returns.keys())[:4]
        
        # ä¸ºæ¯ä¸ªçŠ¶æ€ç»˜åˆ¶æ”¶æ•›æ›²çº¿
        for idx, state_id in enumerate(state_ids):
            if idx >= 4:
                break
            
            ax = axes[idx // 2, idx % 2]
            
            if state_id in self.state_returns:
                returns_obj = self.state_returns[state_id]
                
                # è®¡ç®—ç´¯ç§¯å¹³å‡
                if returns_obj.returns:
                    cumulative_means = np.cumsum(returns_obj.returns) / np.arange(1, len(returns_obj.returns) + 1)
                    
                    ax.plot(cumulative_means, 'b-', alpha=0.7)
                    ax.axhline(y=returns_obj.mean, color='r', linestyle='--', 
                              label=f'Final: {returns_obj.mean:.2f}')
                    
                    # æ·»åŠ ç½®ä¿¡å¸¦
                    if len(returns_obj.returns) > 10:
                        window = 10
                        stds = []
                        for i in range(window, len(returns_obj.returns)):
                            window_returns = returns_obj.returns[i-window:i]
                            stds.append(np.std(window_returns))
                        
                        if stds:
                            upper = cumulative_means[window:] + np.array(stds)
                            lower = cumulative_means[window:] - np.array(stds)
                            ax.fill_between(range(window, len(cumulative_means)), 
                                          lower, upper, alpha=0.2, color='blue')
                    
                    ax.set_xlabel('Visit Number')
                    ax.set_ylabel('Value Estimate')
                    ax.set_title(f'State: {state_id}')
                    ax.legend()
                    ax.grid(True, alpha=0.3)
        
        plt.suptitle('MC Value Convergence', fontsize=14, fontweight='bold')
        plt.tight_layout()
        
        return fig


# ================================================================================
# ç¬¬4.1.4èŠ‚ï¼šMCåŸºç¡€ç†è®ºå±•ç¤º
# Section 4.1.4: MC Foundations Demonstration
# ================================================================================

class MCFoundations:
    """
    MCåŸºç¡€ç†è®ºç»¼åˆ
    MC Foundations Integration
    
    æ•´åˆæ‰€æœ‰MCåŸºç¡€æ¦‚å¿µ
    Integrate all MC foundation concepts
    """
    
    @staticmethod
    def explain_mc_principles():
        """
        è§£é‡ŠMCåŸç†
        Explain MC Principles
        
        æ•™å­¦å‡½æ•°ï¼Œå¸®åŠ©ç†è§£MCçš„æ ¸å¿ƒæ€æƒ³
        Teaching function to help understand core ideas of MC
        """
        print("\n" + "="*80)
        print("è’™ç‰¹å¡æ´›æ–¹æ³•åŸç†")
        print("Monte Carlo Method Principles")
        print("="*80)
        
        print("""
        ğŸ“š 1. ä»€ä¹ˆæ˜¯è’™ç‰¹å¡æ´›æ–¹æ³•ï¼Ÿ
        What is Monte Carlo Method?
        ================================
        
        æ ¸å¿ƒæ€æƒ³ï¼šé€šè¿‡éšæœºé‡‡æ ·æ¥è¿‘ä¼¼è®¡ç®—
        Core idea: Approximate computation through random sampling
        
        åœ¨RLä¸­çš„åº”ç”¨ï¼š
        Application in RL:
        v_Ï€(s) = E_Ï€[G_t | S_t = s] â‰ˆ (1/n) Î£áµ¢ G_i(s)
        
        å…¶ä¸­G_i(s)æ˜¯ä»çŠ¶æ€så¼€å§‹çš„ç¬¬iä¸ªå›åˆçš„å›æŠ¥
        Where G_i(s) is the return of i-th episode starting from state s
        
        ğŸ“š 2. MC vs DP
        ================================
        
        | æ–¹é¢ Aspect | DP | MC |
        |------------|----|----|
        | æ¨¡å‹ Model | éœ€è¦ Required | ä¸éœ€è¦ Not required |
        | æ›´æ–° Update | å…¨å®½åº¦ Full-width | é‡‡æ · Sampling |
        | åå·® Bias | æ—  None | æ—  None |
        | æ–¹å·® Variance | æ—  None | é«˜ High |
        | é€‚ç”¨ Application | å°ç©ºé—´ Small space | å¤§ç©ºé—´ Large space |
        | ä»»åŠ¡ Tasks | ä»»æ„ Any | å›åˆå¼ Episodic |
        
        ğŸ“š 3. First-Visit vs Every-Visit
        ================================
        
        First-Visit MC:
        - åªä½¿ç”¨æ¯ä¸ªçŠ¶æ€çš„é¦–æ¬¡è®¿é—®
          Only use first visit to each state
        - ç†è®ºæ€§è´¨æ›´å¥½ï¼ˆç‹¬ç«‹æ ·æœ¬ï¼‰
          Better theoretical properties (independent samples)
        - æ”¶æ•›åˆ°v_Ï€(s)
          Converges to v_Ï€(s)
        
        Every-Visit MC:
        - ä½¿ç”¨æ¯ä¸ªçŠ¶æ€çš„æ‰€æœ‰è®¿é—®
          Use all visits to each state
        - æ›´å¤šæ•°æ®ï¼Œå¯èƒ½æ”¶æ•›æ›´å¿«
          More data, may converge faster
        - ä¹Ÿæ”¶æ•›åˆ°v_Ï€(s)ï¼ˆä½†æ ·æœ¬ç›¸å…³ï¼‰
          Also converges to v_Ï€(s) (but samples correlated)
        
        ğŸ“š 4. å¢é‡å®ç°
        Incremental Implementation
        ================================
        
        é¿å…å­˜å‚¨æ‰€æœ‰å›æŠ¥ï¼š
        Avoid storing all returns:
        
        V(s) â† V(s) + Î±[G - V(s)]
        
        å…¶ä¸­ï¼š
        Where:
        - Î± = 1/n(s) ä¿è¯æ”¶æ•›åˆ°æ ·æœ¬å‡å€¼
          Î± = 1/n(s) ensures convergence to sample mean
        - Î± = constant å…è®¸è·Ÿè¸ªéå¹³ç¨³é—®é¢˜
          Î± = constant allows tracking non-stationary problems
        
        è¿™ä¸ªæ›´æ–°è§„åˆ™è´¯ç©¿æ•´ä¸ªRLï¼
        This update rule permeates all of RL!
        
        ğŸ“š 5. MCçš„ä¼˜åŠ¿åœºæ™¯
        When MC Shines
        ================================
        
        1. åªå…³å¿ƒæŸäº›çŠ¶æ€çš„ä»·å€¼
           Only care about value of certain states
           - MCå¯ä»¥åªä¼°è®¡è¿™äº›çŠ¶æ€
             MC can estimate only these states
           - DPå¿…é¡»è®¡ç®—æ‰€æœ‰çŠ¶æ€
             DP must compute all states
        
        2. éé©¬å°”å¯å¤«ç¯å¢ƒ
           Non-Markovian environments
           - MCä¸ä¾èµ–é©¬å°”å¯å¤«æ€§
             MC doesn't rely on Markov property
           - åªè¦èƒ½ç”Ÿæˆå›åˆå³å¯
             Just need to generate episodes
        
        3. æ¨¡å‹æœªçŸ¥æˆ–å¤æ‚
           Model unknown or complex
           - MCç›´æ¥ä»ç»éªŒå­¦ä¹ 
             MC learns directly from experience
           - ä¸éœ€è¦è½¬ç§»æ¦‚ç‡
             No need for transition probabilities
        """)
    
    @staticmethod
    def demonstrate_mc_vs_dp_comparison():
        """
        æ¼”ç¤ºMCä¸DPçš„å¯¹æ¯”
        Demonstrate MC vs DP Comparison
        
        é€šè¿‡ç®€å•ä¾‹å­å±•ç¤ºä¸¤ç§æ–¹æ³•çš„åŒºåˆ«
        Show difference between two methods through simple example
        """
        print("\n" + "="*60)
        print("MC vs DP å¯¹æ¯”æ¼”ç¤º")
        print("MC vs DP Comparison Demo")
        print("="*60)
        
        # åˆ›å»ºä¸€ä¸ªç®€å•çš„é©¬å°”å¯å¤«é“¾
        # Create a simple Markov chain
        states = ['A', 'B', 'C', 'Terminal']
        
        # è½¬ç§»æ¦‚ç‡
        P = {
            'A': {'B': 0.5, 'C': 0.5},
            'B': {'Terminal': 1.0},
            'C': {'Terminal': 1.0}
        }
        
        # å¥–åŠ±
        R = {
            'A': {'B': 0, 'C': 0},
            'B': {'Terminal': 1},
            'C': {'Terminal': 10}
        }
        
        print("\né—®é¢˜è®¾ç½®:")
        print("  çŠ¶æ€: A â†’ {B, C} â†’ Terminal")
        print("  è½¬ç§»: P(B|A)=0.5, P(C|A)=0.5")
        print("  å¥–åŠ±: R(Bâ†’T)=1, R(Câ†’T)=10")
        
        # DPè§£æ³•ï¼ˆç²¾ç¡®ï¼‰
        print("\n1. DPè§£æ³•ï¼ˆç²¾ç¡®ï¼‰:")
        v_dp = {
            'Terminal': 0,
            'B': 1,
            'C': 10,
            'A': 0.5 * 1 + 0.5 * 10  # = 5.5
        }
        print(f"  V(A) = 0.5 Ã— V(B) + 0.5 Ã— V(C)")
        print(f"       = 0.5 Ã— 1 + 0.5 Ã— 10 = {v_dp['A']}")
        
        # MCè§£æ³•ï¼ˆæ¨¡æ‹Ÿï¼‰
        print("\n2. MCè§£æ³•ï¼ˆé‡‡æ ·ï¼‰:")
        np.random.seed(42)
        
        returns_A = []
        n_episodes = 1000
        
        for _ in range(n_episodes):
            # æ¨¡æ‹Ÿä¸€ä¸ªå›åˆ
            if np.random.random() < 0.5:
                # A â†’ B â†’ Terminal
                G = 1
            else:
                # A â†’ C â†’ Terminal
                G = 10
            returns_A.append(G)
        
        # è®¡ç®—MCä¼°è®¡
        mc_estimates = []
        for i in range(1, len(returns_A) + 1):
            mc_estimates.append(np.mean(returns_A[:i]))
        
        print(f"  è¿è¡Œ{n_episodes}ä¸ªå›åˆ")
        print(f"  MCä¼°è®¡: {mc_estimates[-1]:.3f}")
        print(f"  çœŸå®å€¼: {v_dp['A']}")
        print(f"  è¯¯å·®: {abs(mc_estimates[-1] - v_dp['A']):.3f}")
        
        # å¯è§†åŒ–
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
        
        # å·¦å›¾ï¼šæ”¶æ•›è¿‡ç¨‹
        ax1.plot(mc_estimates, 'b-', alpha=0.7, label='MC Estimate')
        ax1.axhline(y=v_dp['A'], color='r', linestyle='--', label=f"DP Solution = {v_dp['A']}")
        
        # æ·»åŠ æ ‡å‡†è¯¯å·®å¸¦
        n_points = len(mc_estimates)
        std_errors = []
        for i in range(1, n_points + 1):
            se = np.std(returns_A[:i]) / np.sqrt(i) if i > 1 else 0
            std_errors.append(se)
        
        upper = np.array(mc_estimates) + 1.96 * np.array(std_errors)
        lower = np.array(mc_estimates) - 1.96 * np.array(std_errors)
        ax1.fill_between(range(n_points), lower, upper, alpha=0.2, color='blue', label='95% CI')
        
        ax1.set_xlabel('Number of Episodes')
        ax1.set_ylabel('Value Estimate')
        ax1.set_title('MC Convergence to DP Solution')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # å³å›¾ï¼šå›æŠ¥åˆ†å¸ƒ
        ax2.hist(returns_A, bins=20, density=True, alpha=0.7, color='green', edgecolor='black')
        ax2.axvline(x=v_dp['A'], color='r', linestyle='--', linewidth=2, label='Expected Value')
        ax2.set_xlabel('Return')
        ax2.set_ylabel('Frequency')
        ax2.set_title('Distribution of Returns')
        ax2.legend()
        ax2.grid(True, alpha=0.3, axis='y')
        
        plt.suptitle('Monte Carlo vs Dynamic Programming', fontsize=14, fontweight='bold')
        plt.tight_layout()
        
        return fig


# ================================================================================
# ä¸»å‡½æ•°ï¼šæ¼”ç¤ºMCåŸºç¡€
# Main Function: Demonstrate MC Foundations
# ================================================================================

def main():
    """
    è¿è¡ŒMCåŸºç¡€æ¼”ç¤º
    Run MC Foundations Demo
    """
    print("\n" + "="*80)
    print("ç¬¬4.1èŠ‚ï¼šè’™ç‰¹å¡æ´›æ–¹æ³•åŸºç¡€")
    print("Section 4.1: Monte Carlo Foundations")
    print("="*80)
    
    # 1. è§£é‡ŠMCåŸç†
    MCFoundations.explain_mc_principles()
    
    # 2. æ¼”ç¤ºå¤§æ•°å®šå¾‹
    print("\næ¼”ç¤ºå¤§æ•°å®šå¾‹...")
    fig1 = LawOfLargeNumbers.demonstrate_convergence()
    
    # 3. åå·®-æ–¹å·®åˆ†æ
    LawOfLargeNumbers.analyze_bias_variance_tradeoff()
    
    # 4. MC vs DPå¯¹æ¯”
    print("\næ¼”ç¤ºMC vs DP...")
    fig2 = MCFoundations.demonstrate_mc_vs_dp_comparison()
    
    # 5. æµ‹è¯•Episodeç±»
    print("\n" + "="*60)
    print("æµ‹è¯•Episodeç±»")
    print("Testing Episode Class")
    print("="*60)
    
    # åˆ›å»ºæ¨¡æ‹Ÿå›åˆ
    from src.ch03_finite_mdp.mdp_framework import State, Action
    
    episode = Episode()
    
    # æ·»åŠ ä¸€äº›ç»éªŒ
    states = [State(f"s{i}", {}) for i in range(4)]
    actions = [Action(f"a{i}", f"Action {i}") for i in range(2)]
    
    # æ¨¡æ‹Ÿè½¨è¿¹: s0 -> s1 -> s2 -> s3(terminal)
    episode.add_experience(Experience(states[0], actions[0], 1.0, states[1], False))
    episode.add_experience(Experience(states[1], actions[1], 2.0, states[2], False))
    episode.add_experience(Experience(states[2], actions[0], 3.0, states[3], True))
    
    print(f"å›åˆé•¿åº¦: {episode.length()}")
    print(f"å›åˆå®Œæˆ: {episode.is_complete()}")
    
    # è®¡ç®—å›æŠ¥
    returns = episode.compute_returns(gamma=0.9)
    print(f"\nå›æŠ¥ (Î³=0.9):")
    for i, G in enumerate(returns):
        print(f"  G_{i} = {G:.3f}")
    
    # éªŒè¯å›æŠ¥è®¡ç®—
    print(f"\néªŒè¯:")
    print(f"  G_0 = 1 + 0.9Ã—2 + 0.9Â²Ã—3 = {1 + 0.9*2 + 0.81*3:.3f}")
    print(f"  è®¡ç®—çš„G_0 = {returns[0]:.3f}")
    
    # 6. æµ‹è¯•ç»Ÿè®¡æ”¶é›†
    print("\n" + "="*60)
    print("æµ‹è¯•MCç»Ÿè®¡")
    print("Testing MC Statistics")
    print("="*60)
    
    stats = MCStatistics()
    
    # æ¨¡æ‹Ÿå¤šä¸ªå›åˆçš„å›æŠ¥
    np.random.seed(42)
    for _ in range(100):
        # ä¸ºçŠ¶æ€s0æ·»åŠ éšæœºå›æŠ¥
        G = np.random.normal(5.0, 2.0)
        stats.update_state_value(states[0], G)
    
    # è·å–ä¼°è®¡
    estimate = stats.get_state_value_estimate(states[0])
    ci = stats.state_returns[states[0].id].confidence_interval(0.95)
    
    print(f"çŠ¶æ€ {states[0].id}:")
    print(f"  ä¼°è®¡å€¼: {estimate:.3f}")
    print(f"  95% CI: [{ci[0]:.3f}, {ci[1]:.3f}]")
    print(f"  æ ·æœ¬æ•°: {stats.state_visits[states[0].id]}")
    
    # åˆ†ææ”¶æ•›
    stats.analyze_convergence({'s0': 5.0})  # çœŸå®å€¼æ˜¯5.0
    
    print("\n" + "="*80)
    print("MCåŸºç¡€æ¼”ç¤ºå®Œæˆï¼")
    print("MC Foundations Demo Complete!")
    print("\nå…³é”®è¦ç‚¹ Key Takeaways:")
    print("1. MCé€šè¿‡é‡‡æ ·ä¼°è®¡æœŸæœ›å€¼")
    print("   MC estimates expected value through sampling")
    print("2. å¤§æ•°å®šå¾‹ä¿è¯æ”¶æ•›åˆ°çœŸå®å€¼")
    print("   Law of Large Numbers guarantees convergence to true value")
    print("3. MCä¼°è®¡æ— åä½†é«˜æ–¹å·®")
    print("   MC estimates are unbiased but high variance")
    print("4. æ”¶æ•›é€Ÿåº¦æ˜¯O(1/âˆšn)")
    print("   Convergence rate is O(1/âˆšn)")
    print("5. MCä¸éœ€è¦æ¨¡å‹ï¼Œåªéœ€è¦ç»éªŒ")
    print("   MC doesn't need model, only needs experience")
    print("="*80)
    
    plt.show()


if __name__ == "__main__":
    main()