#!/usr/bin/env python
"""
æµ‹è¯•ç¬¬11ç« æ‰€æœ‰ç¦»ç­–ç•¥è¿‘ä¼¼æ¨¡å—
Test all Chapter 11 Off-policy Approximation modules

ç¡®ä¿æ‰€æœ‰ç¦»ç­–ç•¥ç®—æ³•å®ç°æ­£ç¡®
Ensure all off-policy algorithm implementations are correct
"""

import sys
import traceback
import numpy as np
from pathlib import Path
import time

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))


def test_importance_sampling():
    """
    æµ‹è¯•é‡è¦æ€§é‡‡æ ·æ–¹æ³•
    Test importance sampling methods
    """
    print("\n" + "="*60)
    print("æµ‹è¯•é‡è¦æ€§é‡‡æ ·...")
    print("Testing Importance Sampling...")
    print("="*60)
    
    try:
        from src.ch11_off_policy_approximation.importance_sampling import (
            Trajectory, ImportanceSampling, SemiGradientOffPolicyTD,
            PerDecisionImportanceSampling, NStepOffPolicyTD
        )
        
        # æµ‹è¯•è½¨è¿¹
        print("\næµ‹è¯•è½¨è¿¹æ•°æ®ç»“æ„...")
        trajectory = Trajectory(
            states=[0, 1, 2],
            actions=[0, 1, 0],
            rewards=[1.0, -1.0, 2.0],
            probs_b=[0.5, 0.5, 0.5],
            probs_pi=[0.8, 0.2, 0.9]
        )
        
        assert trajectory.length == 3
        rho = trajectory.compute_importance_ratio(0, 2)
        assert rho > 0
        g = trajectory.compute_return(0.9, 0)
        assert isinstance(g, float)
        print("  âœ“ è½¨è¿¹æµ‹è¯•é€šè¿‡")
        
        # æµ‹è¯•é‡è¦æ€§é‡‡æ ·
        print("\næµ‹è¯•é‡è¦æ€§é‡‡æ ·è¯„ä¼°...")
        n_states = 5
        
        # æ™®é€šIS
        ordinary_is = ImportanceSampling(n_states, gamma=0.9, weighted=False)
        ordinary_is.update_from_trajectory(trajectory)
        value = ordinary_is.get_value(0)
        assert isinstance(value, float)
        print("  âœ“ æ™®é€šISæµ‹è¯•é€šè¿‡")
        
        # åŠ æƒIS
        weighted_is = ImportanceSampling(n_states, gamma=0.9, weighted=True)
        weighted_is.update_from_trajectory(trajectory)
        value = weighted_is.get_value(0)
        assert isinstance(value, float)
        print("  âœ“ åŠ æƒISæµ‹è¯•é€šè¿‡")
        
        # æµ‹è¯•åŠæ¢¯åº¦ç¦»ç­–ç•¥TD
        print("\næµ‹è¯•åŠæ¢¯åº¦ç¦»ç­–ç•¥TD...")
        n_features = 8
        
        def simple_features(state):
            features = np.zeros(n_features)
            if isinstance(state, int):
                features[state % n_features] = 1.0
            return features
        
        off_td = SemiGradientOffPolicyTD(
            feature_extractor=simple_features,
            n_features=n_features,
            alpha=0.1,
            gamma=0.9
        )
        
        td_error = off_td.update(0, 1.0, 1, False, importance_ratio=1.2)
        assert isinstance(td_error, float)
        assert off_td.update_count == 1
        print("  âœ“ åŠæ¢¯åº¦ç¦»ç­–ç•¥TDæµ‹è¯•é€šè¿‡")
        
        # æµ‹è¯•Per-decision IS
        print("\næµ‹è¯•Per-decisioné‡è¦æ€§é‡‡æ ·...")
        pd_is = PerDecisionImportanceSampling(
            n_features=n_features,
            feature_extractor=simple_features,
            alpha=0.1,
            gamma=0.9,
            lambda_=0.5
        )
        
        td_error = pd_is.update(0, 0, 1.0, 1, False, prob_b=0.5, prob_pi=0.8)
        assert isinstance(td_error, float)
        assert pd_is.update_count == 1
        print("  âœ“ Per-decision ISæµ‹è¯•é€šè¿‡")
        
        # æµ‹è¯•n-stepç¦»ç­–ç•¥TD
        print("\næµ‹è¯•n-stepç¦»ç­–ç•¥TD...")
        n_step_td = NStepOffPolicyTD(
            n_features=n_features,
            feature_extractor=simple_features,
            n=4,
            alpha=0.1,
            gamma=0.9
        )
        
        # æ·»åŠ ç»éªŒ
        for i in range(5):
            n_step_td.add_experience(i, -1.0, 1.1)
        
        assert n_step_td.update_count > 0
        print(f"  âœ“ n-stepç¦»ç­–ç•¥TDæµ‹è¯•é€šè¿‡ï¼Œæ›´æ–°{n_step_td.update_count}æ¬¡")
        
        print("\nâœ… é‡è¦æ€§é‡‡æ ·æµ‹è¯•å…¨éƒ¨é€šè¿‡ï¼")
        return True
        
    except Exception as e:
        print(f"\nâŒ é‡è¦æ€§é‡‡æ ·æµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return False


def test_gradient_td():
    """
    æµ‹è¯•æ¢¯åº¦TDæ–¹æ³•
    Test gradient TD methods
    """
    print("\n" + "="*60)
    print("æµ‹è¯•æ¢¯åº¦TDæ–¹æ³•...")
    print("Testing Gradient TD Methods...")
    print("="*60)
    
    try:
        from src.ch11_off_policy_approximation.gradient_td import (
            ProjectedBellmanError, GTD2, TDC, HTD, GradientLSTD
        )
        
        n_features = 8
        
        def simple_features(state):
            features = np.zeros(n_features)
            if isinstance(state, int):
                features[state % n_features] = 1.0
            return features / (np.linalg.norm(features) + 0.01)
        
        # æµ‹è¯•æŠ•å½±Bellmanè¯¯å·®
        print("\næµ‹è¯•æŠ•å½±Bellmanè¯¯å·®...")
        pbe = ProjectedBellmanError(n_features, simple_features, gamma=0.9)
        
        # æ·»åŠ æ ·æœ¬
        for i in range(10):
            pbe.update_statistics(i % 5, np.random.randn(), (i+1) % 5, False)
        
        test_weights = np.random.randn(n_features) * 0.1
        pbe_value = pbe.compute_pbe(test_weights)
        gradient = pbe.compute_gradient(test_weights)
        
        assert isinstance(pbe_value, float)
        assert len(gradient) == n_features
        print(f"  âœ“ PBEæµ‹è¯•é€šè¿‡ï¼ŒPBE={pbe_value:.4f}")
        
        # æµ‹è¯•GTD2
        print("\næµ‹è¯•GTD2...")
        gtd2 = GTD2(n_features, simple_features, alpha_w=0.01, alpha_v=0.1, gamma=0.9)
        
        for i in range(10):
            td_error = gtd2.update(i % 5, -1.0, (i+1) % 5, False, importance_ratio=1.1)
            assert isinstance(td_error, float)
        
        assert gtd2.update_count == 10
        print(f"  âœ“ GTD2æµ‹è¯•é€šè¿‡ï¼Œ||w||={np.linalg.norm(gtd2.w):.3f}")
        
        # æµ‹è¯•TDC
        print("\næµ‹è¯•TDC...")
        tdc = TDC(n_features, simple_features, alpha_w=0.01, alpha_v=0.1, gamma=0.9)
        
        for i in range(10):
            td_error = tdc.update(i % 5, -1.0, (i+1) % 5, False, importance_ratio=1.1)
            assert isinstance(td_error, float)
        
        assert tdc.update_count == 10
        print(f"  âœ“ TDCæµ‹è¯•é€šè¿‡ï¼Œ||w||={np.linalg.norm(tdc.w):.3f}")
        
        # æµ‹è¯•HTD
        print("\næµ‹è¯•HTD...")
        htd = HTD(n_features, simple_features, alpha=0.01, beta=0.1, gamma=0.9, lambda_=0.5)
        
        for i in range(10):
            td_error = htd.update(i % 5, -1.0, (i+1) % 5, i == 9, importance_ratio=1.2)
            assert isinstance(td_error, float)
        
        assert htd.update_count == 10
        print(f"  âœ“ HTDæµ‹è¯•é€šè¿‡ï¼Œavg_Ï={htd.avg_importance_ratio:.2f}")
        
        # æµ‹è¯•æ¢¯åº¦LSTD
        print("\næµ‹è¯•æ¢¯åº¦LSTD...")
        glstd = GradientLSTD(n_features, simple_features, alpha=0.1, gamma=0.9)
        
        for i in range(10):
            td_error = glstd.update(i % 5, -1.0, (i+1) % 5, False, importance_ratio=1.0)
            assert isinstance(td_error, float)
        
        assert glstd.update_count == 10
        print(f"  âœ“ æ¢¯åº¦LSTDæµ‹è¯•é€šè¿‡ï¼Œ||w||={np.linalg.norm(glstd.w):.3f}")
        
        print("\nâœ… æ¢¯åº¦TDæ–¹æ³•æµ‹è¯•å…¨éƒ¨é€šè¿‡ï¼")
        return True
        
    except Exception as e:
        print(f"\nâŒ æ¢¯åº¦TDæ–¹æ³•æµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return False


def test_emphatic_td():
    """
    æµ‹è¯•å¼ºè°ƒTDæ–¹æ³•
    Test emphatic TD methods
    """
    print("\n" + "="*60)
    print("æµ‹è¯•å¼ºè°ƒTDæ–¹æ³•...")
    print("Testing Emphatic TD Methods...")
    print("="*60)
    
    try:
        from src.ch11_off_policy_approximation.emphatic_td import (
            EmphasisWeights, EmphaticTDLambda, EmphaticTDC,
            ELSTD, TrueOnlineEmphaticTD
        )
        
        n_features = 8
        
        def simple_features(state):
            features = np.zeros(n_features)
            if isinstance(state, int):
                features[state % n_features] = 1.0
            return features / (np.linalg.norm(features) + 0.01)
        
        def interest_fn(state):
            if isinstance(state, int):
                return 2.0 if state % 5 == 0 else 0.5
            return 1.0
        
        # æµ‹è¯•å¼ºè°ƒæƒé‡
        print("\næµ‹è¯•å¼ºè°ƒæƒé‡è®¡ç®—...")
        emphasis_computer = EmphasisWeights(gamma=0.9, lambda_=0.8, interest_fn=interest_fn)
        
        for i in range(5):
            emphasis = emphasis_computer.compute_emphasis(i, importance_ratio=1.2)
            assert isinstance(emphasis, float)
            assert emphasis > 0
        
        stats = emphasis_computer.get_statistics()
        assert 'mean_emphasis' in stats
        print(f"  âœ“ å¼ºè°ƒæƒé‡æµ‹è¯•é€šè¿‡ï¼Œå¹³å‡M={stats['mean_emphasis']:.3f}")
        
        # æµ‹è¯•å¼ºè°ƒTD(Î»)
        print("\næµ‹è¯•å¼ºè°ƒTD(Î»)...")
        emphatic_td = EmphaticTDLambda(
            n_features=n_features,
            feature_extractor=simple_features,
            alpha=0.05,
            gamma=0.9,
            lambda_=0.8,
            interest_fn=interest_fn
        )
        
        for i in range(10):
            td_error = emphatic_td.update(i % 5, -1.0, (i+1) % 5, i == 9, importance_ratio=1.1)
            assert isinstance(td_error, float)
        
        assert emphatic_td.update_count == 10
        print(f"  âœ“ å¼ºè°ƒTD(Î»)æµ‹è¯•é€šè¿‡ï¼Œ||w||={np.linalg.norm(emphatic_td.w):.3f}")
        
        # æµ‹è¯•å¼ºè°ƒTDC
        print("\næµ‹è¯•å¼ºè°ƒTDC...")
        emphatic_tdc = EmphaticTDC(
            n_features=n_features,
            feature_extractor=simple_features,
            alpha_w=0.01,
            alpha_v=0.1,
            gamma=0.9,
            lambda_=0.8,
            interest_fn=interest_fn
        )
        
        for i in range(10):
            td_error = emphatic_tdc.update(i % 5, -1.0, (i+1) % 5, i == 9, importance_ratio=1.1)
            assert isinstance(td_error, float)
        
        assert emphatic_tdc.update_count == 10
        print(f"  âœ“ å¼ºè°ƒTDCæµ‹è¯•é€šè¿‡ï¼Œ||w||={np.linalg.norm(emphatic_tdc.w):.3f}")
        
        # æµ‹è¯•ELSTD
        print("\næµ‹è¯•ELSTD...")
        elstd = ELSTD(
            n_features=n_features,
            feature_extractor=simple_features,
            gamma=0.9,
            lambda_=0.8,
            epsilon=0.01,
            interest_fn=interest_fn
        )
        
        for i in range(20):
            elstd.add_sample(i % 5, -1.0, (i+1) % 5, i == 19, importance_ratio=1.0)
        
        weights = elstd.solve()
        assert len(weights) == n_features
        print(f"  âœ“ ELSTDæµ‹è¯•é€šè¿‡ï¼Œ||w||={np.linalg.norm(weights):.3f}")
        
        # æµ‹è¯•çœŸæ­£çš„åœ¨çº¿å¼ºè°ƒTD
        print("\næµ‹è¯•çœŸæ­£çš„åœ¨çº¿å¼ºè°ƒTD...")
        true_online_etd = TrueOnlineEmphaticTD(
            n_features=n_features,
            feature_extractor=simple_features,
            alpha=0.05,
            gamma=0.9,
            lambda_=0.8,
            interest_fn=interest_fn
        )
        
        for i in range(10):
            td_error = true_online_etd.update(i % 5, -1.0, (i+1) % 5, i == 9, importance_ratio=1.1)
            assert isinstance(td_error, float)
        
        assert true_online_etd.update_count == 10
        print(f"  âœ“ çœŸæ­£çš„åœ¨çº¿å¼ºè°ƒTDæµ‹è¯•é€šè¿‡ï¼Œ||w||={np.linalg.norm(true_online_etd.w):.3f}")
        
        print("\nâœ… å¼ºè°ƒTDæ–¹æ³•æµ‹è¯•å…¨éƒ¨é€šè¿‡ï¼")
        return True
        
    except Exception as e:
        print(f"\nâŒ å¼ºè°ƒTDæ–¹æ³•æµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return False


def test_integration():
    """
    æµ‹è¯•é›†æˆåœºæ™¯
    Test integration scenarios
    """
    print("\n" + "="*60)
    print("æµ‹è¯•é›†æˆåœºæ™¯...")
    print("Testing Integration Scenarios...")
    print("="*60)
    
    try:
        from src.ch11_off_policy_approximation.importance_sampling import (
            ImportanceSampling, Trajectory
        )
        from src.ch11_off_policy_approximation.gradient_td import GTD2, TDC
        from src.ch11_off_policy_approximation.emphatic_td import EmphaticTDLambda
        
        n_features = 8
        n_states = 5
        
        def simple_features(state):
            features = np.zeros(n_features)
            if isinstance(state, int):
                features[state % n_features] = 1.0
                features[(state + 1) % n_features] = 0.3
            return features / (np.linalg.norm(features) + 0.01)
        
        # ç”Ÿæˆè½¨è¿¹
        print("\nç”Ÿæˆæµ‹è¯•è½¨è¿¹...")
        states = list(range(n_states)) * 2
        actions = [i % 2 for i in range(len(states))]
        rewards = [-1.0 if s != 2 else 5.0 for s in states]
        probs_b = [0.5] * len(states)
        probs_pi = [0.8 if a == 0 else 0.2 for a in actions]
        
        trajectory = Trajectory(states, actions, rewards, probs_b, probs_pi)
        print(f"  âœ“ è½¨è¿¹é•¿åº¦: {trajectory.length}")
        
        # æ¯”è¾ƒä¸åŒæ–¹æ³•
        print("\nè®­ç»ƒä¸åŒæ–¹æ³•...")
        
        # GTD2
        gtd2 = GTD2(n_features, simple_features, alpha_w=0.01, alpha_v=0.1, gamma=0.9)
        
        # TDC
        tdc = TDC(n_features, simple_features, alpha_w=0.01, alpha_v=0.1, gamma=0.9)
        
        # å¼ºè°ƒTD
        emphatic_td = EmphaticTDLambda(
            n_features=n_features,
            feature_extractor=simple_features,
            alpha=0.05,
            gamma=0.9,
            lambda_=0.8
        )
        
        # è®­ç»ƒ
        for i in range(len(states) - 1):
            state = states[i]
            reward = rewards[i]
            next_state = states[i + 1]
            rho = probs_pi[i] / probs_b[i]
            
            gtd2.update(state, reward, next_state, False, rho)
            tdc.update(state, reward, next_state, False, rho)
            emphatic_td.update(state, reward, next_state, False, rho)
        
        # æ¯”è¾ƒä»·å€¼ä¼°è®¡
        print("\nä»·å€¼ä¼°è®¡æ¯”è¾ƒ:")
        print("çŠ¶æ€  GTD2    TDC     ETD(Î»)")
        print("-" * 30)
        for state in range(n_states):
            v_gtd2 = gtd2.get_value(state)
            v_tdc = tdc.get_value(state)
            v_etd = emphatic_td.get_value(state)
            print(f"{state:3d}  {v_gtd2:6.3f}  {v_tdc:6.3f}  {v_etd:6.3f}")
        
        print("\nâœ… é›†æˆåœºæ™¯æµ‹è¯•å…¨éƒ¨é€šè¿‡ï¼")
        return True
        
    except Exception as e:
        print(f"\nâŒ é›†æˆåœºæ™¯æµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return False


def main():
    """
    è¿è¡Œæ‰€æœ‰æµ‹è¯•
    Run all tests
    """
    print("\n" + "="*80)
    print("ç¬¬11ç« ï¼šç¦»ç­–ç•¥æ–¹æ³•ä¸è¿‘ä¼¼ - æ¨¡å—æµ‹è¯•")
    print("Chapter 11: Off-policy Methods with Approximation - Module Tests")
    print("="*80)
    
    tests = [
        ("é‡è¦æ€§é‡‡æ ·", test_importance_sampling),
        ("æ¢¯åº¦TDæ–¹æ³•", test_gradient_td),
        ("å¼ºè°ƒTDæ–¹æ³•", test_emphatic_td),
        ("é›†æˆåœºæ™¯", test_integration)
    ]
    
    results = []
    start_time = time.time()
    
    for name, test_func in tests:
        print(f"\nè¿è¡Œæµ‹è¯•: {name}")
        success = test_func()
        results.append((name, success))
        
        if not success:
            print(f"\nâš ï¸ æµ‹è¯•å¤±è´¥ï¼Œåœæ­¢åç»­æµ‹è¯•")
            break
    
    total_time = time.time() - start_time
    
    # æ€»ç»“
    print("\n" + "="*80)
    print("æµ‹è¯•æ€»ç»“ Test Summary")
    print("="*80)
    
    all_passed = True
    for name, success in results:
        status = "âœ… é€šè¿‡" if success else "âŒ å¤±è´¥"
        print(f"{name}: {status}")
        if not success:
            all_passed = False
    
    print(f"\næ€»æµ‹è¯•æ—¶é—´: {total_time:.2f}ç§’")
    
    if all_passed:
        print("\nğŸ‰ ç¬¬11ç« æ‰€æœ‰ç¦»ç­–ç•¥è¿‘ä¼¼æ¨¡å—æµ‹è¯•é€šè¿‡ï¼")
        print("ğŸ‰ All Chapter 11 Off-policy Approximation modules passed!")
        print("\nç¦»ç­–ç•¥è¿‘ä¼¼å®ç°éªŒè¯å®Œæˆ:")
        print("âœ“ é‡è¦æ€§é‡‡æ ·æ–¹æ³•")
        print("âœ“ æ¢¯åº¦TDç®—æ³•")
        print("âœ“ å¼ºè°ƒTDæ–¹æ³•")
        print("âœ“ æŠ•å½±Bellmanè¯¯å·®")
        print("\nè§£å†³äº†è‡´å‘½ä¸‰è¦ç´ é—®é¢˜ï¼")
        print("Solved the deadly triad problem!")
        print("\nå‡†å¤‡è¿›å…¥ç¬¬12ç« ï¼šèµ„æ ¼è¿¹")
        print("Ready to proceed to Chapter 12: Eligibility Traces")
    else:
        print("\nâš ï¸ æœ‰äº›æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä»£ç ")
        print("âš ï¸ Some tests failed, please check the code")
    
    return all_passed


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)