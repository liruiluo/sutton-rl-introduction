#!/usr/bin/env python
"""
æµ‹è¯•ç¬¬10ç« æ‰€æœ‰æ§åˆ¶è¿‘ä¼¼æ¨¡å—
Test all Chapter 10 Control Approximation modules

ç¡®ä¿æ‰€æœ‰æ§åˆ¶ç®—æ³•å®ç°æ­£ç¡®
Ensure all control algorithm implementations are correct
"""

import sys
import traceback
import numpy as np
from pathlib import Path
import time

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))


def test_episodic_semi_gradient():
    """
    æµ‹è¯•å›åˆå¼åŠæ¢¯åº¦æ§åˆ¶
    Test episodic semi-gradient control
    """
    print("\n" + "="*60)
    print("æµ‹è¯•å›åˆå¼åŠæ¢¯åº¦æ§åˆ¶...")
    print("Testing Episodic Semi-gradient Control...")
    print("="*60)
    
    try:
        from src.ch10_on_policy_control_approximation.episodic_semi_gradient import (
            SemiGradientSarsa, SemiGradientExpectedSarsa, 
            SemiGradientNStepSarsa, MountainCarTileCoding, MountainCarState
        )
        
        n_features = 16
        n_actions = 3
        
        # æµ‹è¯•åŠæ¢¯åº¦Sarsa
        print("\næµ‹è¯•åŠæ¢¯åº¦Sarsa...")
        sarsa = SemiGradientSarsa(
            n_features=n_features,
            n_actions=n_actions,
            alpha=0.1,
            gamma=0.9,
            epsilon=0.1
        )
        
        # æ¨¡æ‹Ÿæ›´æ–°
        state = np.random.randn(n_features)
        action = sarsa.select_action(state)
        reward = -1.0
        next_state = np.random.randn(n_features)
        next_action = sarsa.select_action(next_state)
        
        sarsa.update(state, action, reward, next_state, next_action, False)
        assert sarsa.step_count == 1
        print(f"  âœ“ åŠæ¢¯åº¦Sarsaæ›´æ–°æµ‹è¯•é€šè¿‡ï¼Œæ­¥æ•°={sarsa.step_count}")
        
        # æµ‹è¯•Expected Sarsa
        print("\næµ‹è¯•Expected Sarsa...")
        expected_sarsa = SemiGradientExpectedSarsa(
            n_features=n_features,
            n_actions=n_actions,
            alpha=0.1,
            gamma=0.9,
            epsilon=0.1
        )
        
        # è®¡ç®—æœŸæœ›ä»·å€¼
        expected_value = expected_sarsa.get_expected_value(state)
        assert isinstance(expected_value, (float, np.floating))
        
        expected_sarsa.update(state, action, reward, next_state, False)
        assert expected_sarsa.step_count == 1
        print(f"  âœ“ Expected Sarsaæµ‹è¯•é€šè¿‡ï¼ŒæœŸæœ›ä»·å€¼={expected_value:.3f}")
        
        # æµ‹è¯•n-step Sarsa
        print("\næµ‹è¯•n-step Sarsa...")
        n_step = SemiGradientNStepSarsa(
            n_features=n_features,
            n_actions=n_actions,
            n=4,
            alpha=0.1,
            gamma=0.9,
            epsilon=0.1
        )
        
        # å¡«å……ç¼“å†²
        for i in range(6):
            s = np.random.randn(n_features)
            a = n_step.select_action(s)
            r = -1.0
            
            n_step.state_buffer.append(s)
            n_step.action_buffer.append(a)
            n_step.reward_buffer.append(r)
        
        # è®¡ç®—næ­¥å›æŠ¥
        if len(n_step.reward_buffer) >= n_step.n:
            G = n_step.compute_n_step_return(0)
            assert isinstance(G, (float, np.floating))
            print(f"  âœ“ {n_step.n}-step Sarsaæµ‹è¯•é€šè¿‡ï¼Œnæ­¥å›æŠ¥={G:.3f}")
        
        # æµ‹è¯•Mountain Carç“¦ç‰‡ç¼–ç 
        print("\næµ‹è¯•Mountain Carç“¦ç‰‡ç¼–ç ...")
        mc_coder = MountainCarTileCoding(
            n_tilings=8,
            tiles_per_dim=8,
            iht_size=512
        )
        
        # æµ‹è¯•çŠ¶æ€
        test_state = MountainCarState(position=-0.5, velocity=0.0)
        active_tiles = mc_coder.get_active_tiles(test_state)
        features = mc_coder.get_features(test_state)
        
        assert len(active_tiles) == mc_coder.n_tilings
        assert len(features) == 512
        assert np.sum(features) == mc_coder.n_tilings
        
        print(f"  âœ“ Mountain Carç“¦ç‰‡ç¼–ç æµ‹è¯•é€šè¿‡")
        print(f"    æ´»è·ƒç“¦ç‰‡æ•°: {len(active_tiles)}")
        print(f"    ç‰¹å¾ç»´åº¦: {len(features)}")
        print(f"    ç¨€ç–åº¦: {1 - np.sum(features > 0) / len(features):.1%}")
        
        print("\nâœ… å›åˆå¼åŠæ¢¯åº¦æ§åˆ¶æµ‹è¯•å…¨éƒ¨é€šè¿‡ï¼")
        return True
        
    except Exception as e:
        print(f"\nâŒ å›åˆå¼åŠæ¢¯åº¦æ§åˆ¶æµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return False


def test_continuous_tasks():
    """
    æµ‹è¯•è¿ç»­ä»»åŠ¡ç®—æ³•
    Test continuing task algorithms
    """
    print("\n" + "="*60)
    print("æµ‹è¯•è¿ç»­ä»»åŠ¡ç®—æ³•...")
    print("Testing Continuing Task Algorithms...")
    print("="*60)
    
    try:
        from src.ch10_on_policy_control_approximation.continuous_tasks import (
            AverageRewardSetting, DifferentialSemiGradientSarsa,
            AccessControlQueuing
        )
        
        # æµ‹è¯•å¹³å‡å¥–åŠ±è®¾ç½®
        print("\næµ‹è¯•å¹³å‡å¥–åŠ±è®¾ç½®...")
        avg_reward = AverageRewardSetting(alpha=0.01)
        
        # æ¨¡æ‹Ÿå¥–åŠ±æµ
        rewards = []
        for t in range(100):
            reward = 5.0 + 2.0 * np.sin(t * 0.1)
            rewards.append(reward)
            avg_reward.update_average(reward)
        
        true_avg = avg_reward.get_true_average()
        est_avg = avg_reward.average_reward
        recent_avg = avg_reward.get_recent_average(50)
        
        assert abs(true_avg - np.mean(rewards)) < 0.01
        print(f"  âœ“ å¹³å‡å¥–åŠ±è®¾ç½®æµ‹è¯•é€šè¿‡")
        print(f"    çœŸå®å¹³å‡: {true_avg:.3f}")
        print(f"    ä¼°è®¡å¹³å‡: {est_avg:.3f}")
        print(f"    æœ€è¿‘å¹³å‡: {recent_avg:.3f}")
        
        # æµ‹è¯•å·®åˆ†Sarsa
        print("\næµ‹è¯•å·®åˆ†åŠæ¢¯åº¦Sarsa...")
        n_features = 8
        n_actions = 2
        
        diff_sarsa = DifferentialSemiGradientSarsa(
            n_features=n_features,
            n_actions=n_actions,
            alpha=0.1,
            beta=0.01,
            epsilon=0.1
        )
        
        # æ¨¡æ‹Ÿå­¦ä¹ 
        for step in range(10):
            state = np.random.randn(n_features)
            action = diff_sarsa.select_action(state)
            reward = np.random.randn() + 1.0
            next_state = np.random.randn(n_features)
            next_action = diff_sarsa.select_action(next_state)
            
            diff_sarsa.update(state, action, reward, next_state, next_action)
        
        assert diff_sarsa.step_count == 10
        assert len(diff_sarsa.td_errors) == 10
        print(f"  âœ“ å·®åˆ†Sarsaæµ‹è¯•é€šè¿‡")
        print(f"    å¹³å‡å¥–åŠ±: {diff_sarsa.average_reward:.3f}")
        print(f"    TDè¯¯å·®: {np.mean(np.abs(diff_sarsa.td_errors)):.3f}")
        
        # æµ‹è¯•é˜Ÿåˆ—ç³»ç»Ÿ
        print("\næµ‹è¯•Access-Controlé˜Ÿåˆ—...")
        queue_env = AccessControlQueuing(
            n_servers=5,
            n_priorities=4,
            queue_capacity=10,
            arrival_prob=0.6
        )
        
        state = queue_env.reset()
        assert len(state) == 1 + queue_env.n_priorities
        
        # è¿è¡Œå‡ æ­¥
        total_reward = 0.0
        for _ in range(20):
            action = np.random.choice([0, 1])  # éšæœºæ¥å—/æ‹’ç»
            next_state, reward, done, info = queue_env.step(action)
            total_reward += reward
            assert not done  # è¿ç»­ä»»åŠ¡æ°¸ä¸ç»“æŸ
            state = next_state
        
        print(f"  âœ“ é˜Ÿåˆ—ç³»ç»Ÿæµ‹è¯•é€šè¿‡")
        print(f"    æ¥å—é¡¾å®¢: {queue_env.accepted_customers}")
        print(f"    æ‹’ç»é¡¾å®¢: {queue_env.rejected_customers}")
        print(f"    æ€»å¥–åŠ±: {total_reward:.1f}")
        
        print("\nâœ… è¿ç»­ä»»åŠ¡ç®—æ³•æµ‹è¯•å…¨éƒ¨é€šè¿‡ï¼")
        return True
        
    except Exception as e:
        print(f"\nâŒ è¿ç»­ä»»åŠ¡ç®—æ³•æµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return False


def test_control_with_fa():
    """
    æµ‹è¯•é€šç”¨æ§åˆ¶æ¡†æ¶
    Test general control framework
    """
    print("\n" + "="*60)
    print("æµ‹è¯•é€šç”¨æ§åˆ¶æ¡†æ¶...")
    print("Testing General Control Framework...")
    print("="*60)
    
    try:
        from src.ch10_on_policy_control_approximation.control_with_fa import (
            LinearActionValueFunction, ControlWithFA, ActorCriticWithFA
        )
        
        n_features = 8
        n_actions = 3
        
        # ç®€å•ç‰¹å¾æå–å™¨
        def simple_features(state, action):
            if isinstance(state, np.ndarray):
                return state[:n_features]
            return np.random.randn(n_features)
        
        # æµ‹è¯•çº¿æ€§åŠ¨ä½œä»·å€¼å‡½æ•°
        print("\næµ‹è¯•çº¿æ€§åŠ¨ä½œä»·å€¼å‡½æ•°...")
        linear_q = LinearActionValueFunction(
            feature_extractor=simple_features,
            n_features=n_features,
            n_actions=n_actions
        )
        
        test_state = np.random.randn(n_features)
        
        # æµ‹è¯•é¢„æµ‹
        for a in range(n_actions):
            q_val = linear_q.predict(test_state, a)
            assert isinstance(q_val, (float, np.floating))
        
        # æµ‹è¯•æ›´æ–°
        action = 0
        target = 1.0
        td_error = linear_q.update(test_state, action, target, alpha=0.1)
        assert isinstance(td_error, (float, np.floating))
        assert linear_q.update_count == 1
        
        print(f"  âœ“ çº¿æ€§åŠ¨ä½œä»·å€¼å‡½æ•°æµ‹è¯•é€šè¿‡")
        
        # æµ‹è¯•æ§åˆ¶æ¡†æ¶ - Sarsa
        print("\næµ‹è¯•æ§åˆ¶æ¡†æ¶(Sarsa)...")
        sarsa_controller = ControlWithFA(
            approximator=linear_q,
            n_actions=n_actions,
            alpha=0.1,
            gamma=0.9,
            epsilon=0.1,
            method='sarsa'
        )
        
        state = np.random.randn(n_features)
        action = sarsa_controller.select_action(state)
        reward = -1.0
        next_state = np.random.randn(n_features)
        
        td_error = sarsa_controller.learn_step(state, action, reward, next_state, False)
        assert isinstance(td_error, (float, np.floating))
        assert sarsa_controller.step_count == 1
        
        print(f"  âœ“ Sarsaæ§åˆ¶å™¨æµ‹è¯•é€šè¿‡")
        
        # æµ‹è¯•æ§åˆ¶æ¡†æ¶ - Q-learning
        print("\næµ‹è¯•æ§åˆ¶æ¡†æ¶(Q-learning)...")
        q_approximator = LinearActionValueFunction(
            feature_extractor=simple_features,
            n_features=n_features,
            n_actions=n_actions
        )
        
        q_controller = ControlWithFA(
            approximator=q_approximator,
            n_actions=n_actions,
            alpha=0.1,
            gamma=0.9,
            epsilon=0.1,
            method='q_learning'
        )
        
        td_error = q_controller.learn_step(state, action, reward, next_state, False)
        assert isinstance(td_error, (float, np.floating))
        
        print(f"  âœ“ Q-learningæ§åˆ¶å™¨æµ‹è¯•é€šè¿‡")
        
        # æµ‹è¯•æ§åˆ¶æ¡†æ¶ - Expected Sarsa
        print("\næµ‹è¯•æ§åˆ¶æ¡†æ¶(Expected Sarsa)...")
        exp_approximator = LinearActionValueFunction(
            feature_extractor=simple_features,
            n_features=n_features,
            n_actions=n_actions
        )
        
        exp_controller = ControlWithFA(
            approximator=exp_approximator,
            n_actions=n_actions,
            alpha=0.1,
            gamma=0.9,
            epsilon=0.1,
            method='expected_sarsa'
        )
        
        td_error = exp_controller.learn_step(state, action, reward, next_state, False)
        assert isinstance(td_error, (float, np.floating))
        
        print(f"  âœ“ Expected Sarsaæ§åˆ¶å™¨æµ‹è¯•é€šè¿‡")
        
        # æµ‹è¯•Actor-Critic
        print("\næµ‹è¯•Actor-Criticæ¶æ„...")
        state_dim = 4
        ac = ActorCriticWithFA(
            state_dim=state_dim,
            n_actions=n_actions,
            actor_lr=0.01,
            critic_lr=0.1,
            gamma=0.9
        )
        
        test_state = np.random.randn(state_dim)
        
        # æµ‹è¯•åŠ¨ä½œæ¦‚ç‡
        probs = ac.get_action_probabilities(test_state)
        assert len(probs) == n_actions
        assert abs(np.sum(probs) - 1.0) < 1e-6
        
        # æµ‹è¯•åŠ¨ä½œé€‰æ‹©
        action = ac.select_action(test_state)
        assert 0 <= action < n_actions
        
        # æµ‹è¯•çŠ¶æ€ä»·å€¼
        value = ac.get_state_value(test_state)
        assert isinstance(value, (float, np.floating))
        
        # æµ‹è¯•æ›´æ–°
        reward = 1.0
        next_state = np.random.randn(state_dim)
        ac.update(test_state, action, reward, next_state, False)
        
        print(f"  âœ“ Actor-Criticæµ‹è¯•é€šè¿‡")
        print(f"    åŠ¨ä½œæ¦‚ç‡: {[f'{p:.3f}' for p in probs]}")
        print(f"    çŠ¶æ€ä»·å€¼: {value:.3f}")
        
        print("\nâœ… é€šç”¨æ§åˆ¶æ¡†æ¶æµ‹è¯•å…¨éƒ¨é€šè¿‡ï¼")
        return True
        
    except Exception as e:
        print(f"\nâŒ é€šç”¨æ§åˆ¶æ¡†æ¶æµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return False


def test_integration():
    """
    æµ‹è¯•é›†æˆåœºæ™¯
    Test integration scenarios
    """
    print("\n" + "="*60)
    print("æµ‹è¯•é›†æˆåœºæ™¯...")
    print("Testing Integration Scenarios...")
    print("="*60)
    
    try:
        from src.ch10_on_policy_control_approximation.episodic_semi_gradient import (
            SemiGradientSarsa
        )
        from src.ch10_on_policy_control_approximation.continuous_tasks import (
            DifferentialSemiGradientSarsa, AccessControlQueuing
        )
        
        # æµ‹è¯•å›åˆå¼åˆ°è¿ç»­ä»»åŠ¡çš„è½¬æ¢
        print("\næµ‹è¯•ä»»åŠ¡ç±»å‹è½¬æ¢...")
        
        # å›åˆå¼ä»»åŠ¡
        n_features = 8
        n_actions = 2
        
        episodic_agent = SemiGradientSarsa(
            n_features=n_features,
            n_actions=n_actions,
            alpha=0.1,
            gamma=0.9,
            epsilon=0.1
        )
        
        # è¿ç»­ä»»åŠ¡
        continuous_agent = DifferentialSemiGradientSarsa(
            n_features=n_features,
            n_actions=n_actions,
            alpha=0.1,
            beta=0.01,
            epsilon=0.1
        )
        
        # æ¨¡æ‹Ÿè½¬æ¢
        state = np.random.randn(n_features)
        
        # ä¸¤ç§agenté€‰æ‹©åŠ¨ä½œ
        episodic_action = episodic_agent.select_action(state)
        continuous_action = continuous_agent.select_action(state)
        
        assert 0 <= episodic_action < n_actions
        assert 0 <= continuous_action < n_actions
        
        print(f"  âœ“ ä»»åŠ¡ç±»å‹è½¬æ¢æµ‹è¯•é€šè¿‡")
        print(f"    å›åˆå¼é€‰æ‹©: åŠ¨ä½œ{episodic_action}")
        print(f"    è¿ç»­ä»»åŠ¡é€‰æ‹©: åŠ¨ä½œ{continuous_action}")
        
        # æµ‹è¯•é˜Ÿåˆ—ç³»ç»Ÿä¸å·®åˆ†Sarsaé›†æˆ
        print("\næµ‹è¯•é˜Ÿåˆ—ç³»ç»Ÿé›†æˆ...")
        queue_env = AccessControlQueuing(
            n_servers=3,
            n_priorities=2,
            queue_capacity=5
        )
        
        state_size = 1 + queue_env.n_priorities
        learner = DifferentialSemiGradientSarsa(
            n_features=state_size,
            n_actions=2,
            alpha=0.1,
            beta=0.01,
            epsilon=0.2
        )
        
        # å­¦ä¹ å‡ æ­¥
        state = queue_env.reset()
        action = learner.select_action(state)
        
        for step in range(20):
            next_state, reward, _, _ = queue_env.step(action)
            next_action = learner.select_action(next_state)
            
            learner.update(state, action, reward, next_state, next_action)
            
            state = next_state
            action = next_action
        
        assert learner.step_count == 20
        print(f"  âœ“ é˜Ÿåˆ—ç³»ç»Ÿé›†æˆæµ‹è¯•é€šè¿‡")
        print(f"    å­¦ä¹ æ­¥æ•°: {learner.step_count}")
        print(f"    å¹³å‡å¥–åŠ±: {learner.average_reward:.3f}")
        
        print("\nâœ… é›†æˆåœºæ™¯æµ‹è¯•å…¨éƒ¨é€šè¿‡ï¼")
        return True
        
    except Exception as e:
        print(f"\nâŒ é›†æˆåœºæ™¯æµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return False


def main():
    """
    è¿è¡Œæ‰€æœ‰æµ‹è¯•
    Run all tests
    """
    print("\n" + "="*80)
    print("ç¬¬10ç« ï¼šä½¿ç”¨è¿‘ä¼¼çš„åŒç­–ç•¥æ§åˆ¶ - æ¨¡å—æµ‹è¯•")
    print("Chapter 10: On-policy Control with Approximation - Module Tests")
    print("="*80)
    
    tests = [
        ("å›åˆå¼åŠæ¢¯åº¦æ§åˆ¶", test_episodic_semi_gradient),
        ("è¿ç»­ä»»åŠ¡ç®—æ³•", test_continuous_tasks),
        ("é€šç”¨æ§åˆ¶æ¡†æ¶", test_control_with_fa),
        ("é›†æˆåœºæ™¯", test_integration)
    ]
    
    results = []
    start_time = time.time()
    
    for name, test_func in tests:
        print(f"\nè¿è¡Œæµ‹è¯•: {name}")
        success = test_func()
        results.append((name, success))
        
        if not success:
            print(f"\nâš ï¸ æµ‹è¯•å¤±è´¥ï¼Œåœæ­¢åç»­æµ‹è¯•")
            break
    
    total_time = time.time() - start_time
    
    # æ€»ç»“
    print("\n" + "="*80)
    print("æµ‹è¯•æ€»ç»“ Test Summary")
    print("="*80)
    
    all_passed = True
    for name, success in results:
        status = "âœ… é€šè¿‡" if success else "âŒ å¤±è´¥"
        print(f"{name}: {status}")
        if not success:
            all_passed = False
    
    print(f"\næ€»æµ‹è¯•æ—¶é—´: {total_time:.2f}ç§’")
    
    if all_passed:
        print("\nğŸ‰ ç¬¬10ç« æ‰€æœ‰æ§åˆ¶è¿‘ä¼¼æ¨¡å—æµ‹è¯•é€šè¿‡ï¼")
        print("ğŸ‰ All Chapter 10 Control Approximation modules passed!")
        print("\næ§åˆ¶è¿‘ä¼¼å®ç°éªŒè¯å®Œæˆ:")
        print("âœ“ å›åˆå¼åŠæ¢¯åº¦æ§åˆ¶")
        print("âœ“ è¿ç»­ä»»åŠ¡ä¸å¹³å‡å¥–åŠ±")
        print("âœ“ é€šç”¨æ§åˆ¶æ¡†æ¶")
        print("âœ“ Actor-Criticæ¶æ„")
        print("\nä»é¢„æµ‹åˆ°æ§åˆ¶çš„æˆåŠŸæ‰©å±•ï¼")
        print("Successful extension from prediction to control!")
        print("\nå‡†å¤‡è¿›å…¥ç¬¬11ç« ï¼šç¦»ç­–ç•¥æ–¹æ³•")
        print("Ready to proceed to Chapter 11: Off-policy Methods")
    else:
        print("\nâš ï¸ æœ‰äº›æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä»£ç ")
        print("âš ï¸ Some tests failed, please check the code")
    
    return all_passed


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)