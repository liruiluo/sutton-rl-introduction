"""
================================================================================
Sutton & Bartoã€Šå¼ºåŒ–å­¦ä¹ ï¼šå¯¼è®ºã€‹å®Œæ•´æ•™ç§‘ä¹¦å¼ä»£ç å®ç°
Complete Textbook-Style Implementation of Sutton & Barto's RL: An Introduction

ä½œè€…è¯´æ˜ Author's Note:
è¿™ä¸ä»…ä»…æ˜¯ä»£ç å®ç°ï¼Œæ›´æ˜¯ä¸€æœ¬å¯ä»¥ç‹¬ç«‹é˜…è¯»çš„å¼ºåŒ–å­¦ä¹ æ•™ç§‘ä¹¦ã€‚
æ¯ä¸ªæ–‡ä»¶éƒ½åŒ…å«è¯¦ç»†çš„è§£é‡Šã€æ•°å­¦æ¨å¯¼ã€ç›´è§‚ä¾‹å­å’Œå¯è¿è¡Œçš„ä»£ç ã€‚
è¯»å®Œè¿™äº›ä»£ç ï¼Œä½ å°±æŒæ¡äº†å¼ºåŒ–å­¦ä¹ çš„ç²¾é«“ã€‚

This is not just code implementation, but a complete RL textbook you can read.
Every file contains detailed explanations, math derivations, intuitive examples, 
and runnable code. After reading this code, you'll master the essence of RL.
================================================================================

å¦‚ä½•ä½¿ç”¨æœ¬é¡¹ç›® How to Use This Project:
1. æŒ‰é¡ºåºé˜…è¯»æ¯ä¸ªç« èŠ‚çš„ä»£ç 
2. è¿è¡Œæ¯ä¸ªæ¨¡å—çš„æ¼”ç¤º
3. ä¿®æ”¹å‚æ•°ï¼Œè§‚å¯Ÿå˜åŒ–
4. å®ç°è¯¾åç»ƒä¹ 

Read each chapter's code in order
Run demonstrations in each module  
Modify parameters and observe changes
Implement exercises
================================================================================
"""

import os
import sys
from typing import Dict, List, Tuple
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))


class RLTextbook:
    """
    å¼ºåŒ–å­¦ä¹ æ•™ç§‘ä¹¦å¯¼èˆªç³»ç»Ÿ
    RL Textbook Navigation System
    
    è¿™ä¸ªç±»æä¾›æ•´æœ¬"ä»£ç æ•™ç§‘ä¹¦"çš„å¯¼èˆªåŠŸèƒ½
    """
    
    def __init__(self):
        """åˆå§‹åŒ–æ•™ç§‘ä¹¦ç»“æ„"""
        self.chapters = self._build_chapter_structure()
        
    def _build_chapter_structure(self) -> Dict:
        """
        æ„å»ºå®Œæ•´çš„ç« èŠ‚ç»“æ„
        
        æ¯ä¸ªç« èŠ‚éƒ½å¯¹åº”Sutton & Bartoä¹¦ä¸­çš„å†…å®¹
        """
        return {
            "00_preface": {
                "title": "å‰è¨€ï¼šä»€ä¹ˆæ˜¯å¼ºåŒ–å­¦ä¹ ",
                "title_en": "Preface: What is Reinforcement Learning",
                "modules": {
                    "what_is_rl.py": "å¼ºåŒ–å­¦ä¹ çš„æœ¬è´¨ä¸æŒ‘æˆ˜",
                },
                "key_concepts": [
                    "è¯•é”™å­¦ä¹  Trial-and-error learning",
                    "å»¶è¿Ÿå¥–åŠ± Delayed reward", 
                    "æ¢ç´¢ä¸åˆ©ç”¨ Exploration vs Exploitation",
                    "ä¿¡ç”¨åˆ†é… Credit assignment"
                ],
                "learning_goal": "ç†è§£å¼ºåŒ–å­¦ä¹ ä¸å…¶ä»–å­¦ä¹ èŒƒå¼çš„åŒºåˆ«"
            },
            
            "01_introduction": {
                "title": "ç¬¬1ç« ï¼šå¼•è¨€",
                "title_en": "Chapter 1: Introduction", 
                "modules": {
                    "rl_fundamentals.py": "å¼ºåŒ–å­¦ä¹ åŸºç¡€æ¦‚å¿µ",
                    "tic_tac_toe.py": "äº•å­—æ£‹å®Œæ•´å®ç°",
                    "history_and_concepts.py": "å¼ºåŒ–å­¦ä¹ çš„å†å²ä¸æ ¸å¿ƒæ¦‚å¿µ"
                },
                "key_concepts": [
                    "æ™ºèƒ½ä½“ä¸ç¯å¢ƒ Agent and Environment",
                    "å¥–åŠ±ä¿¡å· Reward Signal",
                    "ä»·å€¼å‡½æ•° Value Function",
                    "ç­–ç•¥ Policy",
                    "æ¨¡å‹ Model"
                ],
                "learning_goal": "æŒæ¡å¼ºåŒ–å­¦ä¹ çš„åŸºæœ¬è¦ç´ å’Œæ¡†æ¶"
            },
            
            "02_multi_armed_bandits": {
                "title": "ç¬¬2ç« ï¼šå¤šè‡‚èµŒåšæœº",
                "title_en": "Chapter 2: Multi-Armed Bandits",
                "modules": {
                    "bandit_introduction.py": "èµŒåšæœºé—®é¢˜å®šä¹‰",
                    "epsilon_greedy.py": "Îµ-è´ªå©ªç®—æ³•ï¼ˆæ·±åº¦è§£æï¼‰",
                    "ucb_algorithm.py": "ä¸Šç½®ä¿¡ç•Œç®—æ³•",
                    "gradient_bandit.py": "æ¢¯åº¦èµŒåšæœºç®—æ³•"
                },
                "key_concepts": [
                    "åŠ¨ä½œä»·å€¼ Action Values q*(a)",
                    "å¢é‡æ›´æ–° Incremental Updates",
                    "Îµ-è´ªå©ªæ–¹æ³• Îµ-greedy Methods",
                    "ä¹è§‚åˆå§‹å€¼ Optimistic Initial Values",
                    "ä¸Šç½®ä¿¡ç•Œ Upper Confidence Bounds",
                    "æ¢¯åº¦ä¸Šå‡ Gradient Ascent"
                ],
                "learning_goal": "æ·±å…¥ç†è§£æ¢ç´¢ä¸åˆ©ç”¨çš„å¹³è¡¡"
            },
            
            "03_finite_mdp": {
                "title": "ç¬¬3ç« ï¼šæœ‰é™é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹",
                "title_en": "Chapter 3: Finite Markov Decision Processes",
                "modules": {
                    "mdp_framework.py": "MDPæ¡†æ¶å®ç°",
                    "agent_environment_interface.py": "æ™ºèƒ½ä½“-ç¯å¢ƒæ¥å£",
                    "policies_and_values.py": "ç­–ç•¥ä¸ä»·å€¼å‡½æ•°",
                    "gridworld.py": "ç½‘æ ¼ä¸–ç•Œç¯å¢ƒ"
                },
                "key_concepts": [
                    "é©¬å°”å¯å¤«æ€§è´¨ Markov Property",
                    "çŠ¶æ€è½¬ç§»æ¦‚ç‡ State Transition Probability p(s'|s,a)",
                    "è´å°”æ›¼æœŸæœ›æ–¹ç¨‹ Bellman Expectation Equation",
                    "è´å°”æ›¼æœ€ä¼˜æ–¹ç¨‹ Bellman Optimality Equation",
                    "æœ€ä¼˜ç­–ç•¥ Optimal Policy Ï€*",
                    "æœ€ä¼˜ä»·å€¼å‡½æ•° Optimal Value Functions v*, q*"
                ],
                "learning_goal": "æŒæ¡å¼ºåŒ–å­¦ä¹ çš„æ•°å­¦æ¡†æ¶"
            },
            
            "04_dynamic_programming": {
                "title": "ç¬¬4ç« ï¼šåŠ¨æ€è§„åˆ’",
                "title_en": "Chapter 4: Dynamic Programming",
                "modules": {
                    "dp_foundations.py": "åŠ¨æ€è§„åˆ’åŸºç¡€",
                    "policy_iteration.py": "ç­–ç•¥è¿­ä»£ç®—æ³•",
                    "value_iteration.py": "ä»·å€¼è¿­ä»£ç®—æ³•",
                    "generalized_policy_iteration.py": "å¹¿ä¹‰ç­–ç•¥è¿­ä»£",
                    "dp_examples.py": "ç»å…¸DPé—®é¢˜"
                },
                "key_concepts": [
                    "ç­–ç•¥è¯„ä¼° Policy Evaluation",
                    "ç­–ç•¥æ”¹è¿› Policy Improvement",
                    "ç­–ç•¥è¿­ä»£ Policy Iteration",
                    "ä»·å€¼è¿­ä»£ Value Iteration",
                    "å¼‚æ­¥DP Asynchronous DP",
                    "å¹¿ä¹‰ç­–ç•¥è¿­ä»£ GPI"
                ],
                "learning_goal": "ç†è§£å®Œç¾æ¨¡å‹ä¸‹çš„æœ€ä¼˜è§£æ³•"
            },
            
            "05_monte_carlo": {
                "title": "ç¬¬5ç« ï¼šè’™ç‰¹å¡æ´›æ–¹æ³•",
                "title_en": "Chapter 5: Monte Carlo Methods",
                "modules": {
                    "mc_foundations.py": "MCæ–¹æ³•åŸºç¡€",
                    "mc_prediction.py": "MCé¢„æµ‹",
                    "mc_control.py": "MCæ§åˆ¶",
                    "importance_sampling.py": "é‡è¦æ€§é‡‡æ ·",
                    "mc_examples.py": "21ç‚¹æ¸¸æˆ"
                },
                "key_concepts": [
                    "é¦–æ¬¡è®¿é—®MC First-Visit MC",
                    "æ¯æ¬¡è®¿é—®MC Every-Visit MC",
                    "æ¢ç´¢æ€§å¯åŠ¨ Exploring Starts",
                    "è½¯ç­–ç•¥ Soft Policies",
                    "ç¦»ç­–ç•¥å­¦ä¹  Off-Policy Learning",
                    "é‡è¦æ€§é‡‡æ · Importance Sampling"
                ],
                "learning_goal": "ä»å®Œæ•´ç»éªŒåºåˆ—ä¸­å­¦ä¹ "
            },
            
            "06_temporal_difference": {
                "title": "ç¬¬6ç« ï¼šæ—¶åºå·®åˆ†å­¦ä¹ ",
                "title_en": "Chapter 6: Temporal-Difference Learning",
                "modules": {
                    "td_foundations.py": "TDå­¦ä¹ åŸºç¡€",
                    "td_control.py": "TDæ§åˆ¶ç®—æ³•",
                    "n_step_td.py": "næ­¥TDæ–¹æ³•"
                },
                "key_concepts": [
                    "TD(0)ç®—æ³•",
                    "SARSAç®—æ³•",
                    "Q-learningç®—æ³•",
                    "Expected SARSA",
                    "Double Q-learning",
                    "TDè¯¯å·® TD Error Î´"
                ],
                "learning_goal": "ç»“åˆDPå’ŒMCçš„ä¼˜åŠ¿"
            },
            
            "07_n_step_bootstrapping": {
                "title": "ç¬¬7ç« ï¼šnæ­¥è‡ªä¸¾",
                "title_en": "Chapter 7: n-step Bootstrapping",
                "modules": {
                    "n_step_td.py": "næ­¥TDé¢„æµ‹",
                    "n_step_sarsa.py": "næ­¥SARSA",
                    "off_policy_n_step.py": "næ­¥ç¦»ç­–ç•¥å­¦ä¹ ",
                    "tree_backup.py": "æ ‘å¤‡ä»½ç®—æ³•"
                },
                "key_concepts": [
                    "næ­¥å›æŠ¥ n-step Return G_t:t+n",
                    "næ­¥TDé¢„æµ‹ n-step TD Prediction",
                    "næ­¥SARSA n-step SARSA",
                    "næ­¥æœŸæœ›SARSA n-step Expected SARSA",
                    "næ­¥æ ‘å¤‡ä»½ n-step Tree Backup",
                    "ç»Ÿä¸€ç®—æ³• Unifying Algorithm"
                ],
                "learning_goal": "ç»Ÿä¸€è§†è§’çœ‹å¾…MCå’ŒTD"
            },
            
            "08_planning_and_learning": {
                "title": "ç¬¬8ç« ï¼šè§„åˆ’ä¸å­¦ä¹ çš„æ•´åˆ",
                "title_en": "Chapter 8: Planning and Learning with Tabular Methods",
                "modules": {
                    "models_and_planning.py": "æ¨¡å‹ä¸è§„åˆ’",
                    "dyna_q.py": "Dyna-Qç®—æ³•",
                    "prioritized_sweeping.py": "ä¼˜å…ˆæ‰«æ",
                    "expected_vs_sample.py": "æœŸæœ›æ›´æ–°vsæ ·æœ¬æ›´æ–°",
                    "trajectory_sampling.py": "è½¨è¿¹é‡‡æ ·",
                    "mcts.py": "è’™ç‰¹å¡æ´›æ ‘æœç´¢"
                },
                "key_concepts": [
                    "æ¨¡å‹ Model",
                    "è§„åˆ’ Planning",
                    "Dynaæ¶æ„ Dyna Architecture",
                    "ä¼˜å…ˆæ‰«æ Prioritized Sweeping",
                    "MCTSè’™ç‰¹å¡æ´›æ ‘æœç´¢"
                ],
                "learning_goal": "æ•´åˆåŸºäºæ¨¡å‹å’Œæ— æ¨¡å‹æ–¹æ³•"
            },
            
            "09_on_policy_approximation": {
                "title": "ç¬¬9ç« ï¼šåœ¨ç­–ç•¥é¢„æµ‹çš„è¿‘ä¼¼æ–¹æ³•",
                "title_en": "Chapter 9: On-policy Prediction with Approximation",
                "modules": {
                    "gradient_descent.py": "æ¢¯åº¦ä¸‹é™åŸºç¡€",
                    "linear_approximation.py": "çº¿æ€§å‡½æ•°è¿‘ä¼¼",
                    "feature_construction.py": "ç‰¹å¾æ„é€ ",
                    "least_squares_td.py": "æœ€å°äºŒä¹˜TD",
                    "neural_approximation.py": "ç¥ç»ç½‘ç»œè¿‘ä¼¼"
                },
                "key_concepts": [
                    "å‡½æ•°è¿‘ä¼¼ Function Approximation",
                    "éšæœºæ¢¯åº¦ä¸‹é™ SGD",
                    "åŠæ¢¯åº¦æ–¹æ³• Semi-gradient Methods",
                    "ç‰¹å¾å‘é‡ Feature Vectors",
                    "LSTDæœ€å°äºŒä¹˜TD"
                ],
                "learning_goal": "å¤„ç†å¤§è§„æ¨¡çŠ¶æ€ç©ºé—´"
            },
            
            "10_on_policy_control_approximation": {
                "title": "ç¬¬10ç« ï¼šåœ¨ç­–ç•¥æ§åˆ¶çš„è¿‘ä¼¼æ–¹æ³•",
                "title_en": "Chapter 10: On-policy Control with Approximation",
                "modules": {
                    "control_with_fa.py": "å‡½æ•°è¿‘ä¼¼æ§åˆ¶",
                    "episodic_semi_gradient.py": "å›åˆå¼åŠæ¢¯åº¦æ§åˆ¶",
                    "continuous_tasks.py": "è¿ç»­ä»»åŠ¡"
                },
                "key_concepts": [
                    "å±±è½¦é—®é¢˜ Mountain Car",
                    "å›åˆå¼åŠæ¢¯åº¦Sarsa",
                    "å¹³å‡å¥–åŠ±è®¾å®š Average Reward",
                    "å·®åˆ†åŠæ¢¯åº¦Sarsa"
                ],
                "learning_goal": "è¿‘ä¼¼æ–¹æ³•çš„æ§åˆ¶ç®—æ³•"
            },
            
            "11_off_policy_approximation": {
                "title": "ç¬¬11ç« ï¼šç¦»ç­–ç•¥æ–¹æ³•çš„è¿‘ä¼¼",
                "title_en": "Chapter 11: Off-policy Methods with Approximation",
                "modules": {
                    "importance_sampling.py": "é‡è¦æ€§é‡‡æ ·",
                    "gradient_td.py": "æ¢¯åº¦TDæ–¹æ³•",
                    "emphatic_td.py": "å¼ºè°ƒTD"
                },
                "key_concepts": [
                    "åŠæ¢¯åº¦ç¦»ç­–ç•¥TD",
                    "æ¢¯åº¦TD GTD/TDC",
                    "å¼ºè°ƒTD Emphatic TD",
                    "è‡´å‘½ä¸‰è¦ç´  Deadly Triad"
                ],
                "learning_goal": "ç¦»ç­–ç•¥å­¦ä¹ çš„ç¨³å®šæ€§"
            },
            
            "12_eligibility_traces": {
                "title": "ç¬¬12ç« ï¼šèµ„æ ¼è¿¹",
                "title_en": "Chapter 12: Eligibility Traces",
                "modules": {
                    "lambda_return.py": "Î»-å›æŠ¥",
                    "td_lambda.py": "TD(Î»)ç®—æ³•",
                    "control_traces.py": "æ§åˆ¶ç®—æ³•çš„èµ„æ ¼è¿¹"
                },
                "key_concepts": [
                    "èµ„æ ¼è¿¹ Eligibility Traces e(s)",
                    "Î»-å›æŠ¥ Î»-return G^Î»_t",
                    "å‰å‘è§†è§’ Forward View",
                    "åå‘è§†è§’ Backward View",
                    "TD(Î»)ç®—æ³•",
                    "True Online TD(Î»)"
                ],
                "learning_goal": "ç»Ÿä¸€TDå’ŒMCçš„è§†è§’"
            },
            
            "13_policy_gradient": {
                "title": "ç¬¬13ç« ï¼šç­–ç•¥æ¢¯åº¦æ–¹æ³•",
                "title_en": "Chapter 13: Policy Gradient Methods",
                "modules": {
                    "policy_gradient_theorem.py": "ç­–ç•¥æ¢¯åº¦å®šç†",
                    "reinforce.py": "REINFORCEç®—æ³•",
                    "actor_critic.py": "Actor-Criticæ–¹æ³•",
                    "natural_policy_gradient.py": "è‡ªç„¶ç­–ç•¥æ¢¯åº¦"
                },
                "key_concepts": [
                    "ç­–ç•¥æ¢¯åº¦å®šç† Policy Gradient Theorem",
                    "REINFORCEç®—æ³•",
                    "åŸºçº¿ Baseline",
                    "Actor-Criticæ¶æ„",
                    "è‡ªç„¶æ¢¯åº¦ Natural Gradient",
                    "TRPO/PPOç®—æ³•"
                ],
                "learning_goal": "ç›´æ¥ä¼˜åŒ–ç­–ç•¥å‚æ•°"
            }
        }
    
    def show_table_of_contents(self):
        """æ˜¾ç¤ºå®Œæ•´çš„æ•™ç§‘ä¹¦ç›®å½•"""
        print("="*80)
        print("ğŸ“š Sutton & Bartoã€Šå¼ºåŒ–å­¦ä¹ ã€‹æ•™ç§‘ä¹¦å¼ä»£ç å®ç° - å®Œæ•´ç›®å½•")
        print("ğŸ“š Complete Table of Contents - Textbook-Style Implementation")
        print("="*80)
        
        for chapter_dir, info in self.chapters.items():
            print(f"\n{'='*60}")
            print(f"ğŸ“– {info['title']}")
            print(f"   {info['title_en']}")
            print(f"{'='*60}")
            
            print("\nğŸ“‚ æ¨¡å—æ–‡ä»¶ Modules:")
            for module, description in info['modules'].items():
                print(f"   â€¢ {module:30} - {description}")
            
            print("\nğŸ¯ æ ¸å¿ƒæ¦‚å¿µ Key Concepts:")
            for concept in info['key_concepts']:
                print(f"   âœ“ {concept}")
            
            print(f"\nğŸ’¡ å­¦ä¹ ç›®æ ‡: {info['learning_goal']}")
            print(f"   Learning Goal: {info['learning_goal']}")
    
    def show_learning_paths(self):
        """æ˜¾ç¤ºä¸åŒçš„å­¦ä¹ è·¯å¾„"""
        print("\n" + "="*80)
        print("ğŸ›¤ï¸ æ¨èå­¦ä¹ è·¯å¾„ Recommended Learning Paths")
        print("="*80)
        
        paths = {
            "ğŸ“ åˆå­¦è€…è·¯å¾„ Beginner Path": [
                "00_preface â†’ ç†è§£ä»€ä¹ˆæ˜¯å¼ºåŒ–å­¦ä¹ ",
                "01_introduction â†’ æŒæ¡åŸºæœ¬æ¦‚å¿µ",
                "02_multi_armed_bandits â†’ æ¢ç´¢vsåˆ©ç”¨",
                "03_finite_mdp â†’ æ•°å­¦æ¡†æ¶",
                "04_dynamic_programming â†’ ç†æƒ³æƒ…å†µè§£æ³•",
                "06_temporal_difference â†’ å®ç”¨ç®—æ³•"
            ],
            
            "âš¡ å¿«é€Ÿå®è·µè·¯å¾„ Fast Track": [
                "00_preface â†’ å¿«é€Ÿäº†è§£",
                "02_multi_armed_bandits â†’ ç®€å•é—®é¢˜",
                "06_temporal_difference â†’ Q-learning",
                "09_on_policy_approximation â†’ å‡½æ•°è¿‘ä¼¼",
                "13_policy_gradient â†’ ç°ä»£æ–¹æ³•"
            ],
            
            "ğŸ”¬ ç ”ç©¶è€…è·¯å¾„ Researcher Path": [
                "æŒ‰é¡ºåºå­¦ä¹ æ‰€æœ‰ç« èŠ‚",
                "é‡ç‚¹ç†è§£æ•°å­¦æ¨å¯¼",
                "å®ç°æ‰€æœ‰ç®—æ³•å˜ä½“",
                "å®Œæˆä¹¦ä¸­ç»ƒä¹ é¢˜",
                "å¯¹æ¯”ä¸åŒç®—æ³•æ€§èƒ½"
            ],
            
            "ğŸ’¼ å·¥ç¨‹å¸ˆè·¯å¾„ Engineer Path": [
                "00_preface â†’ æ¦‚è§ˆ",
                "03_finite_mdp â†’ é—®é¢˜å»ºæ¨¡",
                "06_temporal_difference â†’ DQNåŸºç¡€",
                "08_planning_and_learning â†’ MCTS",
                "13_policy_gradient â†’ PPO/A3C"
            ]
        }
        
        for path_name, steps in paths.items():
            print(f"\n{path_name}:")
            for i, step in enumerate(steps, 1):
                print(f"  {i}. {step}")
    
    def run_chapter_demo(self, chapter_number: int):
        """è¿è¡ŒæŒ‡å®šç« èŠ‚çš„æ¼”ç¤º"""
        chapter_map = {
            0: "00_preface",
            1: "01_introduction",
            2: "02_multi_armed_bandits",
            3: "03_finite_mdp",
            4: "04_dynamic_programming",
            5: "05_monte_carlo",
            6: "06_temporal_difference",
            7: "07_n_step_bootstrapping",
            8: "08_planning_and_learning",
            9: "09_on_policy_approximation",
            10: "10_on_policy_control_approximation",
            11: "11_off_policy_approximation",
            12: "12_eligibility_traces",
            13: "13_policy_gradient"
        }
        
        if chapter_number not in chapter_map:
            print(f"ç« èŠ‚ {chapter_number} ä¸å­˜åœ¨")
            return
        
        chapter_dir = chapter_map[chapter_number]
        chapter_info = self.chapters[chapter_dir]
        
        print(f"\nè¿è¡Œç« èŠ‚æ¼”ç¤º: {chapter_info['title']}")
        print(f"Running Chapter Demo: {chapter_info['title_en']}")
        print("="*60)
        
        # è¿™é‡Œå¯ä»¥å¯¼å…¥å¹¶è¿è¡Œç›¸åº”ç« èŠ‚çš„æ¼”ç¤ºä»£ç 
        if chapter_number == 0:
            from src.preface import run_preface_demonstrations
            run_preface_demonstrations()
        elif chapter_number == 2:
            from src.ch02_multi_armed_bandits.epsilon_greedy import demonstrate_epsilon_greedy
            demonstrate_epsilon_greedy()
        # ... å…¶ä»–ç« èŠ‚ç±»ä¼¼
        
    def show_implementation_status(self):
        """æ˜¾ç¤ºå„ç« èŠ‚çš„å®ç°çŠ¶æ€"""
        print("\n" + "="*80)
        print("ğŸ“Š å®ç°çŠ¶æ€ Implementation Status")
        print("="*80)
        
        status = {
            "00_preface": "âœ… å®Œæˆ Complete",
            "01_introduction": "âœ… å®Œæˆ Complete",
            "02_multi_armed_bandits": "âœ… å®Œæˆ Complete (æ•™ç§‘ä¹¦å¼é‡æ„)",
            "03_finite_mdp": "âœ… å®Œæˆ Complete",
            "04_dynamic_programming": "âœ… å®Œæˆ Complete",
            "05_monte_carlo": "âœ… å®Œæˆ Complete",
            "06_temporal_difference": "âœ… å®Œæˆ Complete",
            "07_n_step_bootstrapping": "âœ… å®Œæˆ Complete",
            "08_planning_and_learning": "âœ… å®Œæˆ Complete",
            "09_on_policy_approximation": "âœ… å®Œæˆ Complete",
            "10_on_policy_control_approximation": "âœ… å®Œæˆ Complete",
            "11_off_policy_approximation": "âœ… å®Œæˆ Complete",
            "12_eligibility_traces": "âœ… å®Œæˆ Complete",
            "13_policy_gradient": "âœ… å®Œæˆ Complete"
        }
        
        for chapter, stat in status.items():
            chapter_info = self.chapters[chapter]
            print(f"{chapter:30} {stat:20} - {chapter_info['title']}")
        
        print(f"\næ€»ä½“å®Œæˆåº¦: 13/13 ç« èŠ‚ (100%)")
        print(f"Overall Progress: 13/13 chapters (100%)")


def main():
    """ä¸»ç¨‹åºå…¥å£"""
    print("â•”" + "â•"*78 + "â•—")
    print("â•‘" + " "*10 + "Sutton & Bartoã€Šå¼ºåŒ–å­¦ä¹ ã€‹æ•™ç§‘ä¹¦å¼ä»£ç å®ç°".center(58) + " "*10 + "â•‘")
    print("â•‘" + " "*15 + "Textbook-Style RL Implementation Navigator".center(48) + " "*15 + "â•‘")
    print("â•š" + "â•"*78 + "â•")
    
    textbook = RLTextbook()
    
    while True:
        print("\n" + "="*60)
        print("è¯·é€‰æ‹©æ“ä½œ Select Operation:")
        print("="*60)
        print("1. ğŸ“š æŸ¥çœ‹å®Œæ•´ç›®å½• (View Table of Contents)")
        print("2. ğŸ›¤ï¸ æŸ¥çœ‹å­¦ä¹ è·¯å¾„ (View Learning Paths)")
        print("3. ğŸ“Š æŸ¥çœ‹å®ç°çŠ¶æ€ (View Implementation Status)")
        print("4. â–¶ï¸ è¿è¡Œç« èŠ‚æ¼”ç¤º (Run Chapter Demo)")
        print("5. âŒ é€€å‡º (Exit)")
        
        choice = input("\nè¯·è¾“å…¥é€‰æ‹© (1-5): ").strip()
        
        if choice == '1':
            textbook.show_table_of_contents()
        elif choice == '2':
            textbook.show_learning_paths()
        elif choice == '3':
            textbook.show_implementation_status()
        elif choice == '4':
            chapter = input("è¯·è¾“å…¥ç« èŠ‚å· (0-13): ").strip()
            if chapter.isdigit():
                textbook.run_chapter_demo(int(chapter))
            else:
                print("æ— æ•ˆçš„ç« èŠ‚å·")
        elif choice == '5':
            print("\næ„Ÿè°¢ä½¿ç”¨ï¼ç¥å­¦ä¹ æ„‰å¿«ï¼")
            print("Thank you! Happy learning!")
            break
        else:
            print("æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡è¯•")


if __name__ == "__main__":
    main()