"""
================================================================================
ç¬¬3.1èŠ‚ï¼šåŠ¨æ€è§„åˆ’åŸºç¡€ - RLç®—æ³•çš„ç†è®ºæ ¹åŸº
Section 3.1: Dynamic Programming Foundations - Theoretical Basis of RL Algorithms
================================================================================

åŠ¨æ€è§„åˆ’(Dynamic Programming, DP)è¿™ä¸ªåå­—å¬èµ·æ¥å¾ˆé…·ï¼Œä½†å®ƒçš„æ ¸å¿ƒæ€æƒ³å…¶å®å¾ˆç®€å•ï¼š
The name "Dynamic Programming" sounds cool, but its core idea is actually simple:
å°†å¤æ‚é—®é¢˜åˆ†è§£æˆæ›´ç®€å•çš„å­é—®é¢˜ï¼Œç„¶åç»„åˆå­é—®é¢˜çš„è§£æ¥å¾—åˆ°åŸé—®é¢˜çš„è§£ã€‚
Break complex problems into simpler subproblems, then combine solutions to get the original solution.

åœ¨RLä¸­ï¼ŒDPåˆ©ç”¨äº†ä»·å€¼å‡½æ•°çš„é€’å½’ç»“æ„ï¼ˆè´å°”æ›¼æ–¹ç¨‹ï¼‰æ¥å¯»æ‰¾æœ€ä¼˜ç­–ç•¥ã€‚
In RL, DP exploits the recursive structure of value functions (Bellman equations) to find optimal policies.

ä¸ºä»€ä¹ˆå«"åŠ¨æ€è§„åˆ’"ï¼Ÿ
Why is it called "Dynamic Programming"?
- "åŠ¨æ€"ï¼šé—®é¢˜å…·æœ‰æ—¶åºç»“æ„ï¼Œéœ€è¦åšåºåˆ—å†³ç­–
  "Dynamic": Problems have temporal structure, requiring sequential decisions
- "è§„åˆ’"ï¼šé€šè¿‡è®¡ç®—æ¥ä¼˜åŒ–å†³ç­–
  "Programming": Optimize decisions through computation

å†å²è¶£é—»ï¼šRichard Bellmanåœ¨1950å¹´ä»£åˆ›é€ è¿™ä¸ªæœ¯è¯­æ—¶ï¼Œæ•…æ„é€‰äº†ä¸€ä¸ª
å¬èµ·æ¥å¾ˆå‰å®³ä½†åˆæ¨¡ç³Šçš„åå­—ï¼Œä»¥é¿å…ä»–çš„ç ”ç©¶è¢«å½“æ—¶çš„å›½é˜²éƒ¨é•¿å¦å†³ã€‚
Historical note: Richard Bellman coined this term in the 1950s, deliberately choosing
an impressive but vague name to avoid his research being rejected by the Secretary of Defense.
"""

import numpy as np
from typing import Dict, List, Tuple, Optional, Callable, Any
from dataclasses import dataclass
import logging
from abc import ABC, abstractmethod
import matplotlib.pyplot as plt
import seaborn as sns
from collections import defaultdict

# å¯¼å…¥ç¬¬2ç« çš„ç»„ä»¶
import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent))

from ch02_mdp.mdp_framework import (
    State, Action, MDPEnvironment,
    TransitionProbability, RewardFunction
)
from ch02_mdp.policies_and_values import (
    Policy, StateValueFunction, ActionValueFunction,
    DeterministicPolicy, StochasticPolicy
)

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


# ================================================================================
# ç¬¬3.1.1èŠ‚ï¼šåŠ¨æ€è§„åˆ’çš„æ ¸å¿ƒæ€æƒ³
# Section 3.1.1: Core Ideas of Dynamic Programming
# ================================================================================

class DynamicProgrammingFoundations:
    """
    åŠ¨æ€è§„åˆ’åŸºç¡€ç†è®º
    Dynamic Programming Foundations
    
    DPçš„ä¸¤ä¸ªå…³é”®æ€§è´¨ï¼š
    Two key properties for DP:
    
    1. æœ€ä¼˜å­ç»“æ„(Optimal Substructure)
       - æœ€ä¼˜è§£å¯ä»¥é€šè¿‡å­é—®é¢˜çš„æœ€ä¼˜è§£æ„é€ 
       - Optimal solution can be constructed from optimal solutions of subproblems
       - åœ¨RLä¸­ï¼šæœ€ä¼˜ç­–ç•¥çš„ä»·å€¼å‡½æ•°æ»¡è¶³è´å°”æ›¼æœ€ä¼˜æ–¹ç¨‹
       - In RL: Value function of optimal policy satisfies Bellman optimality equation
    
    2. é‡å å­é—®é¢˜(Overlapping Subproblems)
       - å­é—®é¢˜ä¼šè¢«å¤šæ¬¡æ±‚è§£
       - Subproblems are solved multiple times
       - åœ¨RLä¸­ï¼šåŒä¸€ä¸ªçŠ¶æ€ä¼šè¢«å¤šæ¬¡è®¿é—®å’Œæ›´æ–°
       - In RL: Same state is visited and updated multiple times
    
    DP vs å…¶ä»–æ–¹æ³•ï¼š
    DP vs Other Methods:
    
    | æ–¹æ³• Method | éœ€è¦æ¨¡å‹? Need Model? | è®¡ç®—æ–¹å¼ Computation | é€‚ç”¨åœºæ™¯ Use Case |
    |-------------|---------------------|-------------------|-----------------|
    | DP | æ˜¯ Yes | å…¨å®½åº¦æ›´æ–° Full-width | å°çŠ¶æ€ç©ºé—´ Small state space |
    | MC | å¦ No | é‡‡æ · Sampling | å›åˆå¼ä»»åŠ¡ Episodic tasks |
    | TD | å¦ No | è‡ªä¸¾ Bootstrapping | åœ¨çº¿å­¦ä¹  Online learning |
    """
    
    @staticmethod
    def explain_dp_principles():
        """
        è¯¦ç»†è§£é‡ŠDPåŸç†
        Detailed Explanation of DP Principles
        
        è¿™æ˜¯ä¸€ä¸ªæ•™å­¦å‡½æ•°ï¼Œé€šè¿‡å…·ä½“ä¾‹å­å¸®åŠ©ç†è§£DP
        This is a teaching function that helps understand DP through concrete examples
        """
        print("\n" + "="*80)
        print("åŠ¨æ€è§„åˆ’æ ¸å¿ƒåŸç†")
        print("Core Principles of Dynamic Programming")
        print("="*80)
        
        print("""
        ğŸ“š 1. ä»€ä¹ˆæ˜¯åŠ¨æ€è§„åˆ’ï¼Ÿ
        What is Dynamic Programming?
        ================================
        
        æƒ³è±¡ä½ è¦ä»å®¶åˆ°å…¬å¸ï¼Œæœ‰å¤šæ¡è·¯çº¿å¯é€‰ï¼š
        Imagine going from home to office with multiple route options:
        
        å®¶ Home â”€â”€â”
                 â”œâ”€â†’ è·¯å£A Junction A â”€â”
                 â”‚                    â”œâ”€â†’ å…¬å¸ Office
                 â””â”€â†’ è·¯å£B Junction B â”€â”˜
        
        åŠ¨æ€è§„åˆ’çš„æ€è·¯ï¼š
        DP approach:
        1. å…ˆè®¡ç®—ä»å„è·¯å£åˆ°å…¬å¸çš„æœ€çŸ­æ—¶é—´
           First calculate shortest time from each junction to office
        2. ç„¶åé€‰æ‹©ï¼šå®¶åˆ°å“ªä¸ªè·¯å£ + è¯¥è·¯å£åˆ°å…¬å¸çš„æ—¶é—´æœ€çŸ­
           Then choose: home to which junction + that junction to office is shortest
        
        è¿™å°±æ˜¯"æœ€ä¼˜å­ç»“æ„"ï¼šæ•´ä½“æœ€ä¼˜è§£åŒ…å«å­é—®é¢˜çš„æœ€ä¼˜è§£
        This is "optimal substructure": overall optimal solution contains optimal solutions of subproblems
        
        ğŸ“š 2. DPåœ¨RLä¸­çš„åº”ç”¨
        DP in RL
        ================================
        
        è´å°”æ›¼æ–¹ç¨‹å°±æ˜¯DPçš„é€’å½’å…³ç³»ï¼š
        Bellman equation is the recursive relation of DP:
        
        v(s) = max_a [r(s,a) + Î³ Î£_s' p(s'|s,a) v(s')]
               â†‘                    â†‘
               å½“å‰å¥–åŠ±              æœªæ¥ä»·å€¼
               immediate reward     future value
        
        è¿™ä¸ªæ–¹ç¨‹è¯´æ˜ï¼š
        This equation shows:
        - ä¸€ä¸ªçŠ¶æ€çš„ä»·å€¼ = ç«‹å³å¥–åŠ± + æŠ˜æ‰£çš„æœªæ¥ä»·å€¼
          Value of a state = immediate reward + discounted future value
        - è¿™æ˜¯ä¸€ä¸ªé€’å½’å®šä¹‰ï¼Œå¯ä»¥ç”¨DPæ±‚è§£
          This is a recursive definition, solvable by DP
        
        ğŸ“š 3. DPçš„è®¡ç®—æ¨¡å¼
        Computation Pattern of DP
        ================================
        
        åŒæ­¥æ›´æ–° Synchronous Update:
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ V_k(s1)  â”‚      â”‚ V_{k+1}  â”‚
        â”‚ V_k(s2)  â”‚ â”€â”€â”€â†’ â”‚   (s1)   â”‚
        â”‚ V_k(s3)  â”‚      â”‚ V_{k+1}  â”‚
        â”‚   ...    â”‚      â”‚   (s2)   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        ä½¿ç”¨æ‰€æœ‰V_k        è®¡ç®—æ‰€æœ‰V_{k+1}
        Use all V_k       Compute all V_{k+1}
        
        å¼‚æ­¥æ›´æ–° Asynchronous Update:
        éšæ—¶æ›´æ–°ä»»æ„çŠ¶æ€ï¼Œæ›´çµæ´»ä½†æ”¶æ•›æ€§åˆ†ææ›´å¤æ‚
        Update any state at any time, more flexible but convergence analysis is complex
        
        ğŸ“š 4. DPçš„ä¼˜åŠ¿ä¸å±€é™
        Advantages and Limitations of DP
        ================================
        
        ä¼˜åŠ¿ Advantages:
        âœ“ æ•°å­¦ä¼˜é›…ï¼Œç†è®ºä¿è¯æ”¶æ•›åˆ°æœ€ä¼˜
          Mathematically elegant, guaranteed to converge to optimal
        âœ“ å……åˆ†åˆ©ç”¨æ¨¡å‹ä¿¡æ¯ï¼Œæ ·æœ¬æ•ˆç‡é«˜
          Fully utilizes model information, high sample efficiency
        âœ“ å¯ä»¥ç¦»çº¿è®¡ç®—ï¼Œä¸éœ€è¦ä¸ç¯å¢ƒäº¤äº’
          Can compute offline, no environment interaction needed
        
        å±€é™ Limitations:
        âœ— éœ€è¦å®Œæ•´çš„ç¯å¢ƒæ¨¡å‹ï¼ˆè½¬ç§»æ¦‚ç‡å’Œå¥–åŠ±å‡½æ•°ï¼‰
          Requires complete environment model (transition probabilities and reward function)
        âœ— è®¡ç®—å¤æ‚åº¦é«˜ï¼šO(|S|Â²|A|) per iteration
          High computational complexity: O(|S|Â²|A|) per iteration
        âœ— ç»´åº¦è¯…å’’ï¼šçŠ¶æ€ç©ºé—´å¤§æ—¶ä¸å¯è¡Œ
          Curse of dimensionality: infeasible for large state spaces
        
        è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬éœ€è¦MCå’ŒTDæ–¹æ³•ï¼
        This is why we need MC and TD methods!
        """)
        
        # åˆ›å»ºå¯è§†åŒ–
        DynamicProgrammingFoundations._visualize_dp_concept()
    
    @staticmethod
    def _visualize_dp_concept():
        """
        å¯è§†åŒ–DPæ¦‚å¿µ
        Visualize DP Concepts
        """
        fig, axes = plt.subplots(1, 3, figsize=(15, 5))
        
        # å›¾1ï¼šæœ€ä¼˜å­ç»“æ„
        ax1 = axes[0]
        ax1.set_title("Optimal Substructure / æœ€ä¼˜å­ç»“æ„")
        
        # ç»˜åˆ¶æ ‘å½¢ç»“æ„
        positions = {
            'root': (0.5, 0.8),
            'left': (0.3, 0.5),
            'right': (0.7, 0.5),
            'leaf1': (0.2, 0.2),
            'leaf2': (0.4, 0.2),
            'leaf3': (0.6, 0.2),
            'leaf4': (0.8, 0.2)
        }
        
        # ç”»èŠ‚ç‚¹
        for node, (x, y) in positions.items():
            circle = plt.Circle((x, y), 0.05, color='lightblue', ec='black')
            ax1.add_patch(circle)
            if node == 'root':
                ax1.text(x, y, 'v(s)', ha='center', va='center', fontweight='bold')
            else:
                ax1.text(x, y, 'v', ha='center', va='center')
        
        # ç”»è¾¹
        edges = [
            ('root', 'left'), ('root', 'right'),
            ('left', 'leaf1'), ('left', 'leaf2'),
            ('right', 'leaf3'), ('right', 'leaf4')
        ]
        for start, end in edges:
            x1, y1 = positions[start]
            x2, y2 = positions[end]
            ax1.plot([x1, x2], [y1, y2], 'k-', alpha=0.5)
        
        ax1.set_xlim(0, 1)
        ax1.set_ylim(0, 1)
        ax1.axis('off')
        ax1.text(0.5, 0.05, "æœ€ä¼˜è§£ä¾èµ–å­é—®é¢˜æœ€ä¼˜è§£\nOptimal depends on suboptimal", 
                ha='center', fontsize=10)
        
        # å›¾2ï¼šè¿­ä»£è¿‡ç¨‹
        ax2 = axes[1]
        ax2.set_title("Iterative Process / è¿­ä»£è¿‡ç¨‹")
        
        # æ¨¡æ‹Ÿä»·å€¼å‡½æ•°æ”¶æ•›
        iterations = 20
        x = np.arange(iterations)
        
        # ä¸åŒçŠ¶æ€çš„ä»·å€¼æ”¶æ•›æ›²çº¿
        np.random.seed(42)
        for i in range(3):
            true_value = np.random.uniform(5, 10)
            values = [0]
            for t in range(1, iterations):
                # æ¨¡æ‹Ÿæ”¶æ•›è¿‡ç¨‹
                values.append(true_value * (1 - np.exp(-0.3 * t)) + np.random.normal(0, 0.1))
            ax2.plot(x, values, marker='o', markersize=3, label=f'State {i+1}')
            ax2.axhline(y=true_value, color='gray', linestyle='--', alpha=0.3)
        
        ax2.set_xlabel('Iteration / è¿­ä»£æ¬¡æ•°')
        ax2.set_ylabel('Value / ä»·å€¼')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        ax2.text(10, 2, "ä»·å€¼å‡½æ•°é€æ¸æ”¶æ•›\nValues converge gradually", 
                ha='center', fontsize=10, bbox=dict(boxstyle="round,pad=0.3", facecolor="yellow", alpha=0.3))
        
        # å›¾3ï¼šåŒæ­¥æ›´æ–°ç¤ºæ„
        ax3 = axes[2]
        ax3.set_title("Synchronous Update / åŒæ­¥æ›´æ–°")
        
        # åˆ›å»ºç½‘æ ¼è¡¨ç¤ºçŠ¶æ€
        grid_size = 4
        old_values = np.random.rand(grid_size, grid_size) * 5
        new_values = old_values * 1.2 + np.random.rand(grid_size, grid_size)
        
        # æ˜¾ç¤ºæ—§å€¼
        im1 = ax3.imshow(old_values, cmap='coolwarm', alpha=0.5, extent=[0, 2, 0, 2])
        
        # æ·»åŠ ç®­å¤´
        ax3.arrow(2.2, 1, 0.6, 0, head_width=0.1, head_length=0.1, fc='black', ec='black')
        
        # æ˜¾ç¤ºæ–°å€¼
        im2 = ax3.imshow(new_values, cmap='coolwarm', alpha=0.5, extent=[3, 5, 0, 2])
        
        ax3.text(1, -0.3, "V_k", ha='center', fontsize=12, fontweight='bold')
        ax3.text(4, -0.3, "V_{k+1}", ha='center', fontsize=12, fontweight='bold')
        ax3.text(2.5, 1, "Bellman\nUpdate", ha='center', va='center', fontsize=10)
        
        ax3.set_xlim(-0.5, 5.5)
        ax3.set_ylim(-0.5, 2.5)
        ax3.axis('off')
        
        # æ·»åŠ é¢œè‰²æ¡
        plt.colorbar(im2, ax=ax3, orientation='horizontal', pad=0.1, fraction=0.05)
        
        plt.suptitle("Dynamic Programming Concepts / åŠ¨æ€è§„åˆ’æ¦‚å¿µ", fontsize=14, fontweight='bold')
        plt.tight_layout()
        return fig


# ================================================================================
# ç¬¬3.1.2èŠ‚ï¼šè´å°”æ›¼ç®—å­
# Section 3.1.2: Bellman Operators
# ================================================================================

class BellmanOperator:
    """
    è´å°”æ›¼ç®—å­ - DPç®—æ³•çš„æ•°å­¦æ ¸å¿ƒ
    Bellman Operators - Mathematical Core of DP Algorithms
    
    è´å°”æ›¼ç®—å­æ˜¯ä½œç”¨åœ¨ä»·å€¼å‡½æ•°ä¸Šçš„æ˜ å°„ï¼Œå°†ä¸€ä¸ªä»·å€¼å‡½æ•°æ˜ å°„åˆ°å¦ä¸€ä¸ªä»·å€¼å‡½æ•°ã€‚
    Bellman operators are mappings on value functions, mapping one value function to another.
    
    ä¸ºä»€ä¹ˆé‡è¦ï¼Ÿ
    Why important?
    1. DPç®—æ³•æœ¬è´¨ä¸Šæ˜¯åå¤åº”ç”¨è´å°”æ›¼ç®—å­
       DP algorithms essentially apply Bellman operators repeatedly
    2. ç®—å­çš„æ€§è´¨ï¼ˆå¦‚æ”¶ç¼©æ€§ï¼‰ä¿è¯äº†ç®—æ³•æ”¶æ•›
       Properties of operators (like contraction) guarantee algorithm convergence
    3. ä¸åŠ¨ç‚¹å°±æ˜¯æˆ‘ä»¬è¦æ‰¾çš„è§£
       Fixed point is the solution we're looking for
    
    æ•°å­¦è¡¨ç¤ºï¼š
    Mathematical representation:
    
    è´å°”æ›¼æœŸæœ›ç®—å­ Bellman Expectation Operator:
    T^Ï€(v)(s) = Î£_a Ï€(a|s) Î£_{s',r} p(s',r|s,a)[r + Î³v(s')]
    
    è´å°”æ›¼æœ€ä¼˜ç®—å­ Bellman Optimality Operator:
    T*(v)(s) = max_a Î£_{s',r} p(s',r|s,a)[r + Î³v(s')]
    
    å…³é”®æ€§è´¨ Key Properties:
    1. å•è°ƒæ€§(Monotonicity): v â‰¤ w âŸ¹ Tv â‰¤ Tw
    2. æ”¶ç¼©æ€§(Contraction): ||Tv - Tw||âˆ â‰¤ Î³||v - w||âˆ
    3. å”¯ä¸€ä¸åŠ¨ç‚¹(Unique Fixed Point): v* = Tv*
    """
    
    def __init__(self, mdp_env: MDPEnvironment, gamma: float = 0.99):
        """
        åˆå§‹åŒ–è´å°”æ›¼ç®—å­
        Initialize Bellman Operator
        
        Args:
            mdp_env: MDPç¯å¢ƒï¼ˆéœ€è¦çŸ¥é“æ¨¡å‹ï¼‰
            gamma: æŠ˜æ‰£å› å­
        
        è®¾è®¡æ€è€ƒï¼š
        Design Consideration:
        æˆ‘ä»¬æŠŠç®—å­è®¾è®¡æˆç±»ï¼Œæ˜¯å› ä¸ºå®ƒéœ€è¦è®°ä½MDPçš„å‚æ•°ï¼ˆP, R, Î³ï¼‰
        We design operator as a class because it needs to remember MDP parameters
        """
        self.env = mdp_env
        self.gamma = gamma
        self.P, self.R = mdp_env.get_dynamics()
        
        logger.info(f"åˆå§‹åŒ–è´å°”æ›¼ç®—å­ï¼ŒÎ³={gamma}")
    
    def bellman_expectation_operator(self, 
                                    v: StateValueFunction,
                                    policy: Policy) -> StateValueFunction:
        """
        è´å°”æ›¼æœŸæœ›ç®—å­ T^Ï€
        Bellman Expectation Operator T^Ï€
        
        è¿™ä¸ªç®—å­ç”¨äºç­–ç•¥è¯„ä¼°ï¼šç»™å®šç­–ç•¥Ï€ï¼Œè®¡ç®—å…¶ä»·å€¼å‡½æ•°
        This operator is used for policy evaluation: given policy Ï€, compute its value function
        
        T^Ï€(v)(s) = Î£_a Ï€(a|s) Î£_{s',r} p(s',r|s,a)[r + Î³v(s')]
        
        ä¸ºä»€ä¹ˆå«"æœŸæœ›"ç®—å­ï¼Ÿ
        Why called "expectation" operator?
        å› ä¸ºå®ƒè®¡ç®—çš„æ˜¯éµå¾ªç­–ç•¥Ï€çš„æœŸæœ›å›æŠ¥
        Because it computes expected return following policy Ï€
        
        Args:
            v: å½“å‰ä»·å€¼å‡½æ•°ä¼°è®¡
               Current value function estimate
            policy: ç­–ç•¥Ï€
                   Policy Ï€
        
        Returns:
            æ–°çš„ä»·å€¼å‡½æ•° T^Ï€(v)
            New value function T^Ï€(v)
        
        æ—¶é—´å¤æ‚åº¦ Time Complexity: O(|S|Â²|A|)
        ç©ºé—´å¤æ‚åº¦ Space Complexity: O(|S|)
        """
        # åˆ›å»ºæ–°çš„ä»·å€¼å‡½æ•°
        v_new = StateValueFunction(self.env.state_space, initial_value=0.0)
        
        # å¯¹æ¯ä¸ªçŠ¶æ€åº”ç”¨ç®—å­
        for state in self.env.state_space:
            if state.is_terminal:
                # ç»ˆæ­¢çŠ¶æ€çš„ä»·å€¼ä¸º0
                # Terminal state has value 0
                v_new.set_value(state, 0.0)
                continue
            
            # è®¡ç®—æœŸæœ›ä»·å€¼
            # Compute expected value
            value = 0.0
            
            # è·å–ç­–ç•¥åœ¨è¯¥çŠ¶æ€çš„åŠ¨ä½œåˆ†å¸ƒ
            action_probs = policy.get_action_probabilities(state)
            
            for action, action_prob in action_probs.items():
                # è®¡ç®—é€‰æ‹©è¯¥åŠ¨ä½œçš„ä»·å€¼
                q_value = self._compute_q_value(state, action, v)
                
                # ç”¨ç­–ç•¥æ¦‚ç‡åŠ æƒ
                value += action_prob * q_value
            
            v_new.set_value(state, value)
            
            logger.debug(f"T^Ï€(v)({state.id}) = {value:.3f}")
        
        return v_new
    
    def bellman_optimality_operator(self, 
                                   v: StateValueFunction) -> StateValueFunction:
        """
        è´å°”æ›¼æœ€ä¼˜ç®—å­ T*
        Bellman Optimality Operator T*
        
        è¿™ä¸ªç®—å­ç”¨äºå¯»æ‰¾æœ€ä¼˜ä»·å€¼å‡½æ•°
        This operator is used to find optimal value function
        
        T*(v)(s) = max_a Î£_{s',r} p(s',r|s,a)[r + Î³v(s')]
        
        ä¸ºä»€ä¹ˆå«"æœ€ä¼˜"ç®—å­ï¼Ÿ
        Why called "optimality" operator?
        å› ä¸ºå®ƒæ€»æ˜¯é€‰æ‹©æœ€å¥½çš„åŠ¨ä½œï¼ˆè´ªå©ªï¼‰
        Because it always chooses the best action (greedy)
        
        Args:
            v: å½“å‰ä»·å€¼å‡½æ•°ä¼°è®¡
               Current value function estimate
        
        Returns:
            æ–°çš„ä»·å€¼å‡½æ•° T*(v)
            New value function T*(v)
        
        æ³¨æ„ï¼šè¿™ä¸ªç®—å­çš„ä¸åŠ¨ç‚¹å°±æ˜¯æœ€ä¼˜ä»·å€¼å‡½æ•°v*ï¼
        Note: The fixed point of this operator is the optimal value function v*!
        """
        v_new = StateValueFunction(self.env.state_space, initial_value=0.0)
        
        for state in self.env.state_space:
            if state.is_terminal:
                v_new.set_value(state, 0.0)
                continue
            
            # æ‰¾æœ€å¤§ä»·å€¼
            # Find maximum value
            max_value = float('-inf')
            
            for action in self.env.action_space:
                q_value = self._compute_q_value(state, action, v)
                max_value = max(max_value, q_value)
            
            v_new.set_value(state, max_value)
            
            logger.debug(f"T*(v)({state.id}) = {max_value:.3f}")
        
        return v_new
    
    def _compute_q_value(self, state: State, action: Action, 
                        v: StateValueFunction) -> float:
        """
        è®¡ç®—Qå€¼ï¼šq(s,a) = Î£_{s',r} p(s',r|s,a)[r + Î³v(s')]
        Compute Q-value: q(s,a) = Î£_{s',r} p(s',r|s,a)[r + Î³v(s')]
        
        è¿™æ˜¯è´å°”æ›¼ç®—å­çš„æ ¸å¿ƒè®¡ç®—
        This is the core computation of Bellman operators
        
        Args:
            state: çŠ¶æ€s
            action: åŠ¨ä½œa
            v: ä»·å€¼å‡½æ•°
        
        Returns:
            åŠ¨ä½œä»·å€¼q(s,a)
        """
        q_value = 0.0
        
        # è·å–æ‰€æœ‰å¯èƒ½çš„è½¬ç§»
        transitions = self.P.get_transitions(state, action)
        
        for next_state, reward, prob in transitions:
            # è´å°”æ›¼æ–¹ç¨‹çš„æ ¸å¿ƒï¼šç«‹å³å¥–åŠ± + æŠ˜æ‰£çš„æœªæ¥ä»·å€¼
            # Core of Bellman equation: immediate reward + discounted future value
            q_value += prob * (reward + self.gamma * v.get_value(next_state))
        
        return q_value
    
    def verify_contraction_property(self, v1: StateValueFunction, 
                                   v2: StateValueFunction) -> float:
        """
        éªŒè¯æ”¶ç¼©æ€§è´¨
        Verify Contraction Property
        
        æ”¶ç¼©æ˜ å°„å®šç†ä¿è¯äº†ä»·å€¼è¿­ä»£çš„æ”¶æ•›æ€§
        Contraction mapping theorem guarantees convergence of value iteration
        
        æ€§è´¨ï¼š||Tv - Tw||âˆ â‰¤ Î³||v - w||âˆ
        Property: ||Tv - Tw||âˆ â‰¤ Î³||v - w||âˆ
        
        è¿™æ„å‘³ç€æ¯æ¬¡è¿­ä»£ï¼Œè¯¯å·®è‡³å°‘ç¼©å°åˆ°åŸæ¥çš„Î³å€
        This means each iteration reduces error by at least factor Î³
        
        Args:
            v1, v2: ä¸¤ä¸ªä»·å€¼å‡½æ•°
        
        Returns:
            æ”¶ç¼©å› å­ï¼ˆåº”è¯¥â‰¤Î³ï¼‰
            Contraction factor (should be â‰¤Î³)
        """
        # è®¡ç®—åŸå§‹è·ç¦»
        original_dist = self._compute_max_norm_distance(v1, v2)
        
        # åº”ç”¨ç®—å­
        tv1 = self.bellman_optimality_operator(v1)
        tv2 = self.bellman_optimality_operator(v2)
        
        # è®¡ç®—æ–°è·ç¦»
        new_dist = self._compute_max_norm_distance(tv1, tv2)
        
        # è®¡ç®—æ”¶ç¼©å› å­
        contraction_factor = new_dist / original_dist if original_dist > 0 else 0
        
        logger.info(f"æ”¶ç¼©éªŒè¯: ||Tv-Tw||={new_dist:.4f}, ||v-w||={original_dist:.4f}, "
                   f"factor={contraction_factor:.4f} (åº”â‰¤{self.gamma})")
        
        return contraction_factor
    
    def _compute_max_norm_distance(self, v1: StateValueFunction, 
                                   v2: StateValueFunction) -> float:
        """
        è®¡ç®—æœ€å¤§èŒƒæ•°è·ç¦» ||v1 - v2||âˆ
        Compute max norm distance
        
        è¿™æ˜¯è¡¡é‡ä¸¤ä¸ªä»·å€¼å‡½æ•°å·®å¼‚çš„æ ‡å‡†æ–¹æ³•
        This is the standard way to measure difference between value functions
        """
        max_diff = 0.0
        for state in self.env.state_space:
            diff = abs(v1.get_value(state) - v2.get_value(state))
            max_diff = max(max_diff, diff)
        return max_diff


# ================================================================================
# ç¬¬3.1.3èŠ‚ï¼šç­–ç•¥è¯„ä¼°ï¼ˆé¢„æµ‹é—®é¢˜ï¼‰
# Section 3.1.3: Policy Evaluation (Prediction Problem)
# ================================================================================

class PolicyEvaluationDP:
    """
    ç­–ç•¥è¯„ä¼° - åŠ¨æ€è§„åˆ’ç‰ˆæœ¬
    Policy Evaluation - Dynamic Programming Version
    
    é—®é¢˜ï¼šç»™å®šç­–ç•¥Ï€ï¼Œè®¡ç®—å…¶ä»·å€¼å‡½æ•°v_Ï€
    Problem: Given policy Ï€, compute its value function v_Ï€
    
    è¿™æ˜¯"é¢„æµ‹é—®é¢˜"ï¼šé¢„æµ‹éµå¾ªç­–ç•¥Ï€èƒ½è·å¾—å¤šå°‘å›æŠ¥
    This is the "prediction problem": predict how much return we get following policy Ï€
    
    ç®—æ³•ï¼šè¿­ä»£åº”ç”¨è´å°”æ›¼æœŸæœ›ç®—å­
    Algorithm: Iteratively apply Bellman expectation operator
    v_{k+1} = T^Ï€(v_k)
    
    ä¸ºä»€ä¹ˆä¼šæ”¶æ•›ï¼Ÿ
    Why does it converge?
    å› ä¸ºT^Ï€æ˜¯Î³-æ”¶ç¼©æ˜ å°„ï¼Œæœ‰å”¯ä¸€ä¸åŠ¨ç‚¹v_Ï€
    Because T^Ï€ is a Î³-contraction mapping with unique fixed point v_Ï€
    
    æ”¶æ•›é€Ÿåº¦ï¼šO(Î³^k)ï¼ŒæŒ‡æ•°æ”¶æ•›ï¼
    Convergence rate: O(Î³^k), exponential convergence!
    """
    
    def __init__(self, mdp_env: MDPEnvironment, gamma: float = 0.99):
        """
        åˆå§‹åŒ–ç­–ç•¥è¯„ä¼°å™¨
        Initialize Policy Evaluator
        
        Args:
            mdp_env: MDPç¯å¢ƒ
            gamma: æŠ˜æ‰£å› å­
        """
        self.env = mdp_env
        self.gamma = gamma
        self.bellman_op = BellmanOperator(mdp_env, gamma)
        
        # è®°å½•è¯„ä¼°å†å²ï¼ˆç”¨äºå¯è§†åŒ–ï¼‰
        self.evaluation_history = []
        
        logger.info("åˆå§‹åŒ–ç­–ç•¥è¯„ä¼°å™¨(DP)")
    
    def evaluate(self, policy: Policy, 
                theta: float = 1e-6,
                max_iterations: int = 1000,
                initial_v: Optional[StateValueFunction] = None) -> StateValueFunction:
        """
        è¿­ä»£ç­–ç•¥è¯„ä¼°
        Iterative Policy Evaluation
        
        è¿™æ˜¯æœ€åŸºç¡€çš„DPç®—æ³•ï¼Œç†è§£å®ƒæ˜¯ç†è§£æ‰€æœ‰DPç®—æ³•çš„å…³é”®ï¼
        This is the most basic DP algorithm, understanding it is key to understanding all DP algorithms!
        
        ç®—æ³•æµç¨‹ï¼š
        Algorithm Flow:
        1. åˆå§‹åŒ–V(s)ä»»æ„ï¼ˆé€šå¸¸ä¸º0ï¼‰
           Initialize V(s) arbitrarily (usually 0)
        2. é‡å¤ç›´åˆ°æ”¶æ•›ï¼š
           Repeat until convergence:
           å¯¹æ¯ä¸ªçŠ¶æ€sï¼š
           For each state s:
             v(s) â† Î£_a Ï€(a|s) Î£_{s',r} p(s',r|s,a)[r + Î³V(s')]
        3. è¿”å›æ”¶æ•›çš„V
           Return converged V
        
        Args:
            policy: è¦è¯„ä¼°çš„ç­–ç•¥
                   Policy to evaluate
            theta: æ”¶æ•›é˜ˆå€¼ï¼ˆæœ€å¤§å€¼å˜åŒ–å°äºæ­¤å€¼æ—¶åœæ­¢ï¼‰
                  Convergence threshold
            max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•°
                          Maximum iterations
            initial_v: åˆå§‹ä»·å€¼å‡½æ•°ï¼ˆå¯é€‰ï¼‰
                      Initial value function (optional)
        
        Returns:
            ç­–ç•¥çš„ä»·å€¼å‡½æ•°v_Ï€
            Value function of policy v_Ï€
        
        æ•™å­¦è¦ç‚¹ï¼š
        Teaching Points:
        1. è¿™æ˜¯åŒæ­¥æ›´æ–°ï¼šæ‰€æœ‰çŠ¶æ€åŒæ—¶æ›´æ–°
           This is synchronous update: all states updated simultaneously
        2. éœ€è¦ä¸¤ä¸ªæ•°ç»„ï¼šæ—§å€¼å’Œæ–°å€¼
           Need two arrays: old values and new values
        3. æ”¶æ•›åˆ¤æ–­åŸºäºæœ€å¤§å˜åŒ–ï¼ˆæ— ç©·èŒƒæ•°ï¼‰
           Convergence based on max change (infinity norm)
        """
        # åˆå§‹åŒ–ä»·å€¼å‡½æ•°
        if initial_v is None:
            v = StateValueFunction(self.env.state_space, initial_value=0.0)
        else:
            v = initial_v
        
        # æ¸…ç©ºå†å²è®°å½•
        self.evaluation_history = []
        
        logger.info(f"å¼€å§‹ç­–ç•¥è¯„ä¼°ï¼Œtheta={theta}")
        
        # è¿­ä»£è¯„ä¼°
        for iteration in range(max_iterations):
            # è®°å½•å½“å‰ä»·å€¼å‡½æ•°ï¼ˆæ·±æ‹·è´ï¼‰
            v_snapshot = StateValueFunction(self.env.state_space)
            for state in self.env.state_space:
                v_snapshot.set_value(state, v.get_value(state))
            self.evaluation_history.append(v_snapshot)
            
            # åº”ç”¨è´å°”æ›¼æœŸæœ›ç®—å­
            v_new = self.bellman_op.bellman_expectation_operator(v, policy)
            
            # è®¡ç®—æœ€å¤§å˜åŒ–ï¼ˆåˆ¤æ–­æ”¶æ•›ï¼‰
            delta = 0.0
            for state in self.env.state_space:
                old_value = v.get_value(state)
                new_value = v_new.get_value(state)
                delta = max(delta, abs(old_value - new_value))
            
            # æ›´æ–°ä»·å€¼å‡½æ•°
            v = v_new
            
            # æ—¥å¿—è®°å½•
            if iteration % 10 == 0:
                logger.debug(f"è¿­ä»£ {iteration}: delta = {delta:.6f}")
            
            # æ£€æŸ¥æ”¶æ•›
            if delta < theta:
                logger.info(f"ç­–ç•¥è¯„ä¼°æ”¶æ•›ï¼è¿­ä»£æ¬¡æ•°: {iteration + 1}, æœ€ç»ˆdelta: {delta:.6f}")
                
                # è®°å½•æœ€ç»ˆçŠ¶æ€
                self.evaluation_history.append(v)
                break
        else:
            logger.warning(f"è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•° {max_iterations}ï¼Œå¯èƒ½æœªå®Œå…¨æ”¶æ•›")
        
        return v
    
    def evaluate_with_trace(self, policy: Policy, 
                           theta: float = 1e-6) -> Tuple[StateValueFunction, List[float]]:
        """
        å¸¦è½¨è¿¹çš„ç­–ç•¥è¯„ä¼°
        Policy Evaluation with Trace
        
        è®°å½•æ¯æ¬¡è¿­ä»£çš„è¯¯å·®ï¼Œç”¨äºåˆ†ææ”¶æ•›æ€§
        Record error at each iteration for convergence analysis
        
        Returns:
            (æœ€ç»ˆä»·å€¼å‡½æ•°, è¯¯å·®è½¨è¿¹)
            (final value function, error trace)
        """
        v = StateValueFunction(self.env.state_space, initial_value=0.0)
        errors = []
        
        for iteration in range(1000):
            v_new = self.bellman_op.bellman_expectation_operator(v, policy)
            
            # è®¡ç®—è¯¯å·®
            delta = 0.0
            for state in self.env.state_space:
                delta = max(delta, abs(v.get_value(state) - v_new.get_value(state)))
            
            errors.append(delta)
            v = v_new
            
            if delta < theta:
                break
        
        return v, errors
    
    def demonstrate_convergence(self, policy: Policy):
        """
        æ¼”ç¤ºæ”¶æ•›è¿‡ç¨‹
        Demonstrate Convergence Process
        
        è¿™ä¸ªå‡½æ•°å±•ç¤ºäº†ä»·å€¼å‡½æ•°å¦‚ä½•é€æ­¥æ”¶æ•›åˆ°çœŸå®å€¼
        This function shows how value function gradually converges to true values
        """
        print("\n" + "="*60)
        print("ç­–ç•¥è¯„ä¼°æ”¶æ•›æ¼”ç¤º")
        print("Policy Evaluation Convergence Demo")
        print("="*60)
        
        # è¿è¡Œè¯„ä¼°
        v_final, errors = self.evaluate_with_trace(policy)
        
        # ç»˜åˆ¶æ”¶æ•›æ›²çº¿
        fig, axes = plt.subplots(1, 2, figsize=(12, 5))
        
        # å·¦å›¾ï¼šè¯¯å·®ä¸‹é™
        ax1 = axes[0]
        ax1.semilogy(errors, 'b-', linewidth=2)
        ax1.set_xlabel('Iteration / è¿­ä»£')
        ax1.set_ylabel('Max Error (log scale) / æœ€å¤§è¯¯å·®ï¼ˆå¯¹æ•°å°ºåº¦ï¼‰')
        ax1.set_title('Convergence of Policy Evaluation / ç­–ç•¥è¯„ä¼°æ”¶æ•›')
        ax1.grid(True, alpha=0.3)
        
        # æ ‡æ³¨å…³é”®ç‚¹
        ax1.axhline(y=1e-6, color='r', linestyle='--', alpha=0.5, label='Î¸=1e-6')
        convergence_iter = len(errors)
        ax1.plot(convergence_iter-1, errors[-1], 'ro', markersize=8)
        ax1.text(convergence_iter-1, errors[-1], f'  Converged at {convergence_iter}', 
                ha='left', va='center')
        ax1.legend()
        
        # å³å›¾ï¼šä»·å€¼å‡½æ•°æ¼”åŒ–
        ax2 = axes[1]
        
        # é€‰æ‹©å‡ ä¸ªçŠ¶æ€å±•ç¤º
        sample_states = self.env.state_space[:min(5, len(self.env.state_space))]
        
        for state in sample_states:
            values = [vh.get_value(state) for vh in self.evaluation_history[::5]]  # æ¯5æ­¥é‡‡æ ·
            ax2.plot(range(0, len(self.evaluation_history), 5), values, 
                    marker='o', markersize=3, label=f'State {state.id}')
        
        ax2.set_xlabel('Iteration / è¿­ä»£')
        ax2.set_ylabel('State Value / çŠ¶æ€ä»·å€¼')
        ax2.set_title('Value Function Evolution / ä»·å€¼å‡½æ•°æ¼”åŒ–')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        # æ‰“å°æ”¶æ•›ç»Ÿè®¡
        print(f"\næ”¶æ•›ç»Ÿè®¡ Convergence Statistics:")
        print(f"  è¿­ä»£æ¬¡æ•° Iterations: {len(errors)}")
        print(f"  æœ€ç»ˆè¯¯å·® Final error: {errors[-1]:.2e}")
        print(f"  æ”¶æ•›é€Ÿåº¦ Convergence rate: ~{self.gamma:.3f} per iteration")
        
        # æ‰“å°æœ€ç»ˆä»·å€¼
        print(f"\næœ€ç»ˆä»·å€¼å‡½æ•° Final Value Function:")
        for state in self.env.state_space[:5]:  # æ˜¾ç¤ºå‰5ä¸ªçŠ¶æ€
            print(f"  V({state.id}) = {v_final.get_value(state):.3f}")
        
        return fig


# ================================================================================
# ç¬¬3.1.4èŠ‚ï¼šç­–ç•¥æ”¹è¿›ï¼ˆæ§åˆ¶é—®é¢˜çš„ä¸€éƒ¨åˆ†ï¼‰
# Section 3.1.4: Policy Improvement (Part of Control Problem)
# ================================================================================

class PolicyImprovementDP:
    """
    ç­–ç•¥æ”¹è¿› - åŸºäºä»·å€¼å‡½æ•°æ”¹è¿›ç­–ç•¥
    Policy Improvement - Improve Policy Based on Value Function
    
    æ ¸å¿ƒæ€æƒ³ï¼šå¦‚æœæˆ‘ä»¬çŸ¥é“v_Ï€ï¼Œå°±å¯ä»¥é€šè¿‡è´ªå©ªåŒ–å¾—åˆ°æ›´å¥½çš„ç­–ç•¥
    Core idea: If we know v_Ï€, we can get a better policy by being greedy
    
    ç­–ç•¥æ”¹è¿›å®šç†ï¼š
    Policy Improvement Theorem:
    å¦‚æœå¯¹æ‰€æœ‰sï¼Œq_Ï€(s, Ï€'(s)) â‰¥ v_Ï€(s)
    åˆ™å¯¹æ‰€æœ‰sï¼Œv_Ï€'(s) â‰¥ v_Ï€(s)
    
    è¿™ä¿è¯äº†è´ªå©ªç­–ç•¥ä¸ä¼šæ›´å·®ï¼
    This guarantees greedy policy is not worse!
    
    æ•°å­¦åŸç†ï¼š
    Mathematical Principle:
    Ï€'(s) = argmax_a q_Ï€(s,a) = argmax_a Î£_{s',r} p(s',r|s,a)[r + Î³v_Ï€(s')]
    """
    
    def __init__(self, mdp_env: MDPEnvironment, gamma: float = 0.99):
        """
        åˆå§‹åŒ–ç­–ç•¥æ”¹è¿›å™¨
        Initialize Policy Improver
        """
        self.env = mdp_env
        self.gamma = gamma
        self.P, self.R = mdp_env.get_dynamics()
        
        logger.info("åˆå§‹åŒ–ç­–ç•¥æ”¹è¿›å™¨(DP)")
    
    def improve(self, v: StateValueFunction) -> Tuple[Policy, bool]:
        """
        åŸºäºä»·å€¼å‡½æ•°æ”¹è¿›ç­–ç•¥
        Improve Policy Based on Value Function
        
        è¿™æ˜¯ç­–ç•¥è¿­ä»£çš„å…³é”®æ­¥éª¤ï¼
        This is the key step in policy iteration!
        
        ç®—æ³•ï¼š
        Algorithm:
        å¯¹æ¯ä¸ªçŠ¶æ€sï¼š
        For each state s:
            Ï€'(s) = argmax_a Î£_{s',r} p(s',r|s,a)[r + Î³v(s')]
        
        Args:
            v: å½“å‰ç­–ç•¥çš„ä»·å€¼å‡½æ•°
               Value function of current policy
        
        Returns:
            (æ”¹è¿›çš„ç­–ç•¥, ç­–ç•¥æ˜¯å¦æ”¹å˜)
            (improved policy, whether policy changed)
        
        æ•™å­¦è¦ç‚¹ï¼š
        Teaching Points:
        1. è¿™åˆ›å»ºäº†ä¸€ä¸ªç¡®å®šæ€§ç­–ç•¥ï¼ˆæ¯ä¸ªçŠ¶æ€é€‰æœ€ä½³åŠ¨ä½œï¼‰
           This creates a deterministic policy (best action for each state)
        2. å¦‚æœç­–ç•¥ä¸å˜ï¼Œè¯´æ˜å·²ç»æ˜¯æœ€ä¼˜ç­–ç•¥
           If policy unchanged, it's optimal policy
        3. æ”¹è¿›æ˜¯å•è°ƒçš„ï¼šæ–°ç­–ç•¥ä¸ä¼šæ›´å·®
           Improvement is monotonic: new policy not worse
        """
        # å­˜å‚¨æ–°ç­–ç•¥
        policy_map = {}
        policy_changed = False
        
        # å¯¹æ¯ä¸ªçŠ¶æ€æ‰¾æœ€ä½³åŠ¨ä½œ
        for state in self.env.state_space:
            if state.is_terminal:
                continue
            
            # è®¡ç®—æ¯ä¸ªåŠ¨ä½œçš„Qå€¼
            action_values = {}
            for action in self.env.action_space:
                q_value = self._compute_q_value(state, action, v)
                action_values[action] = q_value
            
            # é€‰æ‹©æœ€ä½³åŠ¨ä½œï¼ˆè´ªå©ªï¼‰
            best_action = max(action_values, key=action_values.get)
            policy_map[state] = best_action
            
            # è®°å½•è¯¦ç»†ä¿¡æ¯ç”¨äºæ•™å­¦
            logger.debug(f"State {state.id}: "
                        f"Q-values = {{{', '.join(f'{a.id}:{q:.2f}' for a, q in action_values.items())}}}, "
                        f"Best = {best_action.id}")
        
        # åˆ›å»ºæ–°çš„ç¡®å®šæ€§ç­–ç•¥
        new_policy = DeterministicPolicy(policy_map)
        
        return new_policy, policy_changed
    
    def _compute_q_value(self, state: State, action: Action, 
                        v: StateValueFunction) -> float:
        """
        è®¡ç®—åŠ¨ä½œä»·å€¼Q(s,a)
        Compute Action Value Q(s,a)
        
        q_Ï€(s,a) = Î£_{s',r} p(s',r|s,a)[r + Î³v_Ï€(s')]
        
        è¿™æ˜¯é€‰æ‹©æœ€ä½³åŠ¨ä½œçš„ä¾æ®
        This is the basis for selecting best action
        """
        q_value = 0.0
        transitions = self.P.get_transitions(state, action)
        
        for next_state, reward, prob in transitions:
            q_value += prob * (reward + self.gamma * v.get_value(next_state))
        
        return q_value
    
    def demonstrate_improvement(self, initial_policy: Policy):
        """
        æ¼”ç¤ºç­–ç•¥æ”¹è¿›è¿‡ç¨‹
        Demonstrate Policy Improvement Process
        
        å±•ç¤ºä¸€æ¬¡ç­–ç•¥æ”¹è¿›å¦‚ä½•äº§ç”Ÿæ›´å¥½çš„ç­–ç•¥
        Show how one policy improvement produces a better policy
        """
        print("\n" + "="*60)
        print("ç­–ç•¥æ”¹è¿›æ¼”ç¤º")
        print("Policy Improvement Demonstration")
        print("="*60)
        
        # è¯„ä¼°åˆå§‹ç­–ç•¥
        evaluator = PolicyEvaluationDP(self.env, self.gamma)
        v_old = evaluator.evaluate(initial_policy)
        
        print("\nåˆå§‹ç­–ç•¥ä»·å€¼ Initial Policy Values:")
        for state in self.env.state_space[:3]:
            print(f"  V({state.id}) = {v_old.get_value(state):.3f}")
        
        # æ”¹è¿›ç­–ç•¥
        new_policy, _ = self.improve(v_old)
        
        # è¯„ä¼°æ–°ç­–ç•¥
        v_new = evaluator.evaluate(new_policy)
        
        print("\næ”¹è¿›åç­–ç•¥ä»·å€¼ Improved Policy Values:")
        for state in self.env.state_space[:3]:
            old_val = v_old.get_value(state)
            new_val = v_new.get_value(state)
            improvement = new_val - old_val
            print(f"  V({state.id}) = {new_val:.3f} "
                  f"({'â†‘' if improvement > 0 else '='} {improvement:+.3f})")
        
        # éªŒè¯ç­–ç•¥æ”¹è¿›å®šç†
        print("\nç­–ç•¥æ”¹è¿›å®šç†éªŒè¯ Policy Improvement Theorem Verification:")
        all_improved = True
        for state in self.env.state_space:
            if v_new.get_value(state) < v_old.get_value(state) - 1e-6:
                all_improved = False
                break
        
        print(f"  æ‰€æœ‰çŠ¶æ€ä»·å€¼ä¸å‡å°‘: {'âœ“ æ˜¯' if all_improved else 'âœ— å¦'}")
        print(f"  All state values non-decreasing: {'âœ“ Yes' if all_improved else 'âœ— No'}")


# ================================================================================
# ä¸»å‡½æ•°ï¼šæ¼”ç¤ºDPåŸºç¡€
# Main Function: Demonstrate DP Foundations
# ================================================================================

def main():
    """
    è¿è¡ŒåŠ¨æ€è§„åˆ’åŸºç¡€æ¼”ç¤º
    Run Dynamic Programming Foundations Demo
    
    è¿™ä¸ªæ¼”ç¤ºå±•ç¤ºäº†DPçš„æ ¸å¿ƒæ¦‚å¿µå’Œç®—æ³•
    This demo shows core concepts and algorithms of DP
    """
    print("\n" + "="*80)
    print("ç¬¬3.1èŠ‚ï¼šåŠ¨æ€è§„åˆ’åŸºç¡€")
    print("Section 3.1: Dynamic Programming Foundations")
    print("="*80)
    
    # 1. è§£é‡ŠDPåŸç†
    DynamicProgrammingFoundations.explain_dp_principles()
    
    # 2. åˆ›å»ºç®€å•ç¯å¢ƒæµ‹è¯•
    print("\n" + "="*80)
    print("åˆ›å»ºæµ‹è¯•ç¯å¢ƒ")
    print("Creating Test Environment")
    print("="*80)
    
    # ä½¿ç”¨ç¬¬2ç« çš„ç½‘æ ¼ä¸–ç•Œ
    from ch02_mdp.gridworld import GridWorld
    
    # åˆ›å»º3x3ç½‘æ ¼ä¸–ç•Œ
    env = GridWorld(rows=3, cols=3, start_pos=(0, 0), goal_pos=(2, 2))
    print(f"åˆ›å»º {env.rows}Ã—{env.cols} ç½‘æ ¼ä¸–ç•Œ")
    
    # 3. æµ‹è¯•è´å°”æ›¼ç®—å­
    print("\n" + "="*80)
    print("æµ‹è¯•è´å°”æ›¼ç®—å­")
    print("Testing Bellman Operators")
    print("="*80)
    
    bellman_op = BellmanOperator(env, gamma=0.9)
    
    # åˆ›å»ºä¸¤ä¸ªä¸åŒçš„ä»·å€¼å‡½æ•°
    v1 = StateValueFunction(env.state_space, initial_value=0.0)
    v2 = StateValueFunction(env.state_space, initial_value=1.0)
    
    # éªŒè¯æ”¶ç¼©æ€§
    contraction_factor = bellman_op.verify_contraction_property(v1, v2)
    print(f"æ”¶ç¼©å› å­: {contraction_factor:.3f} (åº”è¯¥ â‰¤ 0.9)")
    
    # 4. æ¼”ç¤ºç­–ç•¥è¯„ä¼°
    print("\n" + "="*80)
    print("æ¼”ç¤ºç­–ç•¥è¯„ä¼°")
    print("Demonstrating Policy Evaluation")
    print("="*80)
    
    # åˆ›å»ºéšæœºç­–ç•¥
    from ch02_mdp.policies_and_values import UniformRandomPolicy
    random_policy = UniformRandomPolicy(env.action_space)
    
    # è¯„ä¼°ç­–ç•¥
    evaluator = PolicyEvaluationDP(env, gamma=0.9)
    evaluator.demonstrate_convergence(random_policy)
    
    # 5. æ¼”ç¤ºç­–ç•¥æ”¹è¿›
    print("\n" + "="*80)
    print("æ¼”ç¤ºç­–ç•¥æ”¹è¿›")
    print("Demonstrating Policy Improvement")
    print("="*80)
    
    improver = PolicyImprovementDP(env, gamma=0.9)
    improver.demonstrate_improvement(random_policy)
    
    print("\n" + "="*80)
    print("åŠ¨æ€è§„åˆ’åŸºç¡€æ¼”ç¤ºå®Œæˆï¼")
    print("Dynamic Programming Foundations Demo Complete!")
    print("\nå…³é”®è¦ç‚¹ Key Takeaways:")
    print("1. DPåˆ©ç”¨è´å°”æ›¼æ–¹ç¨‹çš„é€’å½’ç»“æ„")
    print("   DP exploits recursive structure of Bellman equations")
    print("2. è´å°”æ›¼ç®—å­æ˜¯æ”¶ç¼©æ˜ å°„ï¼Œä¿è¯æ”¶æ•›")
    print("   Bellman operators are contraction mappings, guaranteeing convergence")
    print("3. ç­–ç•¥è¯„ä¼°è®¡ç®—v_Ï€ï¼Œç­–ç•¥æ”¹è¿›å¾—åˆ°æ›´å¥½çš„Ï€")
    print("   Policy evaluation computes v_Ï€, policy improvement gets better Ï€")
    print("4. è¿™äº›æ˜¯ç­–ç•¥è¿­ä»£å’Œä»·å€¼è¿­ä»£çš„åŸºç¡€")
    print("   These are foundations of policy iteration and value iteration")
    print("="*80)
    
    plt.show()


if __name__ == "__main__":
    main()