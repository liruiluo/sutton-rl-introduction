"""
================================================================================
ç¬¬5ç« ï¼šæ—¶åºå·®åˆ†å­¦ä¹  - å¼ºåŒ–å­¦ä¹ çš„æ ¸å¿ƒ
Chapter 5: Temporal-Difference Learning - The Core of RL
================================================================================

TDå­¦ä¹ æ˜¯å¼ºåŒ–å­¦ä¹ æœ€é‡è¦çš„æ€æƒ³ï¼
TD learning is the most important idea in RL!

TD = Monte Carlo + Dynamic Programming
- åƒMCï¼šä¸éœ€è¦æ¨¡å‹ï¼Œä»ç»éªŒå­¦ä¹ 
  Like MC: Model-free, learn from experience
- åƒDPï¼šè‡ªä¸¾ï¼ˆbootstrapï¼‰ï¼Œä¸éœ€è¦å®Œæ•´å›åˆ
  Like DP: Bootstrap, no need for complete episodes

æ ¸å¿ƒåˆ›æ–° Core Innovation:
ä½¿ç”¨ä¼°è®¡å€¼æ›´æ–°ä¼°è®¡å€¼ï¼
Use estimates to update estimates!

TDè¯¯å·®ï¼ˆTD Errorï¼‰:
Î´_t = R_{t+1} + Î³V(S_{t+1}) - V(S_t)

è¿™ä¸ªç®€å•çš„å…¬å¼é©å‘½æ€§åœ°æ”¹å˜äº†å¼ºåŒ–å­¦ä¹ ï¼
This simple formula revolutionized RL!

ä¸ºä»€ä¹ˆTDå¦‚æ­¤é‡è¦ï¼Ÿ
Why is TD so important?
1. åœ¨çº¿å­¦ä¹ ï¼ˆæ¯æ­¥éƒ½èƒ½å­¦ä¹ ï¼‰
   Online learning (learn at every step)
2. ä¸éœ€è¦å®Œæ•´å›åˆ
   No need for complete episodes
3. ä½æ–¹å·®ï¼ˆæ¯”MCï¼‰
   Lower variance (than MC)
4. æ”¶æ•›ä¿è¯ï¼ˆåœ¨æŸäº›æ¡ä»¶ä¸‹ï¼‰
   Convergence guarantees (under conditions)

TDæ–¹æ³•å®¶æ— TD Method Family:
- TD(0): ä¸€æ­¥TDï¼Œæœ€åŸºæœ¬
  One-step TD, most basic
- SARSA: On-policy TDæ§åˆ¶
  On-policy TD control
- Q-learning: Off-policy TDæ§åˆ¶ï¼ˆæœ€è‘—åï¼ï¼‰
  Off-policy TD control (most famous!)
- Expected SARSA: SARSAçš„æ”¹è¿›
  Improvement of SARSA
- n-step TD: å¤šæ­¥TDï¼Œä»‹äºTDå’ŒMCä¹‹é—´
  Multi-step TD, between TD and MC
- TD(Î»): èµ„æ ¼è¿¹ï¼Œç»Ÿä¸€æ‰€æœ‰æ–¹æ³•
  Eligibility traces, unifies all methods

å†å²æ„ä¹‰ Historical Significance:
Q-learning (Watkins, 1989) æ˜¯æ·±åº¦å¼ºåŒ–å­¦ä¹ çš„åŸºç¡€
Q-learning is the foundation of Deep RL
- DQN = Q-learning + Deep Neural Networks
- å¼€å¯äº†ç°ä»£AIé©å‘½ï¼
  Started the modern AI revolution!

æœ¬ç« å°†æ·±å…¥ç†è§£TDçš„æ•°å­¦åŸç†å’Œå®ç°ç»†èŠ‚
This chapter deeply understands TD's mathematical principles and implementation details
"""

import numpy as np
from typing import Dict, List, Tuple, Optional, Any, Callable, Union
from dataclasses import dataclass, field
from collections import defaultdict, deque
from abc import ABC, abstractmethod
import logging
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
import time

# å¯¼å…¥åŸºç¡€ç»„ä»¶
# Import base components  
import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent))

from src.ch03_finite_mdp.mdp_framework import State, Action, MDPEnvironment, MDPAgent
from src.ch03_finite_mdp.policies_and_values import (
    Policy, StateValueFunction, ActionValueFunction,
    StochasticPolicy, DeterministicPolicy
)

# è®¾ç½®æ—¥å¿—
# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


# ================================================================================
# ç¬¬5.1èŠ‚ï¼šTDå­¦ä¹ çš„æ•°å­¦åŸºç¡€
# Section 5.1: Mathematical Foundation of TD Learning
# ================================================================================

class TDTheory:
    """
    TDå­¦ä¹ ç†è®º
    TD Learning Theory
    
    æ·±å…¥ç†è§£TDçš„æ•°å­¦åŸç†
    Deep understanding of TD mathematical principles
    
    æ ¸å¿ƒæ€æƒ³ï¼šå¢é‡å¼è´å°”æ›¼æ–¹ç¨‹
    Core idea: Incremental Bellman equation
    
    è´å°”æ›¼æ–¹ç¨‹ï¼š
    Bellman equation:
    V^Ï€(s) = E_Ï€[R_{t+1} + Î³V^Ï€(S_{t+1}) | S_t = s]
    
    TDæ›´æ–°ï¼ˆå°†æœŸæœ›å˜ä¸ºé‡‡æ ·ï¼‰ï¼š
    TD update (expectation to sampling):
    V(S_t) â† V(S_t) + Î±[R_{t+1} + Î³V(S_{t+1}) - V(S_t)]
                           â†‘________________â†‘
                              TD target    é¢„æµ‹å€¼
                                          prediction
    
    TDè¯¯å·®/ä¼˜åŠ¿ TD error/advantage:
    Î´_t = R_{t+1} + Î³V(S_{t+1}) - V(S_t)
    
    è¿™æ˜¯ï¼š
    This is:
    - é¢„æµ‹è¯¯å·® Prediction error
    - æ—¶åºå·®åˆ† Temporal difference  
    - Bellmanè¯¯å·®çš„æ ·æœ¬ Sample of Bellman error
    - ä¼˜åŠ¿å‡½æ•°çš„ä¼°è®¡ Estimate of advantage function
    
    ä¸ºä»€ä¹ˆå«"æ—¶åºå·®åˆ†"ï¼Ÿ
    Why called "Temporal Difference"?
    å› ä¸ºå®ƒæ˜¯ä¸¤ä¸ªæ—¶åˆ»ä»·å€¼ä¼°è®¡çš„å·®ï¼
    Because it's the difference between value estimates at two time points!
    
    V(S_{t+1})åœ¨t+1æ—¶åˆ»ï¼ŒV(S_t)åœ¨tæ—¶åˆ»
    V(S_{t+1}) at time t+1, V(S_t) at time t
    """
    
    @staticmethod
    def explain_td_vs_mc_vs_dp():
        """
        è¯¦è§£TD vs MC vs DP
        Detailed explanation of TD vs MC vs DP
        
        ä¸‰ç§æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«
        Essential differences of three methods
        """
        print("\n" + "="*80)
        print("TD vs MC vs DP æ·±åº¦å¯¹æ¯”")
        print("TD vs MC vs DP Deep Comparison")
        print("="*80)
        
        print("""
        ğŸ“Š ä¸‰ç§æ–¹æ³•çš„æ›´æ–°å…¬å¼
        Update Formulas of Three Methods
        =================================
        
        1. åŠ¨æ€è§„åˆ’ Dynamic Programming (DP):
        ------------------------------------------
        V(s) = Î£_a Ï€(a|s) Î£_{s',r} p(s',r|s,a)[r + Î³V(s')]
        
        ç‰¹ç‚¹ Characteristics:
        - éœ€è¦å®Œæ•´æ¨¡å‹ p(s',r|s,a)
          Needs complete model
        - å…¨å®½åº¦æ›´æ–°ï¼ˆè€ƒè™‘æ‰€æœ‰å¯èƒ½ï¼‰
          Full-width update (consider all possibilities)
        - ç²¾ç¡®ä½†è®¡ç®—é‡å¤§
          Exact but computationally expensive
        
        2. è’™ç‰¹å¡æ´› Monte Carlo (MC):
        ------------------------------------------
        V(S_t) â† V(S_t) + Î±[G_t - V(S_t)]
        
        å…¶ä¸­ where: G_t = R_{t+1} + Î³R_{t+2} + Î³Â²R_{t+3} + ...
        
        ç‰¹ç‚¹ Characteristics:
        - ä¸éœ€è¦æ¨¡å‹
          Model-free
        - ä½¿ç”¨çœŸå®å›æŠ¥G_t
          Uses actual return G_t
        - å¿…é¡»ç­‰åˆ°å›åˆç»“æŸ
          Must wait until episode ends
        - æ— åä½†é«˜æ–¹å·®
          Unbiased but high variance
        
        3. æ—¶åºå·®åˆ† Temporal Difference (TD):
        ------------------------------------------
        V(S_t) â† V(S_t) + Î±[R_{t+1} + Î³V(S_{t+1}) - V(S_t)]
        
        TD target: R_{t+1} + Î³V(S_{t+1})
        
        ç‰¹ç‚¹ Characteristics:
        - ä¸éœ€è¦æ¨¡å‹
          Model-free
        - ä½¿ç”¨ä¼°è®¡å€¼V(S_{t+1})
          Uses estimate V(S_{t+1})
        - æ¯æ­¥éƒ½å¯ä»¥å­¦ä¹ 
          Can learn at every step
        - æœ‰åä½†ä½æ–¹å·®
          Biased but low variance
        
        ğŸ¯ å…³é”®æ´å¯Ÿ Key Insights
        ========================
        
        1. Bootstrapï¼ˆè‡ªä¸¾ï¼‰:
        -------------------
        DP: âœ“ (ä½¿ç”¨V(s')æ›´æ–°V(s))
        MC: âœ— (ä½¿ç”¨çœŸå®G_t)
        TD: âœ“ (ä½¿ç”¨V(S_{t+1}))
        
        Bootstrap = ç”¨ä¼°è®¡æ›´æ–°ä¼°è®¡
        Bootstrap = Update estimate with estimate
        
        2. Samplingï¼ˆé‡‡æ ·ï¼‰:
        -------------------
        DP: âœ— (è€ƒè™‘æ‰€æœ‰è½¬ç§»)
        MC: âœ“ (é‡‡æ ·å®Œæ•´è½¨è¿¹)
        TD: âœ“ (é‡‡æ ·å•æ­¥è½¬ç§»)
        
        Sampling = ç”¨æ ·æœ¬ä»£æ›¿æœŸæœ›
        Sampling = Replace expectation with samples
        
        3. æ›´æ–°æ—¶æœº Update Timing:
        --------------------------
        DP: ä»»æ„ï¼ˆé€šå¸¸sweepæ‰€æœ‰çŠ¶æ€ï¼‰
             Arbitrary (usually sweep all states)
        MC: å›åˆç»“æŸ
             Episode end
        TD: æ¯ä¸€æ­¥
             Every step
        
        4. åå·®-æ–¹å·®æƒè¡¡ Bias-Variance Tradeoff:
        ----------------------------------------
        æ–¹å·® Variance: MC > TD > DP
        åå·® Bias:     DP = 0, TD > 0 (åˆæœŸ), MC = 0
        
        MCé«˜æ–¹å·®å› ä¸ºG_tåŒ…å«æ•´æ¡è½¨è¿¹çš„éšæœºæ€§
        MC high variance because G_t contains randomness of entire trajectory
        
        TDä½æ–¹å·®å› ä¸ºåªæœ‰ä¸€æ­¥éšæœºæ€§
        TD low variance because only one step randomness
        
        TDæœ‰åå› ä¸ºä½¿ç”¨æœ‰åçš„ä¼°è®¡V(S_{t+1})
        TD biased because uses biased estimate V(S_{t+1})
        
        5. æ”¶æ•›æ€§ Convergence:
        ----------------------
        DP: æ€»æ˜¯æ”¶æ•›åˆ°V^Ï€
             Always converges to V^Ï€
        MC: æ”¶æ•›åˆ°V^Ï€ï¼ˆè¶³å¤Ÿæ¢ç´¢ï¼‰
             Converges to V^Ï€ (sufficient exploration)  
        TD: æ”¶æ•›åˆ°V^Ï€ï¼ˆçº¿æ€§è¿‘ä¼¼+é€’å‡æ­¥é•¿ï¼‰
             Converges to V^Ï€ (linear approx + decreasing stepsize)
        
        6. æ•ˆç‡ Efficiency:
        -------------------
        æ•°æ®æ•ˆç‡ Data efficiency: TD > MC
        (TDæ¯æ­¥å­¦ä¹ ï¼ŒMCç­‰å›åˆç»“æŸ)
        (TD learns every step, MC waits for episode end)
        
        è®¡ç®—æ•ˆç‡ Computational: TD â‰ˆ MC >> DP
        (DPéœ€è¦éå†æ‰€æœ‰çŠ¶æ€)
        (DP needs to sweep all states)
        
        å†…å­˜æ•ˆç‡ Memory: TD â‰ˆ MC > DP
        (DPéœ€è¦å­˜å‚¨å®Œæ•´æ¨¡å‹)
        (DP needs to store complete model)
        
        ğŸ”‘ TDçš„ä¼˜åŠ¿ TD's Advantages
        ============================
        
        1. åœ¨çº¿å­¦ä¹  Online Learning:
           å¯ä»¥åœ¨ä¸ç¯å¢ƒäº¤äº’æ—¶ç«‹å³å­¦ä¹ 
           Can learn immediately during interaction
           
        2. ä¸å®Œæ•´å›åˆ Incomplete Episodes:
           å¯ä»¥å­¦ä¹ æŒç»­æ€§ä»»åŠ¡
           Can learn continuing tasks
           
        3. ä½æ–¹å·® Low Variance:
           æ¯”MCæ›´ç¨³å®š
           More stable than MC
           
        4. è®¡ç®—ç®€å• Simple Computation:
           åªéœ€è¦ç®€å•çš„å¢é‡æ›´æ–°
           Only needs simple incremental update
           
        5. ç”Ÿç‰©å­¦åˆç†æ€§ Biological Plausibility:
           ç±»ä¼¼å¤šå·´èƒºç¥ç»å…ƒçš„é¢„æµ‹è¯¯å·®ä¿¡å·
           Similar to dopamine neuron prediction error signal
        """)
    
    @staticmethod
    def demonstrate_td_convergence():
        """
        æ¼”ç¤ºTDæ”¶æ•›æ€§
        Demonstrate TD convergence
        
        å±•ç¤ºTDå¦‚ä½•æ”¶æ•›åˆ°çœŸå®ä»·å€¼
        Show how TD converges to true values
        """
        print("\n" + "="*80)
        print("TDæ”¶æ•›æ€§æ¼”ç¤º")
        print("TD Convergence Demonstration")
        print("="*80)
        
        # åˆ›å»ºç®€å•é©¬å°”å¯å¤«é“¾
        # Create simple Markov chain
        print("\nç®€å•é©¬å°”å¯å¤«å¥–åŠ±è¿‡ç¨‹ Simple Markov Reward Process:")
        print("""
        A â†’ B â†’ C â†’ D â†’ E â†’ [ç»ˆæ­¢]
        0   0   0   0   1
        
        åªæœ‰åˆ°è¾¾Eæ‰æœ‰å¥–åŠ±+1
        Only reward +1 at E
        
        çœŸå®ä»·å€¼ï¼ˆÎ³=1ï¼‰True values (Î³=1):
        V(A)=0.5, V(B)=0.5, V(C)=0.5, V(D)=0.5, V(E)=1.0
        
        å› ä¸ºä»ä»»ä½•çŠ¶æ€ï¼Œ50%æ¦‚ç‡å‘å³ï¼Œ50%æ¦‚ç‡ç»ˆæ­¢
        Because from any state, 50% right, 50% terminate
        """)
        
        # æ¨¡æ‹ŸTDå­¦ä¹ 
        # Simulate TD learning
        states = ['A', 'B', 'C', 'D', 'E']
        true_values = {'A': 0.5, 'B': 0.5, 'C': 0.5, 'D': 0.5, 'E': 1.0}
        
        # TD(0)å­¦ä¹ 
        # TD(0) learning
        V = {s: 0.0 for s in states}  # åˆå§‹åŒ–ä¸º0
        alpha = 0.1  # å­¦ä¹ ç‡
        gamma = 1.0  # æ— æŠ˜æ‰£
        
        episodes = 100
        np.random.seed(42)
        
        # è®°å½•å­¦ä¹ è¿‡ç¨‹
        # Record learning process
        history = {s: [] for s in states}
        
        print("\nå¼€å§‹TD(0)å­¦ä¹ ...")
        print("Starting TD(0) learning...")
        
        for episode in range(episodes):
            # ç”Ÿæˆè½¨è¿¹ï¼ˆç®€åŒ–ï¼šæ€»æ˜¯ä»Aå¼€å§‹ï¼‰
            # Generate trajectory (simplified: always start from A)
            trajectory = ['A']
            current = 'A'
            
            while current != 'E':
                if np.random.random() < 0.5:
                    # ç»ˆæ­¢
                    # Terminate
                    break
                else:
                    # å‘å³ç§»åŠ¨
                    # Move right
                    idx = states.index(current)
                    if idx < len(states) - 1:
                        current = states[idx + 1]
                        trajectory.append(current)
            
            # TDæ›´æ–°ï¼ˆæ²¿è½¨è¿¹ï¼‰
            # TD update (along trajectory)
            for i in range(len(trajectory) - 1):
                s = trajectory[i]
                s_next = trajectory[i + 1]
                
                # å¥–åŠ±ï¼ˆåªæœ‰åˆ°Eæ‰æœ‰ï¼‰
                # Reward (only at E)
                r = 1.0 if s_next == 'E' else 0.0
                
                # TDè¯¯å·®
                # TD error
                td_error = r + gamma * V[s_next] - V[s]
                
                # æ›´æ–°
                # Update
                V[s] += alpha * td_error
            
            # è®°å½•
            # Record
            for s in states:
                history[s].append(V[s])
            
            if episode % 20 == 0:
                print(f"Episode {episode}: ", end="")
                for s in states[:3]:  # æ˜¾ç¤ºå‰3ä¸ªçŠ¶æ€
                    print(f"V({s})={V[s]:.3f} ", end="")
                print()
        
        # å¯è§†åŒ–æ”¶æ•›è¿‡ç¨‹
        # Visualize convergence process
        fig, axes = plt.subplots(2, 3, figsize=(15, 10))
        axes = axes.flatten()
        
        for i, s in enumerate(states):
            ax = axes[i]
            
            # å­¦ä¹ æ›²çº¿
            # Learning curve
            ax.plot(history[s], 'b-', alpha=0.7, label='TD estimate')
            ax.axhline(y=true_values[s], color='r', linestyle='--', 
                      label=f'True value = {true_values[s]}')
            ax.set_xlabel('Episode')
            ax.set_ylabel('Value Estimate')
            ax.set_title(f'State {s}')
            ax.legend()
            ax.grid(True, alpha=0.3)
        
        # æœ€åä¸€ä¸ªå­å›¾ï¼šæ”¶æ•›è¯¯å·®
        # Last subplot: Convergence error
        ax = axes[5]
        errors = []
        for ep in range(episodes):
            error = sum((history[s][ep] - true_values[s])**2 for s in states)
            errors.append(np.sqrt(error / len(states)))  # RMSE
        
        ax.plot(errors, 'g-', linewidth=2)
        ax.set_xlabel('Episode')
        ax.set_ylabel('RMSE')
        ax.set_title('Convergence Error')
        ax.grid(True, alpha=0.3)
        
        plt.suptitle('TD(0) Convergence to True Values', fontsize=14, fontweight='bold')
        plt.tight_layout()
        
        print("\næœ€ç»ˆä¼°è®¡ vs çœŸå®ä»·å€¼:")
        print("Final estimates vs true values:")
        print("-" * 40)
        for s in states:
            error = abs(V[s] - true_values[s])
            print(f"{s}: TD={V[s]:.3f}, True={true_values[s]:.3f}, Error={error:.3f}")
        
        return fig


# ================================================================================  
# ç¬¬5.2èŠ‚ï¼šTDè¯¯å·®å’Œä¼˜åŠ¿å‡½æ•°
# Section 5.2: TD Error and Advantage Function
# ================================================================================

@dataclass
class TDError:
    """
    TDè¯¯å·®åˆ†æ
    TD Error Analysis
    
    TDè¯¯å·®Î´æ˜¯å¼ºåŒ–å­¦ä¹ æœ€é‡è¦çš„ä¿¡å·ï¼
    TD error Î´ is the most important signal in RL!
    
    æ•°å­¦å®šä¹‰ Mathematical Definition:
    Î´_t = R_{t+1} + Î³V(S_{t+1}) - V(S_t)
    
    å¤šç§è§£é‡Š Multiple Interpretations:
    
    1. é¢„æµ‹è¯¯å·® Prediction Error:
       å®é™…å‘ç”Ÿçš„ vs é¢„æœŸçš„
       What happened vs What expected
       
    2. ä¼˜åŠ¿ Advantage:
       A^Ï€(s,a) â‰ˆ Î´ when following Ï€
       è¿™ä¸ªåŠ¨ä½œæ¯”å¹³å‡å¥½å¤šå°‘
       How much better this action than average
       
    3. å­¦ä¹ ä¿¡å· Learning Signal:
       æ­£Î´ â†’ æé«˜V(S_t)
       Positive Î´ â†’ Increase V(S_t)
       è´ŸÎ´ â†’ é™ä½V(S_t)  
       Negative Î´ â†’ Decrease V(S_t)
       
    4. ç¥ç»ç§‘å­¦ Neuroscience:
       å¤šå·´èƒºç¥ç»å…ƒç¼–ç TDè¯¯å·®ï¼
       Dopamine neurons encode TD error!
       å¥–åŠ±é¢„æµ‹è¯¯å·®å‡è¯´(Schultz et al., 1997)
       Reward prediction error hypothesis
       
    TDè¯¯å·®çš„æ€§è´¨ Properties of TD Error:
    
    1. æœŸæœ›ä¸º0ï¼ˆæ”¶æ•›åï¼‰ï¼š
       E[Î´_t | S_t=s] = 0 when V = V^Ï€
       
    2. ä¸ä¼˜åŠ¿å‡½æ•°çš„å…³ç³»ï¼š
       E[Î´_t | S_t=s, A_t=a] = Q^Ï€(s,a) - V^Ï€(s) = A^Ï€(s,a)
       
    3. è´å°”æ›¼æ®‹å·®ï¼š
       Î´æ˜¯è´å°”æ›¼æ–¹ç¨‹è¯¯å·®çš„æ— åä¼°è®¡
       Î´ is unbiased estimate of Bellman equation error
    """
    
    # TDè¯¯å·®å€¼
    # TD error value
    value: float
    
    # æ—¶é—´æ­¥
    # Time step
    timestep: int
    
    # ç›¸å…³çŠ¶æ€
    # Related states
    state: State
    next_state: Optional[State] = None
    
    # å¥–åŠ±å’Œä»·å€¼
    # Reward and values
    reward: float = 0.0
    state_value: float = 0.0
    next_state_value: float = 0.0
    
    # å…¶ä»–ä¿¡æ¯
    # Other info
    info: Dict[str, Any] = field(default_factory=dict)
    
    def __repr__(self):
        return f"TDError(Î´={self.value:.3f}, t={self.timestep})"
    
    def analyze(self):
        """
        åˆ†æTDè¯¯å·®
        Analyze TD error
        
        æä¾›è¯Šæ–­ä¿¡æ¯
        Provide diagnostic information
        """
        print(f"\nTDè¯¯å·®åˆ†æ TD Error Analysis:")
        print(f"æ—¶é—´æ­¥ Timestep: {self.timestep}")
        print(f"TDè¯¯å·®å€¼ TD Error Value: {self.value:.3f}")
        
        if self.value > 0:
            print("  â†’ æ­£è¯¯å·®ï¼šå®é™…æ¯”é¢„æœŸå¥½")
            print("     Positive: Better than expected")
            print("  â†’ åº”è¯¥å¢åŠ V(S_t)")
            print("     Should increase V(S_t)")
        elif self.value < 0:
            print("  â†’ è´Ÿè¯¯å·®ï¼šå®é™…æ¯”é¢„æœŸå·®")
            print("     Negative: Worse than expected")  
            print("  â†’ åº”è¯¥å‡å°‘V(S_t)")
            print("     Should decrease V(S_t)")
        else:
            print("  â†’ é›¶è¯¯å·®ï¼šå®Œç¾é¢„æµ‹")
            print("     Zero: Perfect prediction")
        
        print(f"\nåˆ†è§£ Decomposition:")
        print(f"  R_{{{self.timestep+1}}} = {self.reward:.3f}")
        print(f"  Î³V(S_{{{self.timestep+1}}}) = {self.next_state_value:.3f}")
        print(f"  V(S_{{{self.timestep}}}) = {self.state_value:.3f}")
        print(f"  Î´ = {self.reward:.3f} + {self.next_state_value:.3f} - {self.state_value:.3f}")
        print(f"    = {self.value:.3f}")


class TDErrorAnalyzer:
    """
    TDè¯¯å·®åˆ†æå™¨
    TD Error Analyzer
    
    æ”¶é›†å’Œåˆ†æTDè¯¯å·®æ¨¡å¼
    Collect and analyze TD error patterns
    
    ç”¨äºï¼š
    Used for:
    1. è°ƒè¯•ç®—æ³•
       Debug algorithms
    2. ç›‘æ§æ”¶æ•›
       Monitor convergence  
    3. å‘ç°é—®é¢˜
       Discover issues
    """
    
    def __init__(self, window_size: int = 100):
        """
        åˆå§‹åŒ–åˆ†æå™¨
        Initialize analyzer
        
        Args:
            window_size: æ»‘åŠ¨çª—å£å¤§å°
                        Sliding window size
        """
        self.window_size = window_size
        
        # TDè¯¯å·®å†å²
        # TD error history
        self.errors: List[TDError] = []
        
        # æ»‘åŠ¨çª—å£
        # Sliding window
        self.recent_errors = deque(maxlen=window_size)
        
        # ç»Ÿè®¡
        # Statistics
        self.total_errors = 0
        self.sum_errors = 0.0
        self.sum_squared_errors = 0.0
        
        logger.info(f"åˆå§‹åŒ–TDè¯¯å·®åˆ†æå™¨ï¼Œçª—å£å¤§å°={window_size}")
    
    def add_error(self, td_error: TDError):
        """
        æ·»åŠ TDè¯¯å·®
        Add TD error
        """
        self.errors.append(td_error)
        self.recent_errors.append(td_error.value)
        
        self.total_errors += 1
        self.sum_errors += td_error.value
        self.sum_squared_errors += td_error.value ** 2
    
    def get_statistics(self) -> Dict[str, float]:
        """
        è·å–ç»Ÿè®¡ä¿¡æ¯
        Get statistics
        
        Returns:
            ç»Ÿè®¡å­—å…¸
            Statistics dict
        """
        if self.total_errors == 0:
            return {}
        
        mean = self.sum_errors / self.total_errors
        variance = self.sum_squared_errors / self.total_errors - mean ** 2
        std = np.sqrt(variance) if variance > 0 else 0
        
        # æœ€è¿‘çš„ç»Ÿè®¡
        # Recent statistics
        if self.recent_errors:
            recent_mean = np.mean(self.recent_errors)
            recent_std = np.std(self.recent_errors)
            recent_abs_mean = np.mean(np.abs(self.recent_errors))
        else:
            recent_mean = recent_std = recent_abs_mean = 0
        
        return {
            'total_errors': self.total_errors,
            'mean': mean,
            'std': std,
            'recent_mean': recent_mean,
            'recent_std': recent_std,
            'recent_abs_mean': recent_abs_mean,
            'convergence_metric': recent_abs_mean  # è¶Šå°è¶Šæ”¶æ•›
        }
    
    def plot_analysis(self):
        """
        ç»˜åˆ¶åˆ†æå›¾
        Plot analysis
        
        å¯è§†åŒ–TDè¯¯å·®æ¨¡å¼
        Visualize TD error patterns
        """
        if not self.errors:
            print("æ²¡æœ‰TDè¯¯å·®æ•°æ®")
            return None
        
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        
        # å›¾1ï¼šTDè¯¯å·®æ—¶é—´åºåˆ—
        # Plot 1: TD error time series
        ax1 = axes[0, 0]
        error_values = [e.value for e in self.errors]
        ax1.plot(error_values, 'b-', alpha=0.5, linewidth=0.5)
        ax1.axhline(y=0, color='r', linestyle='--', alpha=0.5)
        ax1.set_xlabel('Time Step')
        ax1.set_ylabel('TD Error')
        ax1.set_title('TD Error Over Time')
        ax1.grid(True, alpha=0.3)
        
        # æ·»åŠ ç§»åŠ¨å¹³å‡
        # Add moving average
        if len(error_values) > 20:
            window = min(50, len(error_values) // 10)
            moving_avg = np.convolve(error_values, 
                                     np.ones(window)/window, 
                                     mode='valid')
            ax1.plot(range(window-1, len(error_values)), 
                    moving_avg, 'r-', linewidth=2, 
                    label=f'Moving Avg (w={window})')
            ax1.legend()
        
        # å›¾2ï¼šTDè¯¯å·®åˆ†å¸ƒ
        # Plot 2: TD error distribution  
        ax2 = axes[0, 1]
        ax2.hist(error_values, bins=50, density=True, 
                alpha=0.7, color='blue', edgecolor='black')
        ax2.axvline(x=0, color='r', linestyle='--', alpha=0.5)
        
        # æ‹Ÿåˆæ­£æ€åˆ†å¸ƒ
        # Fit normal distribution
        mean, std = np.mean(error_values), np.std(error_values)
        x = np.linspace(min(error_values), max(error_values), 100)
        ax2.plot(x, stats.norm.pdf(x, mean, std), 'r-', linewidth=2,
                label=f'Normal(Î¼={mean:.3f}, Ïƒ={std:.3f})')
        ax2.set_xlabel('TD Error')
        ax2.set_ylabel('Density')
        ax2.set_title('TD Error Distribution')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # å›¾3ï¼šç»å¯¹TDè¯¯å·®ï¼ˆæ”¶æ•›æŒ‡æ ‡ï¼‰
        # Plot 3: Absolute TD error (convergence metric)
        ax3 = axes[1, 0]
        abs_errors = np.abs(error_values)
        
        # è®¡ç®—æ»‘åŠ¨å¹³å‡
        # Compute sliding average
        window = min(100, len(abs_errors) // 10) if len(abs_errors) > 10 else 1
        abs_moving_avg = np.convolve(abs_errors,
                                     np.ones(window)/window,
                                     mode='valid')
        
        ax3.plot(range(window-1, len(abs_errors)), 
                abs_moving_avg, 'g-', linewidth=2)
        ax3.set_xlabel('Time Step')
        ax3.set_ylabel('Mean Absolute TD Error')
        ax3.set_title('Convergence Metric (|Î´|)')
        ax3.grid(True, alpha=0.3)
        
        # å›¾4ï¼šTDè¯¯å·®è‡ªç›¸å…³
        # Plot 4: TD error autocorrelation
        ax4 = axes[1, 1]
        if len(error_values) > 50:
            from scipy.signal import correlate
            # è®¡ç®—è‡ªç›¸å…³
            # Compute autocorrelation
            autocorr = correlate(error_values[:1000], error_values[:1000], mode='same')
            autocorr = autocorr / np.max(autocorr)  # å½’ä¸€åŒ–
            center = len(autocorr) // 2
            lags = 50
            ax4.plot(range(-lags, lags+1), 
                    autocorr[center-lags:center+lags+1], 'b-')
            ax4.axhline(y=0, color='r', linestyle='--', alpha=0.5)
            ax4.set_xlabel('Lag')
            ax4.set_ylabel('Autocorrelation')
            ax4.set_title('TD Error Autocorrelation')
            ax4.grid(True, alpha=0.3)
        
        plt.suptitle('TD Error Analysis', fontsize=14, fontweight='bold')
        plt.tight_layout()
        
        # æ‰“å°ç»Ÿè®¡æ‘˜è¦
        # Print statistical summary
        stats_dict = self.get_statistics()
        print("\nTDè¯¯å·®ç»Ÿè®¡æ‘˜è¦ TD Error Statistics Summary:")
        print("-" * 40)
        for key, value in stats_dict.items():
            print(f"{key}: {value:.4f}")
        
        return fig


# ================================================================================
# ç¬¬5.3èŠ‚ï¼šTD(0)ç®—æ³•å®ç°
# Section 5.3: TD(0) Algorithm Implementation
# ================================================================================

class TD0:
    """
    TD(0)ç®—æ³• - æœ€åŸºæœ¬çš„TDæ–¹æ³•
    TD(0) Algorithm - Most basic TD method
    
    ä¹Ÿå«ä¸€æ­¥TD (One-step TD)
    Also called one-step TD
    
    æ›´æ–°è§„åˆ™ Update rule:
    V(S_t) â† V(S_t) + Î±[R_{t+1} + Î³V(S_{t+1}) - V(S_t)]
    
    ç®—æ³•æ­¥éª¤ Algorithm steps:
    1. åˆå§‹åŒ–V(s)ä»»æ„ï¼ŒV(terminal)=0
       Initialize V(s) arbitrarily, V(terminal)=0
    2. é‡å¤æ¯ä¸ªå›åˆï¼š
       Repeat for each episode:
       a. åˆå§‹åŒ–S
          Initialize S
       b. é‡å¤æ¯æ­¥ï¼š
          Repeat for each step:
          - é€‰æ‹©å¹¶æ‰§è¡ŒåŠ¨ä½œAï¼ˆæ ¹æ®ç­–ç•¥Ï€ï¼‰
            Choose and execute action A (according to Ï€)
          - è§‚å¯ŸRå’ŒS'
            Observe R and S'
          - V(S) â† V(S) + Î±[R + Î³V(S') - V(S)]
          - S â† S'
       ç›´åˆ°Sæ˜¯ç»ˆæ­¢çŠ¶æ€
       Until S is terminal
    
    ç‰¹æ€§ Properties:
    1. åœ¨çº¿ Online: æ¯æ­¥æ›´æ–°
                   Update every step
    2. å¢é‡ Incremental: ä¸å­˜å‚¨å†å²
                        No history storage
    3. æ¨¡å‹æ— å…³ Model-free: ä¸éœ€è¦På’ŒR
                           No need for P and R
    4. è‡ªä¸¾ Bootstrapping: ç”¨V(S')ä¼°è®¡
                          Use V(S') estimate
    
    æ”¶æ•›æ¡ä»¶ Convergence conditions:
    1. ç­–ç•¥Ï€å›ºå®š
       Policy Ï€ fixed
    2. æ­¥é•¿æ»¡è¶³Robbins-Monroæ¡ä»¶ï¼š
       Step size satisfies Robbins-Monro:
       Î£Î±_t = âˆ, Î£Î±_tÂ² < âˆ
    3. æ‰€æœ‰çŠ¶æ€è¢«æ— é™è®¿é—®
       All states visited infinitely
    
    åˆ™Væ”¶æ•›åˆ°V^Ï€ (æ¦‚ç‡1)
    Then V converges to V^Ï€ (with probability 1)
    """
    
    def __init__(self, 
                 env: MDPEnvironment,
                 gamma: float = 1.0,
                 alpha: Union[float, Callable] = 0.1):
        """
        åˆå§‹åŒ–TD(0)
        Initialize TD(0)
        
        Args:
            env: ç¯å¢ƒ
                Environment
            gamma: æŠ˜æ‰£å› å­
                  Discount factor
            alpha: å­¦ä¹ ç‡ï¼ˆå›ºå®šæˆ–å‡½æ•°ï¼‰
                  Learning rate (fixed or function)
        """
        self.env = env
        self.gamma = gamma
        
        # å­¦ä¹ ç‡ï¼ˆå¯ä»¥æ˜¯å¸¸æ•°æˆ–é€’å‡å‡½æ•°ï¼‰
        # Learning rate (can be constant or decreasing function)
        if callable(alpha):
            self.alpha_func = alpha
        else:
            self.alpha_func = lambda t: alpha
        
        # ä»·å€¼å‡½æ•°
        # Value function
        self.V = StateValueFunction(env.state_space, initial_value=0.0)
        
        # TDè¯¯å·®åˆ†æå™¨
        # TD error analyzer
        self.td_analyzer = TDErrorAnalyzer()
        
        # ç»Ÿè®¡
        # Statistics
        self.episode_count = 0
        self.step_count = 0
        self.episode_returns = []
        
        logger.info(f"åˆå§‹åŒ–TD(0): Î³={gamma}, Î±={alpha}")
    
    def learn_episode(self, policy: Policy) -> float:
        """
        å­¦ä¹ ä¸€ä¸ªå›åˆ
        Learn one episode
        
        Args:
            policy: è¦è¯„ä¼°çš„ç­–ç•¥
                   Policy to evaluate
        
        Returns:
            å›åˆå›æŠ¥
            Episode return
        """
        state = self.env.reset()
        episode_return = 0.0
        episode_steps = 0
        
        while True:
            # é€‰æ‹©åŠ¨ä½œ
            # Select action
            action = policy.select_action(state)
            
            # æ‰§è¡ŒåŠ¨ä½œ
            # Execute action
            next_state, reward, done, info = self.env.step(action)
            
            # TDæ›´æ–°
            # TD update
            if not state.is_terminal:
                # è·å–å½“å‰å­¦ä¹ ç‡
                # Get current learning rate
                alpha = self.alpha_func(self.step_count)
                
                # è®¡ç®—TDè¯¯å·®
                # Compute TD error
                v_current = self.V.get_value(state)
                v_next = self.V.get_value(next_state) if not done else 0.0
                td_error = reward + self.gamma * v_next - v_current
                
                # æ›´æ–°ä»·å€¼å‡½æ•°
                # Update value function
                new_value = v_current + alpha * td_error
                self.V.set_value(state, new_value)
                
                # è®°å½•TDè¯¯å·®
                # Record TD error
                td_err_obj = TDError(
                    value=td_error,
                    timestep=self.step_count,
                    state=state,
                    next_state=next_state,
                    reward=reward,
                    state_value=v_current,
                    next_state_value=v_next
                )
                self.td_analyzer.add_error(td_err_obj)
            
            # ç´¯ç§¯å›æŠ¥
            # Accumulate return
            episode_return += reward * (self.gamma ** episode_steps)
            
            # æ›´æ–°è®¡æ•°
            # Update counts
            self.step_count += 1
            episode_steps += 1
            
            # è½¬ç§»åˆ°ä¸‹ä¸€çŠ¶æ€
            # Transition to next state
            state = next_state
            
            if done:
                break
        
        self.episode_count += 1
        self.episode_returns.append(episode_return)
        
        return episode_return
    
    def learn(self, 
             policy: Policy,
             n_episodes: int = 1000,
             verbose: bool = True) -> StateValueFunction:
        """
        å­¦ä¹ ä»·å€¼å‡½æ•°
        Learn value function
        
        Args:
            policy: ç­–ç•¥
                   Policy
            n_episodes: å›åˆæ•°
                       Number of episodes
            verbose: æ˜¯å¦è¾“å‡ºè¿›åº¦
                    Whether to output progress
        
        Returns:
            å­¦ä¹ çš„ä»·å€¼å‡½æ•°
            Learned value function
        """
        if verbose:
            print(f"\nå¼€å§‹TD(0)å­¦ä¹ : {n_episodes}å›åˆ")
            print(f"Starting TD(0) learning: {n_episodes} episodes")
        
        for episode in range(n_episodes):
            episode_return = self.learn_episode(policy)
            
            if verbose and (episode + 1) % max(1, n_episodes // 10) == 0:
                stats = self.td_analyzer.get_statistics()
                avg_return = np.mean(self.episode_returns[-100:]) if self.episode_returns else 0
                
                print(f"Episode {episode + 1}/{n_episodes}: "
                      f"Return={episode_return:.2f}, "
                      f"Avg Return={avg_return:.2f}, "
                      f"TD Error={stats.get('recent_mean', 0):.4f}")
        
        if verbose:
            print(f"\nå­¦ä¹ å®Œæˆ!")
            print(f"Learning complete!")
            
            # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
            # Show final statistics
            stats = self.td_analyzer.get_statistics()
            print(f"æœ€ç»ˆTDè¯¯å·®: {stats.get('recent_abs_mean', 0):.4f}")
            print(f"Final TD error: {stats.get('recent_abs_mean', 0):.4f}")
        
        return self.V
    
    def compare_with_mc(self, mc_values: StateValueFunction):
        """
        ä¸MCæ¯”è¾ƒ
        Compare with MC
        
        å±•ç¤ºTD vs MCçš„å·®å¼‚
        Show differences between TD and MC
        
        Args:
            mc_values: MCä¼°è®¡çš„ä»·å€¼å‡½æ•°
                      MC estimated value function
        """
        print("\n" + "="*60)
        print("TD(0) vs Monte Carlo æ¯”è¾ƒ")
        print("TD(0) vs Monte Carlo Comparison")
        print("="*60)
        
        # è®¡ç®—å·®å¼‚
        # Compute differences
        differences = []
        
        for state in self.env.state_space:
            if not state.is_terminal:
                td_value = self.V.get_value(state)
                mc_value = mc_values.get_value(state)
                diff = abs(td_value - mc_value)
                differences.append((state.id, td_value, mc_value, diff))
        
        # æŒ‰å·®å¼‚æ’åº
        # Sort by difference
        differences.sort(key=lambda x: x[3], reverse=True)
        
        print("\nä»·å€¼ä¼°è®¡æ¯”è¾ƒï¼ˆå‰10ä¸ªå·®å¼‚æœ€å¤§çš„çŠ¶æ€ï¼‰ï¼š")
        print("Value estimate comparison (top 10 largest differences):")
        print("-" * 60)
        print(f"{'State':<15} {'TD(0)':<10} {'MC':<10} {'|Diff|':<10}")
        print("-" * 60)
        
        for state_id, td_val, mc_val, diff in differences[:10]:
            print(f"{str(state_id):<15} {td_val:<10.3f} {mc_val:<10.3f} {diff:<10.3f}")
        
        # ç»Ÿè®¡åˆ†æ
        # Statistical analysis
        all_diffs = [d[3] for d in differences]
        
        print("\nç»Ÿè®¡åˆ†æ Statistical Analysis:")
        print("-" * 40)
        print(f"å¹³å‡ç»å¯¹å·®å¼‚ Mean Absolute Difference: {np.mean(all_diffs):.4f}")
        print(f"æœ€å¤§å·®å¼‚ Max Difference: {np.max(all_diffs):.4f}")
        print(f"å·®å¼‚æ ‡å‡†å·® Std of Differences: {np.std(all_diffs):.4f}")
        
        # åˆ†æåŸå› 
        # Analyze reasons
        print("\nå·®å¼‚åŸå› åˆ†æ Difference Analysis:")
        print("-" * 40)
        print("""
        TD(0)å’ŒMCçš„å·®å¼‚æ¥æºäºï¼š
        Differences between TD(0) and MC come from:
        
        1. Bootstrap vs Full Return:
           TD: ä½¿ç”¨V(S')çš„ä¼°è®¡
               Uses estimate of V(S')
           MC: ä½¿ç”¨çœŸå®çš„G_t
               Uses actual G_t
        
        2. Bias vs Variance:
           TD: åˆæœŸæœ‰åï¼Œä½†æ–¹å·®å°
               Initially biased, but low variance
           MC: æ— åï¼Œä½†æ–¹å·®å¤§
               Unbiased, but high variance
        
        3. Update Frequency:
           TD: æ¯æ­¥æ›´æ–°ï¼Œä¿¡æ¯ä¼ æ’­å¿«
               Update every step, fast propagation
           MC: å›åˆç»“æŸæ›´æ–°ï¼Œä¿¡æ¯ä¼ æ’­æ…¢
               Update at episode end, slow propagation
        
        4. Convergence Path:
           ä¸åŒçš„è·¯å¾„æ”¶æ•›åˆ°ç›¸åŒçš„V^Ï€
           Different paths converge to same V^Ï€
        """)


# ================================================================================
# ä¸»å‡½æ•°ï¼šæ¼”ç¤ºTDåŸºç¡€
# Main Function: Demonstrate TD Foundations
# ================================================================================

def demonstrate_td_foundations():
    """
    æ¼”ç¤ºTDå­¦ä¹ åŸºç¡€
    Demonstrate TD learning foundations
    """
    print("\n" + "="*80)
    print("ç¬¬5ç« ï¼šæ—¶åºå·®åˆ†å­¦ä¹  - åŸºç¡€ç†è®º")
    print("Chapter 5: Temporal-Difference Learning - Foundations")
    print("="*80)
    
    # 1. TD vs MC vs DPå¯¹æ¯”
    # 1. TD vs MC vs DP comparison
    TDTheory.explain_td_vs_mc_vs_dp()
    
    # 2. TDæ”¶æ•›æ€§æ¼”ç¤º
    # 2. TD convergence demonstration
    fig1 = TDTheory.demonstrate_td_convergence()
    
    # 3. åœ¨GridWorldä¸Šæµ‹è¯•TD(0)
    # 3. Test TD(0) on GridWorld
    print("\n" + "="*80)
    print("TD(0)åœ¨GridWorldä¸Šçš„å®éªŒ")
    print("TD(0) Experiment on GridWorld")
    print("="*80)
    
    from src.ch03_finite_mdp.gridworld import GridWorld
    from src.ch03_finite_mdp.policies_and_values import UniformRandomPolicy
    
    # åˆ›å»ºç¯å¢ƒ
    # Create environment
    env = GridWorld(rows=4, cols=4, start_pos=(0,0), goal_pos=(3,3))
    print(f"åˆ›å»º4Ã—4 GridWorld")
    
    # åˆ›å»ºéšæœºç­–ç•¥
    # Create random policy
    policy = UniformRandomPolicy(env.action_space)
    
    # TD(0)å­¦ä¹ 
    # TD(0) learning
    td0 = TD0(env, gamma=0.9, alpha=0.1)
    V_td = td0.learn(policy, n_episodes=1000, verbose=True)
    
    # åˆ†æTDè¯¯å·®
    # Analyze TD errors
    fig2 = td0.td_analyzer.plot_analysis()
    
    # æ˜¾ç¤ºå­¦ä¹ çš„ä»·å€¼å‡½æ•°
    # Show learned value function
    print("\nå­¦ä¹ çš„çŠ¶æ€ä»·å€¼ï¼ˆéƒ¨åˆ†ï¼‰ï¼š")
    print("Learned state values (partial):")
    for i in range(min(5, len(env.state_space))):
        state = env.state_space[i]
        if not state.is_terminal:
            value = V_td.get_value(state)
            print(f"  V({state.id}) = {value:.3f}")
    
    print("\n" + "="*80)
    print("TDåŸºç¡€æ¼”ç¤ºå®Œæˆï¼")
    print("TD Foundation Demo Complete!")
    print("="*80)
    
    plt.show()


# ================================================================================
# æ‰§è¡Œä¸»å‡½æ•°
# Execute Main Function  
# ================================================================================

if __name__ == "__main__":
    demonstrate_td_foundations()