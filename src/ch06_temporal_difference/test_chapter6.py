#!/usr/bin/env python
"""
æµ‹è¯•ç¬¬5ç« æ‰€æœ‰æ—¶åºå·®åˆ†æ¨¡å—
Test all Chapter 5 Temporal Difference modules

ç¡®ä¿æ‰€æœ‰TDç®—æ³•å®ç°æ­£ç¡®
Ensure all TD algorithm implementations are correct
"""

import sys
import traceback
import numpy as np
from pathlib import Path
import time

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))


def test_td_foundations():
    """
    æµ‹è¯•TDåŸºç¡€ç†è®º
    Test TD Foundations
    """
    print("\n" + "="*60)
    print("æµ‹è¯•TDåŸºç¡€ç†è®º...")
    print("Testing TD Foundations...")
    print("="*60)
    
    try:
        from src.ch06_temporal_difference.td_foundations import (
            TDTheory, TDError, TDErrorAnalyzer, TD0
        )
        from src.ch03_finite_mdp.gridworld import GridWorld
        from src.ch03_finite_mdp.policies_and_values import UniformRandomPolicy
        
        # æµ‹è¯•TDç†è®º
        print("æµ‹è¯•TDç†è®ºå±•ç¤º...")
        # TDTheory.explain_td_vs_mc_vs_dp()  # åªæ˜¯æ‰“å°ï¼Œä¸éœ€è¦æµ‹è¯•è¿”å›å€¼
        print("âœ“ TDç†è®ºå±•ç¤ºé€šè¿‡")
        
        # æµ‹è¯•TDè¯¯å·®
        print("æµ‹è¯•TDè¯¯å·®...")
        from src.ch03_finite_mdp.mdp_framework import State
        test_state = State("test", features={'value': 1})
        td_error = TDError(
            value=0.5,
            timestep=0,
            state=test_state,
            reward=1.0,
            state_value=0.0,
            next_state_value=0.5
        )
        assert td_error.value == 0.5, "TDè¯¯å·®å€¼é”™è¯¯"
        print("âœ“ TDè¯¯å·®æµ‹è¯•é€šè¿‡")
        
        # æµ‹è¯•TDè¯¯å·®åˆ†æå™¨
        print("æµ‹è¯•TDè¯¯å·®åˆ†æå™¨...")
        analyzer = TDErrorAnalyzer(window_size=10)
        for i in range(20):
            td_err = TDError(
                value=np.random.normal(0, 1),
                timestep=i,
                state=test_state
            )
            analyzer.add_error(td_err)
        
        stats = analyzer.get_statistics()
        assert 'total_errors' in stats, "ç»Ÿè®¡ä¿¡æ¯ç¼ºå¤±"
        assert stats['total_errors'] == 20, "è¯¯å·®è®¡æ•°é”™è¯¯"
        print("âœ“ TDè¯¯å·®åˆ†æå™¨æµ‹è¯•é€šè¿‡")
        
        # æµ‹è¯•TD(0)
        print("æµ‹è¯•TD(0)ç®—æ³•...")
        env = GridWorld(rows=3, cols=3, start_pos=(0,0), goal_pos=(2,2))
        policy = UniformRandomPolicy(env.action_space)
        
        td0 = TD0(env, gamma=0.9, alpha=0.1)
        V = td0.learn(policy, n_episodes=50, verbose=False)
        
        # æ£€æŸ¥ä»·å€¼å‡½æ•°åˆç†æ€§
        for state in env.state_space:
            if not state.is_terminal:
                value = V.get_value(state)
                assert -100 < value < 100, f"TD(0)ä»·å€¼å¼‚å¸¸: {value}"
        
        print("âœ“ TD(0)æµ‹è¯•é€šè¿‡")
        
        print("\nâœ… TDåŸºç¡€ç†è®ºæµ‹è¯•å…¨éƒ¨é€šè¿‡ï¼")
        return True
        
    except Exception as e:
        print(f"\nâŒ TDåŸºç¡€ç†è®ºæµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return False


def test_td_control():
    """
    æµ‹è¯•TDæ§åˆ¶ç®—æ³•
    Test TD Control Algorithms
    """
    print("\n" + "="*60)
    print("æµ‹è¯•TDæ§åˆ¶ç®—æ³•...")
    print("Testing TD Control Algorithms...")
    print("="*60)
    
    try:
        from src.ch06_temporal_difference.td_control import (
            SARSA, QLearning, ExpectedSARSA, TDControlComparator
        )
        from src.ch03_finite_mdp.gridworld import GridWorld
        
        # åˆ›å»ºç¯å¢ƒ
        env = GridWorld(rows=3, cols=3, start_pos=(0,0), goal_pos=(2,2))
        print(f"âœ“ åˆ›å»º3Ã—3ç½‘æ ¼ä¸–ç•Œ")
        
        # æµ‹è¯•SARSA
        print("\næµ‹è¯•SARSA...")
        sarsa = SARSA(env, gamma=0.9, alpha=0.1, epsilon=0.1)
        Q_sarsa = sarsa.learn(n_episodes=50, verbose=False)
        
        # æ£€æŸ¥Qå‡½æ•°åˆç†æ€§
        q_values_sarsa = []
        for state in env.state_space:
            if not state.is_terminal:
                for action in env.action_space:
                    q = Q_sarsa.get_value(state, action)
                    q_values_sarsa.append(q)
                    assert -100 < q < 100, f"SARSA Qå€¼å¼‚å¸¸: {q}"
        
        assert len(sarsa.episode_returns) == 50, "SARSAå›åˆæ•°ä¸åŒ¹é…"
        print("âœ“ SARSAæµ‹è¯•é€šè¿‡")
        
        # æµ‹è¯•Q-Learning
        print("æµ‹è¯•Q-Learning...")
        qlearning = QLearning(env, gamma=0.9, alpha=0.1, epsilon=0.1)
        Q_qlearning = qlearning.learn(n_episodes=50, verbose=False)
        
        q_values_ql = []
        for state in env.state_space:
            if not state.is_terminal:
                for action in env.action_space:
                    q = Q_qlearning.get_value(state, action)
                    q_values_ql.append(q)
                    assert -100 < q < 100, f"Q-Learning Qå€¼å¼‚å¸¸: {q}"
        
        assert len(qlearning.episode_returns) == 50, "Q-Learningå›åˆæ•°ä¸åŒ¹é…"
        print("âœ“ Q-Learningæµ‹è¯•é€šè¿‡")
        
        # æµ‹è¯•Expected SARSA
        print("æµ‹è¯•Expected SARSA...")
        expected_sarsa = ExpectedSARSA(env, gamma=0.9, alpha=0.1, epsilon=0.1)
        Q_expected = expected_sarsa.learn(n_episodes=50, verbose=False)
        
        q_values_exp = []
        for state in env.state_space:
            if not state.is_terminal:
                for action in env.action_space:
                    q = Q_expected.get_value(state, action)
                    q_values_exp.append(q)
                    assert -100 < q < 100, f"Expected SARSA Qå€¼å¼‚å¸¸: {q}"
        
        print("âœ“ Expected SARSAæµ‹è¯•é€šè¿‡")
        
        # æµ‹è¯•ç®—æ³•æ¯”è¾ƒå™¨
        print("\næµ‹è¯•TDæ§åˆ¶ç®—æ³•æ¯”è¾ƒå™¨...")
        comparator = TDControlComparator(env)
        results = comparator.run_comparison(
            n_episodes=20,
            n_runs=2,
            gamma=0.9,
            alpha=0.1,
            epsilon=0.1,
            verbose=False
        )
        
        assert 'SARSA' in results, "æ¯”è¾ƒç»“æœç¼ºå°‘SARSA"
        assert 'Q-Learning' in results, "æ¯”è¾ƒç»“æœç¼ºå°‘Q-Learning"
        assert 'Expected SARSA' in results, "æ¯”è¾ƒç»“æœç¼ºå°‘Expected SARSA"
        print("âœ“ TDæ§åˆ¶ç®—æ³•æ¯”è¾ƒå™¨æµ‹è¯•é€šè¿‡")
        
        # æ¯”è¾ƒä¸‰ç§ç®—æ³•çš„ç»“æœ
        print("\nç®—æ³•æ¯”è¾ƒ:")
        print(f"  SARSAå¹³å‡Qå€¼: {np.mean(q_values_sarsa):.3f}")
        print(f"  Q-Learningå¹³å‡Qå€¼: {np.mean(q_values_ql):.3f}")
        print(f"  Expected SARSAå¹³å‡Qå€¼: {np.mean(q_values_exp):.3f}")
        
        print("\nâœ… TDæ§åˆ¶ç®—æ³•æµ‹è¯•å…¨éƒ¨é€šè¿‡ï¼")
        return True
        
    except Exception as e:
        print(f"\nâŒ TDæ§åˆ¶ç®—æ³•æµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return False


def test_n_step_td():
    """
    æµ‹è¯•n-step TDæ–¹æ³•
    Test n-step TD Methods
    """
    print("\n" + "="*60)
    print("æµ‹è¯•n-step TDæ–¹æ³•...")
    print("Testing n-step TD Methods...")
    print("="*60)
    
    try:
        from src.ch06_temporal_difference.n_step_td import (
            NStepExperience, NStepTD, NStepSARSA, NStepComparator
        )
        from src.ch03_finite_mdp.gridworld import GridWorld
        from src.ch03_finite_mdp.policies_and_values import UniformRandomPolicy
        from src.ch03_finite_mdp.mdp_framework import State, Action
        
        # æµ‹è¯•n-stepç»éªŒ
        print("æµ‹è¯•n-stepç»éªŒ...")
        states = [State(f"s{i}", features={'value': i}) for i in range(4)]
        actions = [Action(f"a{i}", f"Action {i}") for i in range(3)]
        rewards = [1.0, 2.0, 3.0]
        
        n_exp = NStepExperience(states=states[:4], actions=actions, rewards=rewards)
        assert n_exp.n == 3, "nå€¼è®¡ç®—é”™è¯¯"
        
        g = n_exp.compute_n_step_return(gamma=0.9, final_value=10.0)
        expected_g = 1.0 + 0.9*2.0 + 0.81*3.0 + 0.729*10.0
        assert abs(g - expected_g) < 0.001, f"n-stepå›æŠ¥è®¡ç®—é”™è¯¯: {g} vs {expected_g}"
        print("âœ“ n-stepç»éªŒæµ‹è¯•é€šè¿‡")
        
        # æµ‹è¯•n-step TDé¢„æµ‹
        print("\næµ‹è¯•n-step TDé¢„æµ‹...")
        env = GridWorld(rows=3, cols=3, start_pos=(0,0), goal_pos=(2,2))
        policy = UniformRandomPolicy(env.action_space)
        
        for n in [1, 3, 5]:
            n_step_td = NStepTD(env, n=n, gamma=0.9, alpha=0.1)
            V = n_step_td.learn(policy, n_episodes=30, verbose=False)
            
            # æ£€æŸ¥ä»·å€¼å‡½æ•°
            for state in env.state_space:
                if not state.is_terminal:
                    value = V.get_value(state)
                    assert -100 < value < 100, f"{n}-step TDä»·å€¼å¼‚å¸¸: {value}"
            
            print(f"âœ“ {n}-step TDæµ‹è¯•é€šè¿‡")
        
        # æµ‹è¯•n-step SARSA
        print("\næµ‹è¯•n-step SARSA...")
        n_step_sarsa = NStepSARSA(env, n=3, gamma=0.9, alpha=0.1, epsilon=0.1)
        Q = n_step_sarsa.learn(n_episodes=30, verbose=False)
        
        # æ£€æŸ¥Qå‡½æ•°
        for state in env.state_space:
            if not state.is_terminal:
                for action in env.action_space:
                    q = Q.get_value(state, action)
                    assert -100 < q < 100, f"n-step SARSA Qå€¼å¼‚å¸¸: {q}"
        
        print("âœ“ n-step SARSAæµ‹è¯•é€šè¿‡")
        
        # æµ‹è¯•nå€¼æ¯”è¾ƒå™¨
        print("\næµ‹è¯•nå€¼æ¯”è¾ƒå™¨...")
        comparator = NStepComparator(env)
        results = comparator.compare_n_values(
            n_values=[1, 2, 3],
            n_episodes=20,
            n_runs=2,
            gamma=0.9,
            alpha=0.1,
            verbose=False
        )
        
        assert 1 in results, "æ¯”è¾ƒç»“æœç¼ºå°‘n=1"
        assert 2 in results, "æ¯”è¾ƒç»“æœç¼ºå°‘n=2"
        assert 3 in results, "æ¯”è¾ƒç»“æœç¼ºå°‘n=3"
        
        for n, data in results.items():
            assert 'final_return_mean' in data, f"n={n}ç¼ºå°‘æœ€ç»ˆå›æŠ¥"
            assert 'convergence_mean' in data, f"n={n}ç¼ºå°‘æ”¶æ•›ä¿¡æ¯"
        
        print("âœ“ nå€¼æ¯”è¾ƒå™¨æµ‹è¯•é€šè¿‡")
        
        print("\nâœ… n-step TDæ–¹æ³•æµ‹è¯•å…¨éƒ¨é€šè¿‡ï¼")
        return True
        
    except Exception as e:
        print(f"\nâŒ n-step TDæ–¹æ³•æµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return False


def test_convergence_comparison():
    """
    æµ‹è¯•TDæ–¹æ³•çš„æ”¶æ•›æ€§æ¯”è¾ƒ
    Test convergence comparison of TD methods
    """
    print("\n" + "="*60)
    print("æµ‹è¯•TDæ–¹æ³•æ”¶æ•›æ€§æ¯”è¾ƒ...")
    print("Testing TD Methods Convergence Comparison...")
    print("="*60)
    
    try:
        from src.ch06_temporal_difference.td_foundations import TD0
        from src.ch06_temporal_difference.td_control import SARSA, QLearning
        from src.ch06_temporal_difference.n_step_td import NStepTD
        from src.ch03_finite_mdp.gridworld import GridWorld
        from src.ch03_finite_mdp.policies_and_values import UniformRandomPolicy
        
        # åˆ›å»ºç¯å¢ƒ
        env = GridWorld(rows=4, cols=4, start_pos=(0,0), goal_pos=(3,3))
        policy = UniformRandomPolicy(env.action_space)
        
        print("æ¯”è¾ƒä¸åŒTDæ–¹æ³•çš„æ”¶æ•›é€Ÿåº¦...")
        
        # TD(0)
        td0 = TD0(env, gamma=0.9, alpha=0.1)
        td0_returns = []
        for _ in range(50):
            ret = td0.learn_episode(policy)
            td0_returns.append(ret)
        
        # 3-step TD
        n_step_td = NStepTD(env, n=3, gamma=0.9, alpha=0.1)
        n_step_returns = []
        for _ in range(50):
            ret = n_step_td.learn_episode(policy)
            n_step_returns.append(ret)
        
        # SARSA
        sarsa = SARSA(env, gamma=0.9, alpha=0.1, epsilon=0.1)
        for _ in range(50):
            sarsa.learn_episode()
        sarsa_returns = sarsa.episode_returns
        
        # Q-Learning
        qlearning = QLearning(env, gamma=0.9, alpha=0.1, epsilon=0.1)
        for _ in range(50):
            qlearning.learn_episode()
        ql_returns = qlearning.episode_returns
        
        # åˆ†ææ”¶æ•›
        print("\næ”¶æ•›åˆ†æï¼ˆæœ€å10å›åˆå¹³å‡ï¼‰ï¼š")
        print(f"  TD(0): {np.mean(td0_returns[-10:]):.3f}")
        print(f"  3-step TD: {np.mean(n_step_returns[-10:]):.3f}")
        print(f"  SARSA: {np.mean(sarsa_returns[-10:]):.3f}")
        print(f"  Q-Learning: {np.mean(ql_returns[-10:]):.3f}")
        
        # æ£€æŸ¥æ˜¯å¦éƒ½åœ¨åˆç†èŒƒå›´
        for name, returns in [("TD(0)", td0_returns),
                              ("3-step TD", n_step_returns),
                              ("SARSA", sarsa_returns),
                              ("Q-Learning", ql_returns)]:
            avg = np.mean(returns[-10:]) if len(returns) >= 10 else np.mean(returns)
            assert -100 < avg < 100, f"{name}æ”¶æ•›å€¼å¼‚å¸¸: {avg}"
        
        print("\nâœ… TDæ–¹æ³•æ”¶æ•›æ€§æ¯”è¾ƒæµ‹è¯•é€šè¿‡ï¼")
        return True
        
    except Exception as e:
        print(f"\nâŒ TDæ–¹æ³•æ”¶æ•›æ€§æ¯”è¾ƒæµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return False


def test_td_vs_mc_comparison():
    """
    æµ‹è¯•TDä¸MCçš„æ¯”è¾ƒ
    Test TD vs MC Comparison
    """
    print("\n" + "="*60)
    print("æµ‹è¯•TDä¸MCæ¯”è¾ƒ...")
    print("Testing TD vs MC Comparison...")
    print("="*60)
    
    try:
        from src.ch06_temporal_difference.td_foundations import TD0
        from src.ch05_monte_carlo.mc_prediction import FirstVisitMC
        from src.ch03_finite_mdp.gridworld import GridWorld
        from src.ch03_finite_mdp.policies_and_values import UniformRandomPolicy
        
        # åˆ›å»ºç¯å¢ƒ
        env = GridWorld(rows=3, cols=3, start_pos=(0,0), goal_pos=(2,2))
        policy = UniformRandomPolicy(env.action_space)
        
        print("è¿è¡ŒTD(0)...")
        td0 = TD0(env, gamma=0.9, alpha=0.1)
        V_td = td0.learn(policy, n_episodes=100, verbose=False)
        
        print("è¿è¡ŒFirst-Visit MC...")
        mc = FirstVisitMC(env, gamma=0.9)
        V_mc = mc.estimate_V(policy, n_episodes=100, verbose=False)
        
        # æ¯”è¾ƒä»·å€¼å‡½æ•°
        print("\nä»·å€¼å‡½æ•°æ¯”è¾ƒï¼ˆéƒ¨åˆ†çŠ¶æ€ï¼‰ï¼š")
        differences = []
        
        for i, state in enumerate(env.state_space[:5]):
            if not state.is_terminal:
                td_value = V_td.get_value(state)
                mc_value = V_mc.get_value(state)
                diff = abs(td_value - mc_value)
                differences.append(diff)
                
                print(f"  State {state.id}: TD={td_value:.3f}, MC={mc_value:.3f}, Diff={diff:.3f}")
        
        # å¹³å‡å·®å¼‚åº”è¯¥ä¸å¤ªå¤§ï¼ˆéƒ½æ”¶æ•›åˆ°V^Ï€ï¼‰
        avg_diff = np.mean(differences)
        print(f"\nå¹³å‡å·®å¼‚: {avg_diff:.3f}")
        assert avg_diff < 5.0, f"TDå’ŒMCå·®å¼‚è¿‡å¤§: {avg_diff}"
        
        print("âœ… TDä¸MCæ¯”è¾ƒæµ‹è¯•é€šè¿‡ï¼")
        return True
        
    except Exception as e:
        print(f"\nâŒ TDä¸MCæ¯”è¾ƒæµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return False


def main():
    """
    è¿è¡Œæ‰€æœ‰æµ‹è¯•
    Run all tests
    """
    print("\n" + "="*80)
    print("ç¬¬5ç« ï¼šæ—¶åºå·®åˆ†å­¦ä¹  - æ¨¡å—æµ‹è¯•")
    print("Chapter 5: Temporal-Difference Learning - Module Tests")
    print("="*80)
    
    tests = [
        ("TDåŸºç¡€ç†è®º", test_td_foundations),
        ("TDæ§åˆ¶ç®—æ³•", test_td_control),
        ("n-step TDæ–¹æ³•", test_n_step_td),
        ("æ”¶æ•›æ€§æ¯”è¾ƒ", test_convergence_comparison),
        ("TD vs MCæ¯”è¾ƒ", test_td_vs_mc_comparison)
    ]
    
    results = []
    start_time = time.time()
    
    for name, test_func in tests:
        print(f"\nè¿è¡Œæµ‹è¯•: {name}")
        success = test_func()
        results.append((name, success))
        
        if not success:
            print(f"\nâš ï¸ æµ‹è¯•å¤±è´¥ï¼Œåœæ­¢åç»­æµ‹è¯•")
            break
    
    total_time = time.time() - start_time
    
    # æ€»ç»“
    print("\n" + "="*80)
    print("æµ‹è¯•æ€»ç»“ Test Summary")
    print("="*80)
    
    all_passed = True
    for name, success in results:
        status = "âœ… é€šè¿‡" if success else "âŒ å¤±è´¥"
        print(f"{name}: {status}")
        if not success:
            all_passed = False
    
    print(f"\næ€»æµ‹è¯•æ—¶é—´: {total_time:.2f}ç§’")
    
    if all_passed:
        print("\nğŸ‰ ç¬¬5ç« æ‰€æœ‰TDæ¨¡å—æµ‹è¯•é€šè¿‡ï¼")
        print("ğŸ‰ All Chapter 5 TD modules passed!")
        print("\næ—¶åºå·®åˆ†å­¦ä¹ å®ç°éªŒè¯å®Œæˆ:")
        print("âœ“ TDåŸºç¡€ï¼ˆTD(0)ã€TDè¯¯å·®ã€æ”¶æ•›æ€§ï¼‰")
        print("âœ“ TDæ§åˆ¶ï¼ˆSARSAã€Q-learningã€Expected SARSAï¼‰")
        print("âœ“ n-step TDï¼ˆç»Ÿä¸€MCå’ŒTDï¼‰")
        print("âœ“ ç®—æ³•æ¯”è¾ƒå’Œåˆ†æ")
        print("\nè¿™æ˜¯å¼ºåŒ–å­¦ä¹ æœ€æ ¸å¿ƒçš„å†…å®¹ï¼")
        print("This is the core of reinforcement learning!")
        print("\nå¯ä»¥ç»§ç»­å­¦ä¹ ç¬¬6ç« ï¼šTD(Î»)å’Œèµ„æ ¼è¿¹")
        print("Ready to proceed to Chapter 6: TD(Î») and Eligibility Traces")
    else:
        print("\nâš ï¸ æœ‰äº›æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä»£ç ")
        print("âš ï¸ Some tests failed, please check the code")
    
    return all_passed


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)