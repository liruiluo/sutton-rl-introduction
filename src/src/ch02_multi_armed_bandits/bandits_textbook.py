"""
================================================================================
ç¬¬2ç« ï¼šå¤šè‡‚èµŒåšæœºé—®é¢˜ - æ¢ç´¢ä¸åˆ©ç”¨çš„æ°¸æ’å›°å¢ƒ
Chapter 2: Multi-Armed Bandits - The Eternal Dilemma of Exploration vs Exploitation

æ ¹æ® Sutton & Bartoã€Šå¼ºåŒ–å­¦ä¹ ï¼šå¯¼è®ºã€‹ç¬¬äºŒç‰ˆ ç¬¬2ç« 
Based on Sutton & Barto "Reinforcement Learning: An Introduction" Chapter 2
================================================================================

è®©æˆ‘ç”¨ä¸€ä¸ªæ•…äº‹å¼€å§‹è¿™ä¸€ç« ï¼š

ä½ èµ°è¿›æ‹‰æ–¯ç»´åŠ æ–¯çš„èµŒåœºï¼Œé¢å‰æœ‰ä¸€æ’è€è™æœºï¼ˆslot machinesï¼‰ã€‚
æ¯å°æœºå™¨çœ‹èµ·æ¥éƒ½ä¸€æ ·ï¼Œä½†ä½ çŸ¥é“å®ƒä»¬çš„èµ”ç‡ä¸åŒã€‚

ä½ æœ‰1000ä¸ªç¡¬å¸ï¼Œç›®æ ‡æ˜¯èµšæœ€å¤šçš„é’±ã€‚
é—®é¢˜æ˜¯ï¼šä½ è¯¥æ€ä¹ˆç©ï¼Ÿ

ç­–ç•¥1ï¼šéšæœºé€‰æ‹©æœºå™¨ï¼ˆçº¯æ¢ç´¢ï¼‰
  - ä¼˜ç‚¹ï¼šèƒ½è¯•éæ‰€æœ‰æœºå™¨
  - ç¼ºç‚¹ï¼šæµªè´¹å¤§é‡ç¡¬å¸åœ¨å·®æœºå™¨ä¸Š

ç­–ç•¥2ï¼šæ‰¾åˆ°ä¸€å°è¿˜ä¸é”™çš„å°±ä¸€ç›´ç©ï¼ˆçº¯åˆ©ç”¨ï¼‰
  - ä¼˜ç‚¹ï¼šä¸ä¼šåœ¨æ˜æ˜¾å¾ˆå·®çš„æœºå™¨ä¸Šæµªè´¹
  - ç¼ºç‚¹ï¼šå¯èƒ½é”™è¿‡æœ€å¥½çš„æœºå™¨

ç­–ç•¥3ï¼šèªæ˜åœ°å¹³è¡¡æ¢ç´¢å’Œåˆ©ç”¨ï¼ˆè¿™ç« çš„ä¸»é¢˜ï¼ï¼‰

è¿™å°±æ˜¯å¤šè‡‚èµŒåšæœºé—®é¢˜ï¼ˆMulti-Armed Bandit Problemï¼‰ï¼

================================================================================
ä¸ºä»€ä¹ˆå¤šè‡‚èµŒåšæœºé—®é¢˜å¦‚æ­¤é‡è¦ï¼Ÿ
Why Multi-Armed Bandits Matter?
================================================================================

Sutton & Bartoè¯´ï¼ˆç¬¬25é¡µï¼‰ï¼š
"The most important feature distinguishing reinforcement learning from other types 
of learning is that it uses training information that evaluates the actions taken 
rather than instructs by giving correct actions."

å¤šè‡‚èµŒåšæœºé—®é¢˜çš„ç‰¹ç‚¹ï¼š
1. ç®€åŒ–çš„å¼ºåŒ–å­¦ä¹ ï¼šåªæœ‰ä¸€ä¸ªçŠ¶æ€
2. æ ¸å¿ƒå›°å¢ƒæ¸…æ™°ï¼šæ¢ç´¢vsåˆ©ç”¨
3. ç†è®ºåŸºç¡€æ‰å®ï¼šæœ‰é—æ†¾ç•Œï¼ˆregret boundï¼‰ç­‰ç†è®º
4. åº”ç”¨å¹¿æ³›ï¼šæ¨èç³»ç»Ÿã€ä¸´åºŠè¯•éªŒã€åœ¨çº¿å¹¿å‘Š

ç†è§£äº†å¤šè‡‚èµŒåšæœºï¼Œå°±ç†è§£äº†å¼ºåŒ–å­¦ä¹ çš„çµé­‚ï¼
"""

import numpy as np
from typing import List, Tuple, Optional, Dict, Any
import matplotlib.pyplot as plt
from dataclasses import dataclass
from abc import ABC, abstractmethod
import warnings


# ================================================================================
# ç¬¬2.1èŠ‚ï¼šå¤šè‡‚èµŒåšæœºé—®é¢˜å®šä¹‰
# Section 2.1: Multi-Armed Bandit Problem Definition
# ================================================================================

class KArmedBandit:
    """
    kè‡‚èµŒåšæœº - å¼ºåŒ–å­¦ä¹ æœ€ç®€å•çš„å½¢å¼
    
    è¿™å°±åƒèµŒåœºé‡Œçš„kå°è€è™æœºï¼š
    - æ¯å°æœºå™¨æœ‰è‡ªå·±çš„æœŸæœ›æ”¶ç›Šï¼ˆä½ ä¸çŸ¥é“ï¼‰
    - æ¯æ¬¡æ‹‰æ†è·å¾—çš„å¥–åŠ±æ˜¯éšæœºçš„
    - ä½ çš„ä»»åŠ¡ï¼šæ‰¾å‡ºå¹¶åˆ©ç”¨æœ€å¥½çš„æœºå™¨
    
    æ•°å­¦å®šä¹‰ï¼ˆSutton & Barto ç¬¬28é¡µï¼‰ï¼š
    - kä¸ªåŠ¨ä½œï¼Œæ¯ä¸ªå¯¹åº”ä¸€ä¸ªæœŸæœ›å¥–åŠ± q*(a)
    - é€‰æ‹©åŠ¨ä½œaï¼Œè·å¾—å¥–åŠ±Rï¼ŒæœŸæœ›å€¼E[R|A=a] = q*(a)
    - ç›®æ ‡ï¼šæœ€å¤§åŒ–æ€»æœŸæœ›å¥–åŠ±
    
    å…³é”®æŒ‘æˆ˜ï¼š
    ä½ ä¸çŸ¥é“q*(a)çš„çœŸå®å€¼ï¼Œå¿…é¡»é€šè¿‡å°è¯•æ¥å­¦ä¹ ï¼
    """
    
    def __init__(self, k: int = 10, stationary: bool = True, 
                 initial_mean: float = 0.0, initial_std: float = 1.0):
        """
        åˆå§‹åŒ–kè‡‚èµŒåšæœº
        
        å‚æ•°è§£é‡Šï¼ˆä¸ä¹¦ä¸­Figure 2.1å¯¹åº”ï¼‰ï¼š
        k: è‡‚çš„æ•°é‡ï¼ˆé»˜è®¤10ï¼Œä¹¦ä¸­æ ‡å‡†è®¾ç½®ï¼‰
        stationary: æ˜¯å¦å¹³ç¨³ï¼ˆTrue=èµŒåœºæœºå™¨å›ºå®šï¼ŒFalse=æœºå™¨ä¼šå˜ï¼‰
        initial_mean: çœŸå®ä»·å€¼çš„å‡å€¼ï¼ˆä¹¦ä¸­ç”¨0ï¼‰
        initial_std: çœŸå®ä»·å€¼çš„æ ‡å‡†å·®ï¼ˆä¹¦ä¸­ç”¨1ï¼‰
        
        ä¸ºä»€ä¹ˆè¿™äº›å‚æ•°é‡è¦ï¼Ÿ
        - k=10 è¶³å¤Ÿå¤æ‚ä½†åˆä¸ä¼šå¤ªå¤æ‚
        - stationary å†³å®šæ˜¯å¦éœ€è¦æŒç»­æ¢ç´¢
        - initial_mean=0, std=1 åˆ›å»ºæ ‡å‡†æµ‹è¯•ç¯å¢ƒ
        """
        self.k = k
        self.stationary = stationary
        
        # æ¯ä¸ªè‡‚çš„çœŸå®ä»·å€¼q*(a)
        # ä»æ­£æ€åˆ†å¸ƒN(0,1)é‡‡æ ·ï¼Œè¿™æ˜¯ä¹¦ä¸­çš„æ ‡å‡†è®¾ç½®
        self.q_star = np.random.normal(initial_mean, initial_std, k)
        
        # æœ€ä¼˜åŠ¨ä½œå’Œæœ€ä¼˜ä»·å€¼
        self.optimal_action = np.argmax(self.q_star)
        self.optimal_value = np.max(self.q_star)
        
        # è®°å½•ç»Ÿè®¡ä¿¡æ¯
        self.action_counts = np.zeros(k)  # æ¯ä¸ªè‡‚è¢«æ‹‰çš„æ¬¡æ•°
        self.total_steps = 0
        
        print(f"åˆ›å»ºäº†{k}è‡‚èµŒåšæœº")
        print(f"æœ€ä¼˜è‡‚æ˜¯ç¬¬{self.optimal_action}ä¸ªï¼ŒæœŸæœ›æ”¶ç›Š{self.optimal_value:.3f}")
        
    def step(self, action: int) -> float:
        """
        æ‹‰åŠ¨ç¬¬actionä¸ªè‡‚ï¼Œè¿”å›å¥–åŠ±
        
        è¿™æ¨¡æ‹Ÿäº†çœŸå®ä¸–ç•Œçš„éšæœºæ€§ï¼š
        å³ä½¿æ˜¯æœ€å¥½çš„è€è™æœºï¼Œä¹Ÿä¸æ˜¯æ¯æ¬¡éƒ½èµ¢ï¼
        
        å¥–åŠ±ç”Ÿæˆï¼ˆä¹¦ä¸­å…¬å¼2.1ï¼‰ï¼š
        R_t ~ N(q*(A_t), 1)
        """
        if action < 0 or action >= self.k:
            raise ValueError(f"åŠ¨ä½œ{action}è¶…å‡ºèŒƒå›´[0, {self.k})")
        
        # éå¹³ç¨³æƒ…å†µï¼šçœŸå®ä»·å€¼ä¼šæ¼‚ç§»ï¼ˆç»ƒä¹ 2.5ï¼‰
        if not self.stationary:
            # éšæœºæ¸¸èµ°ï¼šæ¯æ­¥åŠ å°é‡éšæœºå™ªå£°
            self.q_star += np.random.normal(0, 0.01, self.k)
            self.optimal_action = np.argmax(self.q_star)
            self.optimal_value = np.max(self.q_star)
        
        # ç”Ÿæˆå¥–åŠ±ï¼šæœŸæœ›å€¼q*(a)åŠ ä¸Šå™ªå£°
        reward = np.random.normal(self.q_star[action], 1.0)
        
        # æ›´æ–°ç»Ÿè®¡
        self.action_counts[action] += 1
        self.total_steps += 1
        
        return reward
    
    def get_regret(self, action: int) -> float:
        """
        è®¡ç®—é—æ†¾å€¼ï¼ˆé€‰æ‹©actionè€Œéæœ€ä¼˜åŠ¨ä½œçš„æŸå¤±ï¼‰
        
        é—æ†¾(Regret) = q* - q(a)
        
        è¿™æ˜¯ç†è®ºåˆ†æçš„æ ¸å¿ƒæ¦‚å¿µï¼
        ç´¯ç§¯é—æ†¾è¡¡é‡ç®—æ³•çš„æ€§èƒ½ã€‚
        """
        return self.optimal_value - self.q_star[action]
    
    def visualize_true_values(self):
        """å¯è§†åŒ–çœŸå®ä»·å€¼åˆ†å¸ƒ"""
        plt.figure(figsize=(10, 5))
        colors = ['red' if i == self.optimal_action else 'blue' 
                 for i in range(self.k)]
        bars = plt.bar(range(self.k), self.q_star, color=colors)
        plt.axhline(y=0, color='black', linestyle='-', linewidth=0.5)
        plt.xlabel('åŠ¨ä½œ Action')
        plt.ylabel('çœŸå®ä»·å€¼ True Value q*(a)')
        plt.title('å¤šè‡‚èµŒåšæœºçš„çœŸå®ä»·å€¼åˆ†å¸ƒ\n(çº¢è‰²æ˜¯æœ€ä¼˜è‡‚)')
        plt.grid(True, alpha=0.3)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for i, (bar, value) in enumerate(zip(bars, self.q_star)):
            plt.text(bar.get_x() + bar.get_width()/2, value,
                    f'{value:.2f}', ha='center', va='bottom')
        plt.tight_layout()
        plt.show()


# ================================================================================
# ç¬¬2.2èŠ‚ï¼šåŠ¨ä½œä»·å€¼ä¼°è®¡
# Section 2.2: Action-Value Methods
# ================================================================================

class ActionValueEstimator:
    """
    åŠ¨ä½œä»·å€¼ä¼°è®¡ - å­¦ä¹ æ¯ä¸ªåŠ¨ä½œçš„ä»·å€¼
    
    æ ¸å¿ƒæ€æƒ³ï¼ˆä¹¦ä¸­ç¬¬28é¡µï¼‰ï¼š
    æˆ‘ä»¬ä¸çŸ¥é“çœŸå®ä»·å€¼q*(a)ï¼Œä½†å¯ä»¥ä¼°è®¡å®ƒï¼
    
    ä¼°è®¡å€¼ Q_t(a) â‰ˆ q*(a)
    
    å¦‚ä½•ä¼°è®¡ï¼Ÿæ ·æœ¬å¹³å‡æ³•ï¼ˆæœ€è‡ªç„¶çš„æ–¹æ³•ï¼‰ï¼š
    Q_t(a) = (R_1 + R_2 + ... + R_{N_t(a)}) / N_t(a)
    
    å…¶ä¸­N_t(a)æ˜¯åˆ°æ—¶åˆ»tä¸ºæ­¢é€‰æ‹©åŠ¨ä½œaçš„æ¬¡æ•°
    
    ç›´è§‚ç†è§£ï¼š
    å°±åƒè¯„ä»·é¤å…ï¼Œå»çš„æ¬¡æ•°è¶Šå¤šï¼Œè¯„åˆ†è¶Šå‡†ç¡®ï¼
    """
    
    def __init__(self, k: int, initial_value: float = 0.0, 
                 optimistic: bool = False, alpha: Optional[float] = None):
        """
        åˆå§‹åŒ–åŠ¨ä½œä»·å€¼ä¼°è®¡å™¨
        
        å‚æ•°çš„æ·±å±‚å«ä¹‰ï¼š
        
        initial_value: åˆå§‹ä¼°è®¡å€¼Q_1(a)
          - 0.0 = ä¸­ç«‹ï¼ˆä¸åä¸å€šï¼‰
          - 5.0 = ä¹è§‚ï¼ˆå‡è®¾éƒ½å¾ˆå¥½ï¼Œé¼“åŠ±æ¢ç´¢ï¼‰
          - -5.0 = æ‚²è§‚ï¼ˆå‡è®¾éƒ½å¾ˆå·®ï¼Œå‡å°‘æ¢ç´¢ï¼‰
          
        optimistic: æ˜¯å¦ä½¿ç”¨ä¹è§‚åˆå§‹å€¼ï¼ˆä¹¦ä¸­ç¬¬32é¡µï¼‰
          - True: è®¾ç½®Q_1(a) = 5ï¼Œè¿œé«˜äºå®é™…å€¼
          - æ•ˆæœï¼šè‡ªç„¶åœ°é¼“åŠ±æ¢ç´¢ï¼
          
        alpha: å­¦ä¹ ç‡ï¼ˆNone=æ ·æœ¬å¹³å‡ï¼Œå›ºå®šå€¼=æŒ‡æ•°åŠ æƒï¼‰
          - None: ä½¿ç”¨1/nï¼Œç»™æ‰€æœ‰å†å²åŒç­‰æƒé‡
          - 0.1: å›ºå®šå­¦ä¹ ç‡ï¼Œæ›´é‡è§†æœ€è¿‘çš„å¥–åŠ±
        """
        self.k = k
        self.alpha = alpha
        
        # åˆå§‹åŒ–ä»·å€¼ä¼°è®¡
        if optimistic:
            # ä¹è§‚åˆå§‹åŒ–ï¼ˆç»ƒä¹ 2.6ï¼‰
            self.Q = np.ones(k) * 5.0
            print("ä½¿ç”¨ä¹è§‚åˆå§‹å€¼ï¼šå‡è®¾æ¯ä¸ªè‡‚éƒ½å¾ˆå¥½(Q=5)")
        else:
            self.Q = np.ones(k) * initial_value
            
        # è®°å½•æ¯ä¸ªåŠ¨ä½œè¢«é€‰æ‹©çš„æ¬¡æ•°
        self.N = np.zeros(k)
        
    def update(self, action: int, reward: float):
        """
        æ›´æ–°åŠ¨ä½œä»·å€¼ä¼°è®¡ - å¼ºåŒ–å­¦ä¹ çš„æ ¸å¿ƒï¼
        
        å¢é‡æ›´æ–°å…¬å¼ï¼ˆä¹¦ä¸­å…¬å¼2.3ï¼Œæœ€é‡è¦çš„å…¬å¼ä¹‹ä¸€ï¼‰ï¼š
        Q_{n+1} = Q_n + Î±[R_n - Q_n]
        
        å…¶ä¸­ï¼š
        - Q_n: å½“å‰ä¼°è®¡
        - R_n: æ–°è·å¾—çš„å¥–åŠ±
        - Î±: æ­¥é•¿/å­¦ä¹ ç‡
        - [R_n - Q_n]: é¢„æµ‹è¯¯å·®ï¼ˆTDè¯¯å·®çš„å‰èº«ï¼‰
        
        æ·±å±‚ç†è§£ï¼š
        è¿™ä¸ªå…¬å¼è´¯ç©¿æ•´ä¸ªå¼ºåŒ–å­¦ä¹ ï¼
        - æ–°ä¼°è®¡ = è€ä¼°è®¡ + æ­¥é•¿ Ã— è¯¯å·®
        - è¯¯å·®å¤§ â†’ è°ƒæ•´å¤§
        - è¯¯å·®å° â†’ è°ƒæ•´å°
        - è¯¯å·®ä¸º0 â†’ ä¸è°ƒæ•´ï¼ˆå·²æ”¶æ•›ï¼‰
        """
        self.N[action] += 1
        
        # ç¡®å®šå­¦ä¹ ç‡
        if self.alpha is None:
            # æ ·æœ¬å¹³å‡ï¼šÎ± = 1/n
            # ä¿è¯æ”¶æ•›åˆ°çœŸå®å€¼ï¼ˆå¤§æ•°å®šå¾‹ï¼‰
            alpha = 1.0 / self.N[action]
        else:
            # å›ºå®šå­¦ä¹ ç‡ï¼šé€‚åº”éå¹³ç¨³ç¯å¢ƒ
            alpha = self.alpha
            
        # å¢é‡æ›´æ–°ï¼ˆé¿å…å­˜å‚¨æ‰€æœ‰å†å²ï¼‰
        prediction_error = reward - self.Q[action]
        self.Q[action] += alpha * prediction_error
        
        return prediction_error  # è¿”å›è¯¯å·®ç”¨äºåˆ†æ
    
    def get_value(self, action: int) -> float:
        """è·å–åŠ¨ä½œçš„ä¼°è®¡ä»·å€¼"""
        return self.Q[action]
    
    def get_best_action(self) -> int:
        """
        è·å–å½“å‰æœ€ä½³åŠ¨ä½œï¼ˆè´ªå©ªé€‰æ‹©ï¼‰
        
        å¦‚æœæœ‰å¤šä¸ªæœ€ä¼˜ï¼Œéšæœºé€‰ä¸€ä¸ªï¼ˆæ‰“ç ´å¯¹ç§°æ€§ï¼‰
        """
        max_value = np.max(self.Q)
        best_actions = np.where(self.Q == max_value)[0]
        return np.random.choice(best_actions)


# ================================================================================
# ç¬¬2.3èŠ‚ï¼šæ¢ç´¢ç­–ç•¥ - å¦‚ä½•å¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨
# Section 2.3: Exploration Strategies - Balancing Exploration and Exploitation
# ================================================================================

class ExplorationStrategy(ABC):
    """
    æ¢ç´¢ç­–ç•¥çš„æŠ½è±¡åŸºç±»
    
    è¿™æ˜¯å¤šè‡‚èµŒåšæœºçš„æ ¸å¿ƒå†³ç­–ï¼
    æ¯ç§ç­–ç•¥ä»£è¡¨ä¸€ç§æ¢ç´¢ä¸åˆ©ç”¨çš„å¹³è¡¡å“²å­¦ã€‚
    """
    
    @abstractmethod
    def select_action(self, Q: np.ndarray, N: np.ndarray, t: int) -> int:
        """é€‰æ‹©åŠ¨ä½œçš„æ¥å£"""
        pass
    
    @abstractmethod
    def get_name(self) -> str:
        """ç­–ç•¥åç§°"""
        pass


class EpsilonGreedy(ExplorationStrategy):
    """
    Îµ-è´ªå©ªç­–ç•¥ - æœ€ç®€å•ä½†æœ€å®ç”¨çš„ç­–ç•¥ï¼
    
    ç­–ç•¥ï¼ˆä¹¦ä¸­ç¬¬29é¡µï¼‰ï¼š
    - æ¦‚ç‡Îµï¼šéšæœºæ¢ç´¢ï¼ˆè¯•è¯•æ–°é¤å…ï¼‰
    - æ¦‚ç‡1-Îµï¼šé€‰æ‹©å½“å‰æœ€ä¼˜ï¼ˆå»æœ€å–œæ¬¢çš„é¤å…ï¼‰
    
    ä¸ºä»€ä¹ˆæœ‰æ•ˆï¼Ÿ
    1. ä¿è¯æ¢ç´¢ï¼šæ¯ä¸ªåŠ¨ä½œéƒ½æœ‰æœºä¼šè¢«é€‰ä¸­
    2. ä¿è¯åˆ©ç”¨ï¼šå¤§éƒ¨åˆ†æ—¶é—´é€‰æ‹©å½“å‰æœ€ä¼˜
    3. ç®€å•å¯æ§ï¼šä¸€ä¸ªå‚æ•°æ§åˆ¶å¹³è¡¡
    
    å…³é”®æƒè¡¡ï¼š
    - Îµå¤ªå¤§ï¼šæ¢ç´¢å¤ªå¤šï¼Œæµªè´¹åœ¨å·®åŠ¨ä½œä¸Š
    - Îµå¤ªå°ï¼šæ¢ç´¢ä¸è¶³ï¼Œå¯èƒ½é”™è¿‡æœ€ä¼˜
    - å…¸å‹å€¼ï¼šÎµ=0.1ï¼ˆ10%æ¢ç´¢ï¼‰
    """
    
    def __init__(self, epsilon: float = 0.1, decay: bool = False):
        """
        åˆå§‹åŒ–Îµ-è´ªå©ªç­–ç•¥
        
        epsilon: æ¢ç´¢æ¦‚ç‡
        decay: æ˜¯å¦è¡°å‡Îµï¼ˆå¼€å§‹å¤šæ¢ç´¢ï¼ŒåæœŸå¤šåˆ©ç”¨ï¼‰
        """
        self.epsilon = epsilon
        self.initial_epsilon = epsilon
        self.decay = decay
        
    def select_action(self, Q: np.ndarray, N: np.ndarray, t: int) -> int:
        """
        æ ¹æ®Îµ-è´ªå©ªç­–ç•¥é€‰æ‹©åŠ¨ä½œ
        
        å®ç°ç»†èŠ‚ï¼š
        1. ç”Ÿæˆéšæœºæ•°
        2. å¦‚æœ < Îµï¼Œéšæœºé€‰æ‹©ï¼ˆæ¢ç´¢ï¼‰
        3. å¦åˆ™ï¼Œé€‰æ‹©æœ€ä¼˜ï¼ˆåˆ©ç”¨ï¼‰
        """
        # Îµè¡°å‡ï¼ˆå¯é€‰ï¼‰
        if self.decay and t > 0:
            self.epsilon = self.initial_epsilon / np.sqrt(t)
            
        if np.random.random() < self.epsilon:
            # æ¢ç´¢ï¼šéšæœºé€‰æ‹©
            return np.random.randint(len(Q))
        else:
            # åˆ©ç”¨ï¼šé€‰æ‹©æœ€ä¼˜
            return np.argmax(Q)
    
    def get_name(self) -> str:
        return f"Îµ-Greedy (Îµ={self.initial_epsilon})"


class UCB(ExplorationStrategy):
    """
    ç½®ä¿¡ä¸Šç•Œï¼ˆUpper Confidence Boundï¼‰ç­–ç•¥
    
    æ ¸å¿ƒæ€æƒ³ï¼ˆä¹¦ä¸­ç¬¬35é¡µï¼‰ï¼š
    "ä¹è§‚é¢å¯¹ä¸ç¡®å®šæ€§"ï¼ˆOptimism in the Face of Uncertaintyï¼‰
    
    é€‰æ‹©åŠ¨ä½œï¼š
    A_t = argmax[Q_t(a) + câˆš(ln t / N_t(a))]
    
    ä¸¤éƒ¨åˆ†çš„å«ä¹‰ï¼š
    1. Q_t(a)ï¼šåˆ©ç”¨é¡¹ï¼ˆè¿™ä¸ªåŠ¨ä½œæœ‰å¤šå¥½ï¼‰
    2. câˆš(ln t / N_t(a))ï¼šæ¢ç´¢é¡¹ï¼ˆä¸ç¡®å®šæ€§å¥–åŠ±ï¼‰
       - N_t(a)å° â†’ ä¸ç¡®å®šæ€§å¤§ â†’ æ¢ç´¢å¥–åŠ±å¤§
       - tå¢å¤§ â†’ æ•´ä½“ä¸ç¡®å®šæ€§å¢å¤§ â†’ éœ€è¦æ›´å¤šæ¢ç´¢
    
    ä¸ºä»€ä¹ˆæ¯”Îµ-è´ªå©ªå¥½ï¼Ÿ
    - æ™ºèƒ½æ¢ç´¢ï¼šä¼˜å…ˆæ¢ç´¢ä¸ç¡®å®šçš„åŠ¨ä½œ
    - ç†è®ºä¿è¯ï¼šæœ‰å¯¹æ•°é—æ†¾ç•ŒO(ln t)
    - æ— éœ€è°ƒå‚ï¼šcé€šå¸¸å›ºå®šä¸ºâˆš2
    """
    
    def __init__(self, c: float = 2.0):
        """
        åˆå§‹åŒ–UCBç­–ç•¥
        
        c: æ¢ç´¢ç¨‹åº¦å‚æ•°
        - cå¤§ï¼šæ›´å¤šæ¢ç´¢
        - cå°ï¼šæ›´å¤šåˆ©ç”¨
        - c=âˆš2ï¼šç†è®ºæœ€ä¼˜ï¼ˆHoeffdingä¸ç­‰å¼ï¼‰
        """
        self.c = c
        
    def select_action(self, Q: np.ndarray, N: np.ndarray, t: int) -> int:
        """
        UCBåŠ¨ä½œé€‰æ‹©
        
        ç‰¹æ®Šæƒ…å†µï¼š
        å¦‚æœæŸä¸ªåŠ¨ä½œä»æœªè¢«é€‰æ‹©ï¼ˆN=0ï¼‰ï¼Œä¼˜å…ˆé€‰æ‹©å®ƒï¼
        """
        # å¤„ç†æœªæ¢ç´¢çš„åŠ¨ä½œ
        if 0 in N:
            return np.where(N == 0)[0][0]
            
        # è®¡ç®—æ¯ä¸ªåŠ¨ä½œçš„UCBå€¼
        ucb_values = Q + self.c * np.sqrt(np.log(t) / N)
        
        # é€‰æ‹©UCBæœ€å¤§çš„åŠ¨ä½œ
        return np.argmax(ucb_values)
    
    def get_name(self) -> str:
        return f"UCB (c={self.c})"


class GradientBandit(ExplorationStrategy):
    """
    æ¢¯åº¦èµŒåšæœºç®—æ³• - åŸºäºåå¥½çš„è½¯æœ€å¤§åŒ–
    
    æ ¸å¿ƒæ€æƒ³ï¼ˆä¹¦ä¸­ç¬¬37é¡µï¼‰ï¼š
    ä¸ä¼°è®¡åŠ¨ä½œä»·å€¼ï¼Œè€Œæ˜¯å­¦ä¹ åŠ¨ä½œåå¥½H_t(a)ï¼
    
    æ¦‚ç‡åˆ†å¸ƒï¼ˆè½¯æœ€å¤§åŒ–ï¼‰ï¼š
    Ï€_t(a) = exp(H_t(a)) / Î£_b exp(H_t(b))
    
    æ¢¯åº¦ä¸Šå‡æ›´æ–°ï¼š
    H_{t+1}(a) = H_t(a) + Î±(R_t - RÌ„_t)(ğŸ™_{a=A_t} - Ï€_t(a))
    
    å…¶ä¸­ï¼š
    - RÌ„_tï¼šå¹³å‡å¥–åŠ±ï¼ˆåŸºçº¿ï¼‰
    - ğŸ™_{a=A_t}ï¼šæŒ‡ç¤ºå‡½æ•°
    
    ç›´è§‰ç†è§£ï¼š
    - å¦‚æœå¥–åŠ± > å¹³å‡ï¼šå¢åŠ è¯¥åŠ¨ä½œçš„åå¥½
    - å¦‚æœå¥–åŠ± < å¹³å‡ï¼šå‡å°‘è¯¥åŠ¨ä½œçš„åå¥½
    - å…¶ä»–åŠ¨ä½œçš„åå¥½åå‘è°ƒæ•´
    
    ä¸ºä»€ä¹ˆä½¿ç”¨åå¥½è€Œéä»·å€¼ï¼Ÿ
    1. è‡ªç„¶çš„æ¦‚ç‡åˆ†å¸ƒï¼ˆsoftmaxï¼‰
    2. ç›¸å¯¹æ¯”è¾ƒï¼ˆåªå…³å¿ƒå“ªä¸ªæ›´å¥½ï¼‰
    3. æ¢¯åº¦æ–¹æ³•çš„ç†è®ºåŸºç¡€
    """
    
    def __init__(self, alpha: float = 0.1, use_baseline: bool = True):
        """
        åˆå§‹åŒ–æ¢¯åº¦èµŒåšæœº
        
        alpha: å­¦ä¹ ç‡
        use_baseline: æ˜¯å¦ä½¿ç”¨åŸºçº¿ï¼ˆå¹³å‡å¥–åŠ±ï¼‰
        """
        self.alpha = alpha
        self.use_baseline = use_baseline
        self.H = None  # åå¥½å‘é‡
        self.avg_reward = 0.0  # å¹³å‡å¥–åŠ±
        self.n = 0  # æ­¥æ•°
        
    def select_action(self, Q: np.ndarray, N: np.ndarray, t: int) -> int:
        """
        åŸºäºåå¥½çš„åŠ¨ä½œé€‰æ‹©
        
        ä½¿ç”¨softmaxå°†åå¥½è½¬æ¢ä¸ºæ¦‚ç‡
        """
        k = len(Q)
        
        # åˆå§‹åŒ–åå¥½ï¼ˆå…¨0 = å‡åŒ€æ¦‚ç‡ï¼‰
        if self.H is None:
            self.H = np.zeros(k)
            
        # è®¡ç®—åŠ¨ä½œæ¦‚ç‡ï¼ˆsoftmaxï¼‰
        exp_H = np.exp(self.H - np.max(self.H))  # æ•°å€¼ç¨³å®šæ€§
        pi = exp_H / np.sum(exp_H)
        
        # ä¾æ¦‚ç‡é€‰æ‹©åŠ¨ä½œ
        return np.random.choice(k, p=pi)
    
    def update_preference(self, action: int, reward: float):
        """
        æ›´æ–°åå¥½ï¼ˆè¿™é€šå¸¸åœ¨ä¸»å¾ªç¯ä¸­è°ƒç”¨ï¼‰
        
        æ¢¯åº¦ä¸Šå‡çš„å®ç°
        """
        k = len(self.H)
        
        # æ›´æ–°å¹³å‡å¥–åŠ±ï¼ˆå¢é‡æ–¹å¼ï¼‰
        self.n += 1
        if self.use_baseline:
            self.avg_reward += (reward - self.avg_reward) / self.n
            baseline = self.avg_reward
        else:
            baseline = 0
            
        # è®¡ç®—å½“å‰ç­–ç•¥
        exp_H = np.exp(self.H - np.max(self.H))
        pi = exp_H / np.sum(exp_H)
        
        # æ¢¯åº¦æ›´æ–°
        for a in range(k):
            if a == action:
                self.H[a] += self.alpha * (reward - baseline) * (1 - pi[a])
            else:
                self.H[a] -= self.alpha * (reward - baseline) * pi[a]
    
    def get_name(self) -> str:
        return f"Gradient Bandit (Î±={self.alpha})"


# ================================================================================
# ç¬¬2.4èŠ‚ï¼šæ¯”è¾ƒä¸åŒç®—æ³• - Figure 2.6çš„å†ç°
# Section 2.4: Comparing Different Algorithms - Reproducing Figure 2.6
# ================================================================================

class BanditExperiment:
    """
    èµŒåšæœºå®éªŒæ¡†æ¶ - ç³»ç»Ÿåœ°æ¯”è¾ƒä¸åŒç®—æ³•
    
    è¿™ä¸ªç±»å†ç°äº†ä¹¦ä¸­çš„å…³é”®å®éªŒï¼Œç‰¹åˆ«æ˜¯Figure 2.6
    é€šè¿‡å¤§é‡å®éªŒï¼Œæˆ‘ä»¬èƒ½çœ‹åˆ°ï¼š
    1. ä¸åŒç®—æ³•çš„å­¦ä¹ æ›²çº¿
    2. æ¢ç´¢ä¸åˆ©ç”¨çš„æƒè¡¡
    3. å‚æ•°æ•æ„Ÿæ€§åˆ†æ
    """
    
    def __init__(self, k: int = 10, n_bandits: int = 2000, 
                 n_steps: int = 1000):
        """
        åˆå§‹åŒ–å®éªŒ
        
        å‚æ•°ï¼ˆä¸ä¹¦ä¸­Figure 2.2è®¾ç½®ä¸€è‡´ï¼‰ï¼š
        k: è‡‚æ•°é‡
        n_bandits: èµŒåšæœºé—®é¢˜æ•°é‡ï¼ˆç”¨äºå¹³å‡ï¼‰
        n_steps: æ¯ä¸ªé—®é¢˜çš„æ­¥æ•°
        """
        self.k = k
        self.n_bandits = n_bandits
        self.n_steps = n_steps
        
    def run_single_bandit(self, strategy: ExplorationStrategy, 
                         stationary: bool = True,
                         initial_value: float = 0.0,
                         optimistic: bool = False,
                         alpha: Optional[float] = None) -> Tuple[np.ndarray, np.ndarray]:
        """
        åœ¨å•ä¸ªèµŒåšæœºä¸Šè¿è¡Œå®éªŒ
        
        è¿”å›ï¼š
        - rewards: æ¯æ­¥çš„å¥–åŠ±
        - optimal_actions: æ¯æ­¥æ˜¯å¦é€‰æ‹©äº†æœ€ä¼˜åŠ¨ä½œ
        """
        # åˆ›å»ºèµŒåšæœº
        bandit = KArmedBandit(self.k, stationary=stationary)
        
        # åˆ›å»ºä»·å€¼ä¼°è®¡å™¨
        estimator = ActionValueEstimator(
            self.k, 
            initial_value=initial_value,
            optimistic=optimistic,
            alpha=alpha
        )
        
        # è®°å½•ç»“æœ
        rewards = np.zeros(self.n_steps)
        optimal_actions = np.zeros(self.n_steps)
        
        # å¦‚æœæ˜¯æ¢¯åº¦èµŒåšæœºï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†
        if isinstance(strategy, GradientBandit):
            strategy.H = np.zeros(self.k)
            strategy.avg_reward = 0.0
            strategy.n = 0
        
        # è¿è¡Œå®éªŒ
        for t in range(self.n_steps):
            # é€‰æ‹©åŠ¨ä½œ
            action = strategy.select_action(estimator.Q, estimator.N, t+1)
            
            # è·å¾—å¥–åŠ±
            reward = bandit.step(action)
            
            # æ›´æ–°ä¼°è®¡
            estimator.update(action, reward)
            
            # å¦‚æœæ˜¯æ¢¯åº¦èµŒåšæœºï¼Œæ›´æ–°åå¥½
            if isinstance(strategy, GradientBandit):
                strategy.update_preference(action, reward)
            
            # è®°å½•ç»“æœ
            rewards[t] = reward
            optimal_actions[t] = (action == bandit.optimal_action)
            
        return rewards, optimal_actions
    
    def run_experiment(self, strategies: List[ExplorationStrategy],
                      **kwargs) -> Dict[str, Tuple[np.ndarray, np.ndarray]]:
        """
        è¿è¡Œå®Œæ•´å®éªŒï¼Œæ¯”è¾ƒå¤šä¸ªç­–ç•¥
        
        è¿™å†ç°äº†ä¹¦ä¸­çš„æ ¸å¿ƒå®éªŒï¼
        """
        results = {}
        
        for strategy in strategies:
            print(f"\nè¿è¡Œ {strategy.get_name()}...")
            
            # å¯¹æ¯ä¸ªç­–ç•¥è¿è¡Œå¤šä¸ªèµŒåšæœºé—®é¢˜
            all_rewards = np.zeros((self.n_bandits, self.n_steps))
            all_optimal = np.zeros((self.n_bandits, self.n_steps))
            
            for i in range(self.n_bandits):
                if i % 100 == 0:
                    print(f"  è¿›åº¦: {i}/{self.n_bandits}")
                    
                rewards, optimal = self.run_single_bandit(strategy, **kwargs)
                all_rewards[i] = rewards
                all_optimal[i] = optimal
            
            # è®¡ç®—å¹³å‡æ€§èƒ½
            avg_rewards = np.mean(all_rewards, axis=0)
            avg_optimal = np.mean(all_optimal, axis=0)
            
            results[strategy.get_name()] = (avg_rewards, avg_optimal)
            
        return results
    
    def plot_results(self, results: Dict[str, Tuple[np.ndarray, np.ndarray]]):
        """
        ç»˜åˆ¶ç»“æœ - å†ç°Figure 2.2
        
        ä¸¤ä¸ªå­å›¾ï¼š
        1. å¹³å‡å¥–åŠ±
        2. æœ€ä¼˜åŠ¨ä½œç™¾åˆ†æ¯”
        """
        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8))
        
        for name, (rewards, optimal) in results.items():
            ax1.plot(rewards, label=name)
            ax2.plot(optimal * 100, label=name)
            
        # ç¬¬ä¸€ä¸ªå­å›¾ï¼šå¹³å‡å¥–åŠ±
        ax1.set_xlabel('æ­¥æ•° Steps')
        ax1.set_ylabel('å¹³å‡å¥–åŠ± Average Reward')
        ax1.set_title('å¤šè‡‚èµŒåšæœºå­¦ä¹ æ›²çº¿ï¼ˆå¹³å‡å¥–åŠ±ï¼‰')
        ax1.grid(True, alpha=0.3)
        ax1.legend()
        
        # ç¬¬äºŒä¸ªå­å›¾ï¼šæœ€ä¼˜åŠ¨ä½œç™¾åˆ†æ¯”
        ax2.set_xlabel('æ­¥æ•° Steps')
        ax2.set_ylabel('æœ€ä¼˜åŠ¨ä½œ % Optimal Action')
        ax2.set_title('å¤šè‡‚èµŒåšæœºå­¦ä¹ æ›²çº¿ï¼ˆæœ€ä¼˜åŠ¨ä½œé€‰æ‹©ç‡ï¼‰')
        ax2.grid(True, alpha=0.3)
        ax2.legend()
        
        plt.tight_layout()
        plt.show()


# ================================================================================
# ç¬¬2.5èŠ‚ï¼šéå¹³ç¨³é—®é¢˜
# Section 2.5: Nonstationary Problems
# ================================================================================

class NonstationaryBandit:
    """
    éå¹³ç¨³èµŒåšæœº - çœŸå®ä¸–ç•Œçš„æŒ‘æˆ˜ï¼
    
    ç°å®ä¾‹å­ï¼š
    - è‚¡ç¥¨å¸‚åœºï¼šå…¬å¸ä»·å€¼éšæ—¶é—´å˜åŒ–
    - æ¨èç³»ç»Ÿï¼šç”¨æˆ·åå¥½ä¼šæ”¹å˜
    - æ¸¸æˆAIï¼šå¯¹æ‰‹ç­–ç•¥ä¼šé€‚åº”
    
    ä¹¦ä¸­ç»ƒä¹ 2.5ï¼š
    åœ¨éå¹³ç¨³æƒ…å†µä¸‹ï¼Œä½¿ç”¨å›ºå®šå­¦ä¹ ç‡Î±æ¯”æ ·æœ¬å¹³å‡(1/n)æ›´å¥½ï¼
    
    ä¸ºä»€ä¹ˆï¼Ÿ
    - æ ·æœ¬å¹³å‡ï¼šç»™æ‰€æœ‰å†å²åŒç­‰æƒé‡
    - å›ºå®šÎ±ï¼šæŒ‡æ•°è¡°å‡çš„æƒé‡ï¼Œæ›´é‡è§†æœ€è¿‘çš„ç»éªŒ
    
    æƒé‡åˆ†æï¼š
    Q_{n+1} = (1-Î±)^n Q_1 + Î£_{i=1}^n Î±(1-Î±)^{n-i} R_i
    
    æœ€è¿‘çš„å¥–åŠ±æƒé‡æœ€å¤§ï¼
    """
    
    def __init__(self, k: int = 10, drift_std: float = 0.01):
        """
        åˆå§‹åŒ–éå¹³ç¨³èµŒåšæœº
        
        drift_std: æ¯æ­¥æ¼‚ç§»çš„æ ‡å‡†å·®
        - å°ï¼šç¼“æ…¢å˜åŒ–ï¼ˆå¦‚ç”¨æˆ·åå¥½ï¼‰
        - å¤§ï¼šå¿«é€Ÿå˜åŒ–ï¼ˆå¦‚è‚¡å¸‚ï¼‰
        """
        self.k = k
        self.drift_std = drift_std
        
        # åˆå§‹çœŸå®ä»·å€¼
        self.q_star = np.random.normal(0, 1, k)
        self.step_count = 0
        
    def step(self, action: int) -> float:
        """
        æ‰§è¡ŒåŠ¨ä½œï¼ŒçœŸå®ä»·å€¼ä¼šæ¼‚ç§»ï¼
        
        éšæœºæ¸¸èµ°æ¨¡å‹ï¼š
        q*(a) â† q*(a) + N(0, ÏƒÂ²)
        """
        # çœŸå®ä»·å€¼æ¼‚ç§»
        self.q_star += np.random.normal(0, self.drift_std, self.k)
        self.step_count += 1
        
        # ç”Ÿæˆå¥–åŠ±
        reward = np.random.normal(self.q_star[action], 1.0)
        
        return reward
    
    def get_optimal_action(self) -> int:
        """å½“å‰æœ€ä¼˜åŠ¨ä½œï¼ˆä¼šå˜åŒ–ï¼ï¼‰"""
        return np.argmax(self.q_star)


# ================================================================================
# ç¬¬2.6èŠ‚ï¼šå‚æ•°ç ”ç©¶ - å¦‚ä½•é€‰æ‹©æœ€ä½³å‚æ•°
# Section 2.6: Parameter Study - How to Choose Best Parameters
# ================================================================================

def parameter_study():
    """
    å‚æ•°ç ”ç©¶ - å†ç°Figure 2.6
    
    æµ‹è¯•ä¸åŒå‚æ•°å€¼ï¼Œæ‰¾å‡ºæœ€ä½³è®¾ç½®
    
    å…³é”®å‘ç°ï¼ˆä¹¦ä¸­ç¬¬42é¡µï¼‰ï¼š
    1. æ²¡æœ‰å•ä¸€æœ€ä½³ç®—æ³•
    2. å‚æ•°é€‰æ‹©å¾ˆé‡è¦
    3. é—®é¢˜ç‰¹æ€§å†³å®šæœ€ä½³æ–¹æ³•
    """
    print("="*70)
    print("å‚æ•°ç ”ç©¶ï¼šå¯»æ‰¾æœ€ä½³è®¾ç½®")
    print("Parameter Study: Finding Best Settings")
    print("="*70)
    
    # æµ‹è¯•çš„å‚æ•°å€¼
    epsilons = [0, 0.01, 0.1, 0.5, 1.0]
    alphas = [0.1, 0.2, 0.4]
    c_values = [0.5, 1, 2, 4]
    
    results = {}
    
    # æµ‹è¯•Îµ-è´ªå©ª
    print("\næµ‹è¯•Îµ-è´ªå©ªç­–ç•¥...")
    for eps in epsilons:
        strategy = EpsilonGreedy(epsilon=eps)
        # è¿™é‡Œåº”è¯¥è¿è¡Œå®éªŒå¹¶è®°å½•ç»“æœ
        # results[f"Îµ={eps}"] = run_experiment(strategy)
        print(f"  Îµ={eps}: æ¢ç´¢{eps*100:.0f}%ï¼Œåˆ©ç”¨{(1-eps)*100:.0f}%")
    
    # æµ‹è¯•UCB
    print("\næµ‹è¯•UCBç­–ç•¥...")
    for c in c_values:
        strategy = UCB(c=c)
        print(f"  c={c}: æ¢ç´¢å¼ºåº¦{c}")
    
    # æµ‹è¯•æ¢¯åº¦èµŒåšæœº
    print("\næµ‹è¯•æ¢¯åº¦èµŒåšæœº...")
    for alpha in alphas:
        strategy = GradientBandit(alpha=alpha)
        print(f"  Î±={alpha}: å­¦ä¹ ç‡{alpha}")
    
    print("\nå…³é”®æ´å¯Ÿï¼š")
    print("1. Îµ=0.1 é€šå¸¸æ˜¯å¥½çš„èµ·ç‚¹")
    print("2. UCBçš„c=2æä¾›ç†è®ºä¿è¯")
    print("3. éå¹³ç¨³é—®é¢˜éœ€è¦æŒç»­æ¢ç´¢")


# ================================================================================
# ç¬¬2.7èŠ‚ï¼šå®Œæ•´ç¤ºä¾‹ - é¤å…é€‰æ‹©é—®é¢˜
# Section 2.7: Complete Example - Restaurant Selection Problem
# ================================================================================

def restaurant_selection_demo():
    """
    å®Œæ•´ç¤ºä¾‹ï¼šç”¨å¤šè‡‚èµŒåšæœºè§£å†³é¤å…é€‰æ‹©é—®é¢˜
    
    åœºæ™¯ï¼š
    ä½ åˆšæ¬åˆ°æ–°åŸå¸‚ï¼Œæœ‰10å®¶é¤å…å¯é€‰ã€‚
    æ¯æ¬¡åªèƒ½å»ä¸€å®¶ï¼Œå¦‚ä½•æœ€å¿«æ‰¾åˆ°æœ€å¥½çš„ï¼Ÿ
    
    è¿™ä¸ªä¾‹å­å±•ç¤ºäº†å¼ºåŒ–å­¦ä¹ å¦‚ä½•è§£å†³æ—¥å¸¸å†³ç­–é—®é¢˜ï¼
    """
    print("="*70)
    print("é¤å…é€‰æ‹©é—®é¢˜ - å¤šè‡‚èµŒåšæœºçš„å®é™…åº”ç”¨")
    print("Restaurant Selection - Real-world Application of MAB")
    print("="*70)
    
    # é¤å…åç§°å’ŒçœŸå®è¯„åˆ†ï¼ˆä½ ä¸çŸ¥é“ï¼‰
    restaurants = [
        ("è€ç‹ç‰›è‚‰é¢", 7.2),
        ("å°æå·èœé¦†", 8.5),
        ("å¼ å§ç²¤èœ", 6.8),
        ("ä¸œåŒ—é¥ºå­", 7.8),
        ("æ—¥æœ¬æ–™ç†", 9.2),  # æœ€å¥½çš„ï¼
        ("éŸ©å›½çƒ¤è‚‰", 8.0),
        ("è¥¿é¤å…", 6.5),
        ("æ³°å›½èœ", 7.5),
        ("å°åº¦å’–å–±", 5.8),
        ("ç´ é£Ÿé¤å…", 7.0)
    ]
    
    print("\nåœºæ™¯è®¾ç½®ï¼š")
    print(f"åŸå¸‚é‡Œæœ‰{len(restaurants)}å®¶é¤å…")
    print("ä½ çš„ç›®æ ‡ï¼šåœ¨100å¤©å†…æ‰¾åˆ°æœ€å¥½çš„é¤å…")
    print("æŒ‘æˆ˜ï¼šæ¯å¤©åªèƒ½å»ä¸€å®¶ï¼Œå¦‚ä½•å¹³è¡¡æ¢ç´¢æ–°é¤å…vså»å·²çŸ¥å¥½é¤å…ï¼Ÿ")
    
    # åˆ›å»ºèµŒåšæœºï¼ˆé¤å…è¯„åˆ†ï¼‰
    k = len(restaurants)
    true_ratings = np.array([r[1] for r in restaurants])
    
    # å½’ä¸€åŒ–åˆ°æ ‡å‡†æ­£æ€åˆ†å¸ƒ
    normalized_ratings = (true_ratings - np.mean(true_ratings)) / np.std(true_ratings)
    
    class RestaurantBandit:
        def __init__(self):
            self.q_star = normalized_ratings
            self.optimal = np.argmax(self.q_star)
            
        def visit(self, restaurant_idx):
            # æ¯æ¬¡ä½“éªŒæœ‰éšæœºæ€§ï¼ˆæœåŠ¡ã€å¿ƒæƒ…ç­‰ï¼‰
            base_score = self.q_star[restaurant_idx]
            actual_experience = np.random.normal(base_score, 0.5)
            return actual_experience
    
    # æµ‹è¯•ä¸åŒç­–ç•¥
    print("\nç­–ç•¥1ï¼šçº¯éšæœºï¼ˆå‚»ç“œç­–ç•¥ï¼‰")
    print("æ¯å¤©éšæœºé€‰é¤å…ï¼Œä¸å­¦ä¹ ")
    
    print("\nç­–ç•¥2ï¼šçº¯åˆ©ç”¨ï¼ˆä¿å®ˆç­–ç•¥ï¼‰")
    print("å»è¿‡ä¸€å®¶ä¸é”™çš„å°±ä¸€ç›´å»")
    
    print("\nç­–ç•¥3ï¼šÎµ-è´ªå©ªï¼ˆå¹³è¡¡ç­–ç•¥ï¼‰")
    print("90%å»å·²çŸ¥æœ€å¥½çš„ï¼Œ10%å°è¯•æ–°çš„")
    
    print("\nç­–ç•¥4ï¼šUCBï¼ˆæ™ºèƒ½æ¢ç´¢ï¼‰")
    print("ä¼˜å…ˆå°è¯•å»å¾—å°‘çš„é¤å…")
    
    # æ¨¡æ‹Ÿ100å¤©
    n_days = 100
    bandit = RestaurantBandit()
    
    # ä½¿ç”¨Îµ-è´ªå©ªç­–ç•¥
    estimator = ActionValueEstimator(k, initial_value=0)
    strategy = EpsilonGreedy(epsilon=0.1)
    
    print(f"\nå¼€å§‹100å¤©çš„é¤å…æ¢ç´¢...")
    print("-"*40)
    
    total_satisfaction = 0
    visit_counts = np.zeros(k)
    
    for day in range(1, n_days + 1):
        # é€‰æ‹©é¤å…
        choice = strategy.select_action(estimator.Q, estimator.N, day)
        
        # å»é¤å…å°±é¤
        satisfaction = bandit.visit(choice)
        total_satisfaction += satisfaction
        visit_counts[choice] += 1
        
        # æ›´æ–°è¯„ä¼°
        estimator.update(choice, satisfaction)
        
        # å®šæœŸæŠ¥å‘Š
        if day in [10, 30, 50, 100]:
            best_known = np.argmax(estimator.Q)
            print(f"\nç¬¬{day}å¤©æ€»ç»“ï¼š")
            print(f"  ç›®å‰è®¤ä¸ºæœ€å¥½çš„ï¼š{restaurants[best_known][0]}")
            print(f"  å®é™…æœ€å¥½çš„ï¼š{restaurants[bandit.optimal][0]}")
            print(f"  å¹³å‡æ»¡æ„åº¦ï¼š{total_satisfaction/day:.2f}")
            
            if day == 100:
                print(f"\nè®¿é—®æ¬¡æ•°ç»Ÿè®¡ï¼š")
                for i, (name, true_rating) in enumerate(restaurants):
                    visits = int(visit_counts[i])
                    estimated = estimator.Q[i]
                    print(f"  {name:10} - è®¿é—®{visits:3}æ¬¡, "
                          f"ä¼°è®¡è¯„åˆ†:{estimated:+.2f}, "
                          f"çœŸå®:{normalized_ratings[i]:+.2f}")
    
    print("\n" + "="*70)
    print("å®éªŒç»“è®º")
    print("="*70)
    print("""
    1. çº¯éšæœºï¼šç®€å•ä½†ä½æ•ˆï¼Œå¹³å‡æ»¡æ„åº¦æœ€ä½
    2. çº¯åˆ©ç”¨ï¼šå¯èƒ½å›°åœ¨å±€éƒ¨æœ€ä¼˜
    3. Îµ-è´ªå©ªï¼šç®€å•æœ‰æ•ˆï¼Œé€‚åˆå¤§å¤šæ•°åœºæ™¯
    4. UCBï¼šæ™ºèƒ½ä½†å¤æ‚ï¼Œç†è®ºæ€§èƒ½æœ€ä¼˜
    
    å…³é”®æ´å¯Ÿï¼š
    - å¼€å§‹æ—¶å¤šæ¢ç´¢ï¼ˆä¸ç¡®å®šæ€§å¤§ï¼‰
    - åæœŸå¤šåˆ©ç”¨ï¼ˆçŸ¥è¯†ç§¯ç´¯åï¼‰
    - æ²¡æœ‰å®Œç¾ç­–ç•¥ï¼Œéœ€è¦æ ¹æ®å…·ä½“é—®é¢˜è°ƒæ•´
    """)


# ================================================================================
# ç¬¬2.8èŠ‚ï¼šæ€»ç»“ä¸ç»ƒä¹ 
# Section 2.8: Summary and Exercises
# ================================================================================

def chapter_summary():
    """
    ç¬¬2ç« æ€»ç»“ - å¤šè‡‚èµŒåšæœºçš„æ ¸å¿ƒçŸ¥è¯†
    
    é€šè¿‡æœ¬ç« ï¼Œæˆ‘ä»¬å­¦åˆ°äº†ä»€ä¹ˆï¼Ÿ
    """
    print("="*70)
    print("ç¬¬2ç« æ€»ç»“ï¼šå¤šè‡‚èµŒåšæœº")
    print("Chapter 2 Summary: Multi-Armed Bandits")
    print("="*70)
    
    print("""
    æ ¸å¿ƒæ¦‚å¿µå›é¡¾ï¼š
    
    1. æ¢ç´¢ä¸åˆ©ç”¨çš„æƒè¡¡ (Exploration vs Exploitation)
       - è¿™æ˜¯å¼ºåŒ–å­¦ä¹ çš„æ°¸æ’ä¸»é¢˜
       - æ²¡æœ‰å…è´¹çš„åˆé¤ï¼šå¿…é¡»åšå‡ºå–èˆ
    
    2. åŠ¨ä½œä»·å€¼æ–¹æ³• (Action-Value Methods)
       - å¢é‡æ›´æ–°ï¼šQ_{n+1} = Q_n + Î±[R_n - Q_n]
       - è¿™ä¸ªå…¬å¼è´¯ç©¿æ•´ä¸ªå¼ºåŒ–å­¦ä¹ ï¼
    
    3. æ¢ç´¢ç­–ç•¥å¯¹æ¯”ï¼š
       
       ç­–ç•¥        | ä¼˜ç‚¹           | ç¼ºç‚¹           | é€‚ç”¨åœºæ™¯
       ------------|----------------|----------------|----------
       Îµ-è´ªå©ª      | ç®€å•æœ‰æ•ˆ       | æŒç»­éšæœºæ¢ç´¢   | é€šç”¨
       UCB         | æ™ºèƒ½æ¢ç´¢       | è®¡ç®—å¤æ‚       | ç†è®ºç ”ç©¶
       æ¢¯åº¦èµŒåšæœº  | è‡ªç„¶æ¦‚ç‡åˆ†å¸ƒ   | éœ€è¦è°ƒå‚       | å¤§åŠ¨ä½œç©ºé—´
       ä¹è§‚åˆå§‹å€¼  | è‡ªç„¶æ¢ç´¢       | åªåœ¨å¼€å§‹æœ‰æ•ˆ   | å¹³ç¨³é—®é¢˜
    
    4. éå¹³ç¨³é—®é¢˜çš„å¤„ç†
       - ä½¿ç”¨å›ºå®šå­¦ä¹ ç‡Î±è€Œé1/n
       - æŒç»­æ¢ç´¢çš„å¿…è¦æ€§
       - é—å¿˜æ—§çŸ¥è¯†çš„æƒè¡¡
    
    5. å…³é”®å…¬å¼æ€»ç»“ï¼š
       
       æ ·æœ¬å¹³å‡æ›´æ–°ï¼š
       Q_{n+1} = Q_n + (1/n)[R_n - Q_n]
       
       å›ºå®šæ­¥é•¿æ›´æ–°ï¼š
       Q_{n+1} = Q_n + Î±[R_n - Q_n]
       
       UCBåŠ¨ä½œé€‰æ‹©ï¼š
       A_t = argmax[Q_t(a) + câˆš(ln t / N_t(a))]
       
       æ¢¯åº¦æ›´æ–°ï¼š
       H_{t+1}(a) = H_t(a) + Î±(R_t - RÌ„_t)(ğŸ™_{a=A_t} - Ï€_t(a))
    
    ç»ƒä¹ å»ºè®®ï¼š
    
    1. å®ç°ç»ƒä¹ 2.5ï¼šæ¯”è¾ƒå›ºå®šÎ±å’Œ1/nåœ¨éå¹³ç¨³é—®é¢˜ä¸Šçš„è¡¨ç°
    2. å®ç°ç»ƒä¹ 2.6ï¼šæµ‹è¯•ä¹è§‚åˆå§‹å€¼çš„æ•ˆæœ
    3. å®ç°ç»ƒä¹ 2.9ï¼šå®ç°UCBçš„å˜ä½“
    4. å®ç°ç»ƒä¹ 2.11ï¼šå®ç°å‚æ•°ç ”ç©¶ï¼Œæ‰¾å‡ºæœ€ä½³è®¾ç½®
    
    ä¸‹ä¸€ç« é¢„å‘Šï¼š
    ç¬¬3ç«  - æœ‰é™é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹
    ä»å•çŠ¶æ€ï¼ˆèµŒåšæœºï¼‰åˆ°å¤šçŠ¶æ€ï¼ˆå®Œæ•´çš„å¼ºåŒ–å­¦ä¹ é—®é¢˜ï¼‰ï¼
    """)


# ================================================================================
# ä¸»ç¨‹åºï¼šè¿è¡Œç¬¬2ç« çš„å®Œæ•´æ¼”ç¤º
# Main: Run Complete Chapter 2 Demonstration
# ================================================================================

def demonstrate_chapter_2():
    """è¿è¡Œç¬¬2ç« çš„å®Œæ•´æ¼”ç¤º"""
    print("â•”" + "â•"*68 + "â•—")
    print("â•‘" + " "*15 + "ç¬¬2ç« ï¼šå¤šè‡‚èµŒåšæœºé—®é¢˜".center(38) + " "*15 + "â•‘")
    print("â•‘" + " "*10 + "Chapter 2: Multi-Armed Bandits".center(48) + " "*10 + "â•‘")
    print("â•š" + "â•"*68 + "â•")
    
    print("\næœ¬ç« å°†é€šè¿‡ä»£ç å’Œæ•…äº‹ï¼Œè®©ä½ å®Œå…¨ç†è§£æ¢ç´¢ä¸åˆ©ç”¨çš„æƒè¡¡ã€‚")
    print("This chapter will help you fully understand exploration vs exploitation.\n")
    
    # 1. åŸºæœ¬æ¦‚å¿µæ¼”ç¤º
    print("\nã€ç¬¬1éƒ¨åˆ†ï¼šå¤šè‡‚èµŒåšæœºé—®é¢˜ã€‘")
    print("[Part 1: Multi-Armed Bandit Problem]")
    print("="*70)
    
    bandit = KArmedBandit(k=10)
    bandit.visualize_true_values()
    
    print("\nè¿™å°±æ˜¯èµŒåšæœºé—®é¢˜çš„æœ¬è´¨ï¼š")
    print("- ä½ ä¸çŸ¥é“å“ªä¸ªè‡‚æœ€å¥½ï¼ˆéœ€è¦æ¢ç´¢ï¼‰")
    print("- ä½ æƒ³è·å¾—æœ€å¤šå¥–åŠ±ï¼ˆéœ€è¦åˆ©ç”¨ï¼‰")
    print("- å¦‚ä½•å¹³è¡¡ï¼Ÿè¿™å°±æ˜¯æœ¬ç« çš„æ ¸å¿ƒï¼")
    
    # 2. åŠ¨ä½œä»·å€¼ä¼°è®¡
    print("\nã€ç¬¬2éƒ¨åˆ†ï¼šå­¦ä¹ åŠ¨ä½œä»·å€¼ã€‘")
    print("[Part 2: Learning Action Values]")
    print("="*70)
    
    estimator = ActionValueEstimator(k=10)
    print("\né€šè¿‡ä¸æ–­å°è¯•å’Œæ›´æ–°ï¼Œæˆ‘ä»¬é€æ¸å­¦ä¹ æ¯ä¸ªåŠ¨ä½œçš„ä»·å€¼...")
    
    # æ¨¡æ‹Ÿå­¦ä¹ è¿‡ç¨‹
    for _ in range(100):
        action = np.random.randint(10)
        reward = bandit.step(action)
        estimator.update(action, reward)
    
    print(f"100æ¬¡å°è¯•åï¼Œæœ€ä½³åŠ¨ä½œä¼°è®¡ï¼š{estimator.get_best_action()}")
    print(f"çœŸå®æœ€ä½³åŠ¨ä½œï¼š{bandit.optimal_action}")
    
    # 3. ç­–ç•¥æ¯”è¾ƒ
    print("\nã€ç¬¬3éƒ¨åˆ†ï¼šæ¯”è¾ƒä¸åŒç­–ç•¥ã€‘")
    print("[Part 3: Comparing Different Strategies]")
    print("="*70)
    
    # åˆ›å»ºå®éªŒ
    experiment = BanditExperiment(k=10, n_bandits=100, n_steps=1000)
    
    # å®šä¹‰è¦æ¯”è¾ƒçš„ç­–ç•¥
    strategies = [
        EpsilonGreedy(epsilon=0.0),   # çº¯åˆ©ç”¨
        EpsilonGreedy(epsilon=0.1),   # å¹³è¡¡
        EpsilonGreedy(epsilon=1.0),   # çº¯æ¢ç´¢
        UCB(c=2.0)                     # UCB
    ]
    
    print("\nè¿è¡Œå®éªŒæ¯”è¾ƒä¸åŒç­–ç•¥...")
    print("ï¼ˆä¸ºäº†æ¼”ç¤ºé€Ÿåº¦ï¼Œåªè¿è¡Œ100ä¸ªèµŒåšæœºé—®é¢˜ï¼‰")
    
    results = experiment.run_experiment(strategies)
    experiment.plot_results(results)
    
    # 4. é¤å…é€‰æ‹©å®ä¾‹
    print("\nã€ç¬¬4éƒ¨åˆ†ï¼šå®é™…åº”ç”¨ - é¤å…é€‰æ‹©ã€‘")
    print("[Part 4: Real Application - Restaurant Selection]")
    print("="*70)
    
    restaurant_selection_demo()
    
    # 5. ç« èŠ‚æ€»ç»“
    print("\nã€ç¬¬5éƒ¨åˆ†ï¼šç« èŠ‚æ€»ç»“ã€‘")
    print("[Part 5: Chapter Summary]")
    
    chapter_summary()


if __name__ == "__main__":
    demonstrate_chapter_2()