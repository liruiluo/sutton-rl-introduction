"""
================================================================================
ç¬¬1.4èŠ‚ï¼šäº•å­—æ£‹ - é€šè¿‡å®Œæ•´ä¾‹å­ç†è§£å¼ºåŒ–å­¦ä¹ 
Section 1.4: Tic-Tac-Toe - Understanding RL Through a Complete Example

æ ¹æ® Sutton & Bartoã€Šå¼ºåŒ–å­¦ä¹ ï¼šå¯¼è®ºã€‹ç¬¬äºŒç‰ˆ ç¬¬1ç« 
Based on Sutton & Barto "Reinforcement Learning: An Introduction" Chapter 1
================================================================================

è®©æˆ‘ä»¬ä»ä¸€ä¸ªå°æ•…äº‹å¼€å§‹ï¼š

å°æ˜ä»Šå¹´6å²ï¼Œåˆšå­¦ä¼šäº•å­—æ£‹çš„è§„åˆ™ã€‚
ç¬¬ä¸€æ¬¡ç©ï¼Œä»–éšæœºä¸‹æ£‹ï¼Œå¾ˆå¿«å°±è¾“äº†ã€‚
ç¬¬äºŒæ¬¡ç©ï¼Œä»–è®°ä½äº†ä¸Šæ¬¡è¾“çš„ä½ç½®ï¼Œé¿å¼€äº†é‚£äº›èµ°æ³•ã€‚
ç¬¬ä¸‰æ¬¡ç©ï¼Œä»–å‘ç°èµ°ä¸­å¿ƒæ ¼å­ä¼¼ä¹æ›´å®¹æ˜“èµ¢ã€‚
...
ç¬¬100æ¬¡ç©ï¼Œä»–å·²ç»å¾ˆéš¾è¢«æ‰“è´¥äº†ã€‚

è¿™ä¸ªå­¦ä¹ è¿‡ç¨‹ï¼Œå°±æ˜¯å¼ºåŒ–å­¦ä¹ çš„ç²¾é«“ï¼

æ²¡æœ‰äººå‘Šè¯‰å°æ˜"æœ€ä¼˜ç­–ç•¥æ˜¯ä»€ä¹ˆ"ï¼ˆä¸æ˜¯ç›‘ç£å­¦ä¹ ï¼‰
ä»–åªæ˜¯é€šè¿‡ä¸æ–­å¯¹å¼ˆï¼Œä»è¾“èµ¢ä¸­å­¦ä¹ ï¼ˆå¼ºåŒ–å­¦ä¹ ï¼‰

================================================================================
äº•å­—æ£‹ä¸ºä»€ä¹ˆæ˜¯å®Œç¾çš„RLå…¥é—¨ä¾‹å­ï¼Ÿ
Why Tic-Tac-Toe is Perfect for Learning RL?
================================================================================

Sutton & Bartoé€‰æ‹©äº•å­—æ£‹ä½œä¸ºç¬¬ä¸€ä¸ªè¯¦ç»†ä¾‹å­ï¼Œå› ä¸ºï¼š

1. è§„åˆ™ç®€å• Simple Rules
   - 3Ã—3æ ¼å­ï¼Œä¸¤äººè½®æµ
   - ä¸‰ä¸ªè¿æˆçº¿å°±èµ¢
   - äººäººéƒ½ä¼šç©

2. çŠ¶æ€ç©ºé—´å° Small State Space  
   - æ€»å…±åªæœ‰3^9 = 19683ç§å¯èƒ½çŠ¶æ€
   - è€ƒè™‘å¯¹ç§°æ€§åæ›´å°‘
   - å¯ä»¥å®Œå…¨å­˜å‚¨ä»·å€¼è¡¨

3. å»¶è¿Ÿå¥–åŠ±æ˜æ˜¾ Clear Delayed Reward
   - åªæœ‰æ¸¸æˆç»“æŸæ‰çŸ¥é“è¾“èµ¢
   - éœ€è¦å­¦ä¼šè¯„ä¼°ä¸­é—´çŠ¶æ€
   - å®Œç¾å±•ç¤ºæ—¶é—´ä¿¡ç”¨åˆ†é…é—®é¢˜

4. å¯ä»¥è‡ªæˆ‘å¯¹å¼ˆ Self-Play Possible
   - ä¸éœ€è¦äººç±»å¯¹æ‰‹
   - å¯ä»¥å¿«é€Ÿè®­ç»ƒ
   - å±•ç¤ºRLçš„è‡ªä¸»å­¦ä¹ èƒ½åŠ›
"""

import numpy as np
from typing import Optional, Tuple, Dict, List
import matplotlib.pyplot as plt
from dataclasses import dataclass
import pickle
from collections import defaultdict


# ================================================================================
# ç¬¬1.4.1èŠ‚ï¼šäº•å­—æ£‹æ¸¸æˆç¯å¢ƒ
# Section 1.4.1: Tic-Tac-Toe Game Environment
# ================================================================================

class TicTacToeBoard:
    """
    äº•å­—æ£‹æ£‹ç›˜ - æ¸¸æˆç¯å¢ƒ
    
    è¿™æ˜¯å¼ºåŒ–å­¦ä¹ ä¸­çš„"ç¯å¢ƒ"(Environment)éƒ¨åˆ†ã€‚
    å®ƒå®šä¹‰äº†ï¼š
    1. çŠ¶æ€ç©ºé—´ï¼ˆ3Ã—3çš„æ£‹ç›˜ï¼‰
    2. åŠ¨ä½œç©ºé—´ï¼ˆå¯ä»¥ä¸‹æ£‹çš„ä½ç½®ï¼‰
    3. æ¸¸æˆè§„åˆ™ï¼ˆå¦‚ä½•åˆ¤æ–­èƒœè´Ÿï¼‰
    4. çŠ¶æ€è½¬ç§»ï¼ˆä¸‹æ£‹åæ£‹ç›˜å¦‚ä½•å˜åŒ–ï¼‰
    
    å…³é”®è®¾è®¡ï¼š
    - ç”¨æ•°å­—è¡¨ç¤ºï¼š1=Xç©å®¶, -1=Oç©å®¶, 0=ç©º
    - çŠ¶æ€ç”¨å…ƒç»„è¡¨ç¤ºï¼Œæ–¹ä¾¿ä½œä¸ºå­—å…¸çš„é”®
    - å®Œæ•´å®ç°æ‰€æœ‰æ¸¸æˆé€»è¾‘
    """
    
    def __init__(self):
        """åˆå§‹åŒ–ç©ºæ£‹ç›˜"""
        self.board = np.zeros((3, 3), dtype=int)
        self.current_player = 1  # Xå…ˆæ‰‹
        
    def reset(self):
        """é‡ç½®æ£‹ç›˜åˆ°åˆå§‹çŠ¶æ€"""
        self.board = np.zeros((3, 3), dtype=int)
        self.current_player = 1
        return self.get_state()
    
    def get_state(self) -> tuple:
        """
        è·å–å½“å‰çŠ¶æ€çš„å”¯ä¸€è¡¨ç¤º
        
        ä¸ºä»€ä¹ˆç”¨å…ƒç»„ï¼Ÿ
        1. å¯ä»¥ä½œä¸ºå­—å…¸çš„é”®ï¼ˆä¸å¯å˜ï¼‰
        2. æ–¹ä¾¿å­˜å‚¨ä»·å€¼å‡½æ•° V(s)
        3. å”¯ä¸€æ ‡è¯†ä¸€ä¸ªæ£‹ç›˜çŠ¶æ€
        """
        return tuple(self.board.flatten())
    
    def get_available_actions(self) -> List[int]:
        """
        è·å–æ‰€æœ‰åˆæ³•åŠ¨ä½œ
        
        è¿”å›æ‰€æœ‰ç©ºæ ¼å­çš„ä½ç½®ï¼ˆ0-8çš„æ•°å­—ï¼‰
        è¿™å®šä¹‰äº†å½“å‰çŠ¶æ€ä¸‹çš„åŠ¨ä½œç©ºé—´ A(s)
        """
        return [i for i in range(9) if self.board.flat[i] == 0]
    
    def make_move(self, action: int) -> Tuple[int, bool]:
        """
        æ‰§è¡Œä¸€ä¸ªåŠ¨ä½œï¼ˆä¸‹æ£‹ï¼‰
        
        å‚æ•°:
            action: ä½ç½®ç´¢å¼• (0-8)
            
        è¿”å›:
            reward: å¥–åŠ±ä¿¡å· (+1èµ¢, -1è¾“, 0å…¶ä»–)
            done: æ¸¸æˆæ˜¯å¦ç»“æŸ
            
        è¿™æ˜¯ç¯å¢ƒçš„æ ¸å¿ƒï¼šæ‰§è¡ŒåŠ¨ä½œå¹¶è¿”å›å¥–åŠ±ï¼
        """
        if self.board.flat[action] != 0:
            raise ValueError(f"ä½ç½®{action}å·²ç»æœ‰æ£‹å­äº†ï¼")
        
        # ä¸‹æ£‹
        row, col = action // 3, action % 3
        self.board[row, col] = self.current_player
        
        # æ£€æŸ¥æ¸¸æˆç»“æœ
        winner = self._check_winner()
        if winner != 0:
            # æœ‰äººèµ¢äº†
            reward = winner  # èµ¢å®¶å¾—+1ï¼Œè¾“å®¶å¾—-1
            return reward, True
        elif len(self.get_available_actions()) == 0:
            # å¹³å±€
            return 0, True
        else:
            # æ¸¸æˆç»§ç»­ï¼Œåˆ‡æ¢ç©å®¶
            self.current_player = -self.current_player
            return 0, False
    
    def _check_winner(self) -> int:
        """
        æ£€æŸ¥æ˜¯å¦æœ‰èµ¢å®¶
        
        è¿”å›: 1(Xèµ¢), -1(Oèµ¢), 0(æœªç»“æŸæˆ–å¹³å±€)
        
        æ£€æŸ¥æ‰€æœ‰å¯èƒ½çš„è·èƒœçº¿è·¯ï¼š
        - 3æ¡æ¨ªçº¿
        - 3æ¡ç«–çº¿
        - 2æ¡å¯¹è§’çº¿
        """
        # æ£€æŸ¥è¡Œ
        for row in self.board:
            if abs(row.sum()) == 3:
                return row[0]
        
        # æ£€æŸ¥åˆ—
        for col in self.board.T:
            if abs(col.sum()) == 3:
                return col[0]
        
        # æ£€æŸ¥å¯¹è§’çº¿
        diag1 = self.board.diagonal()
        if abs(diag1.sum()) == 3:
            return diag1[0]
        
        diag2 = np.fliplr(self.board).diagonal()
        if abs(diag2.sum()) == 3:
            return diag2[0]
        
        return 0
    
    def render(self):
        """
        å¯è§†åŒ–æ£‹ç›˜
        
        ç”¨ç¬¦å·æ˜¾ç¤ºï¼šX, O, .ï¼ˆç©ºï¼‰
        æ–¹ä¾¿äººç±»ç†è§£å½“å‰å±€é¢
        """
        symbols = {1: 'X', -1: 'O', 0: '.'}
        print("\nå½“å‰æ£‹ç›˜ Current Board:")
        print("  0 1 2")
        for i, row in enumerate(self.board):
            print(f"{i} " + " ".join(symbols[x] for x in row))
        print()
    
    def get_symmetries(self, state: tuple) -> List[tuple]:
        """
        è·å–çŠ¶æ€çš„æ‰€æœ‰å¯¹ç§°å½¢å¼
        
        äº•å­—æ£‹æœ‰8ç§å¯¹ç§°ï¼š
        - æ—‹è½¬90Â°ã€180Â°ã€270Â°ï¼ˆ3ç§ï¼‰
        - æ°´å¹³ç¿»è½¬ï¼ˆ1ç§ï¼‰
        - å‚ç›´ç¿»è½¬ï¼ˆ1ç§ï¼‰
        - ä¸¤æ¡å¯¹è§’çº¿ç¿»è½¬ï¼ˆ2ç§ï¼‰
        - åŸå§‹çŠ¶æ€ï¼ˆ1ç§ï¼‰
        
        åˆ©ç”¨å¯¹ç§°æ€§å¯ä»¥å¤§å¤§å‡å°‘éœ€è¦å­¦ä¹ çš„çŠ¶æ€æ•°ï¼
        """
        board = np.array(state).reshape(3, 3)
        symmetries = []
        
        # 4ç§æ—‹è½¬
        for k in range(4):
            rotated = np.rot90(board, k)
            symmetries.append(tuple(rotated.flatten()))
        
        # ç¿»è½¬åå†æ—‹è½¬
        flipped = np.fliplr(board)
        for k in range(4):
            rotated = np.rot90(flipped, k)
            symmetries.append(tuple(rotated.flatten()))
        
        return list(set(symmetries))  # å»é‡


# ================================================================================
# ç¬¬1.4.2èŠ‚ï¼šä»·å€¼å‡½æ•°ä¸æ—¶åºå·®åˆ†å­¦ä¹ 
# Section 1.4.2: Value Function and Temporal Difference Learning
# ================================================================================

class ValueFunction:
    """
    çŠ¶æ€ä»·å€¼å‡½æ•° V(s)
    
    è¿™æ˜¯å¼ºåŒ–å­¦ä¹ çš„æ ¸å¿ƒæ¦‚å¿µï¼
    
    V(s) è¡¨ç¤ºï¼šä»çŠ¶æ€så¼€å§‹ï¼Œé‡‡ç”¨å½“å‰ç­–ç•¥ï¼Œæœ€ç»ˆè·èƒœçš„æ¦‚ç‡
    
    ä¾‹å¦‚ï¼š
    - V(åˆå§‹çŠ¶æ€) â‰ˆ 0.5ï¼ˆåŒæ–¹æœºä¼šå‡ç­‰ï¼‰
    - V(å³å°†è·èƒœçš„çŠ¶æ€) â‰ˆ 1.0ï¼ˆå¾ˆå¯èƒ½èµ¢ï¼‰
    - V(å³å°†å¤±è´¥çš„çŠ¶æ€) â‰ˆ 0.0ï¼ˆå¾ˆå¯èƒ½è¾“ï¼‰
    
    å…³é”®æ€æƒ³ï¼š
    é€šè¿‡ä¸æ–­å¯¹å¼ˆï¼Œé€æ¸å­¦ä¹ æ¯ä¸ªçŠ¶æ€çš„çœŸå®ä»·å€¼ï¼
    """
    
    def __init__(self, player: int = 1, initial_value: float = 0.5):
        """
        åˆå§‹åŒ–ä»·å€¼å‡½æ•°
        
        å‚æ•°:
            player: ç©å®¶æ ‡è¯† (1æˆ–-1)
            initial_value: åˆå§‹ä»·å€¼ä¼°è®¡ï¼ˆä¹è§‚ï¼Ÿæ‚²è§‚ï¼Ÿä¸­ç«‹ï¼Ÿï¼‰
            
        åˆå§‹å€¼çš„é€‰æ‹©å¾ˆé‡è¦ï¼š
        - 0.5 = ä¸­ç«‹ï¼Œä¸çŸ¥é“å¥½å
        - 1.0 = ä¹è§‚ï¼Œå‡è®¾éƒ½èƒ½èµ¢ï¼ˆé¼“åŠ±æ¢ç´¢ï¼‰
        - 0.0 = æ‚²è§‚ï¼Œå‡è®¾éƒ½ä¼šè¾“
        """
        self.values = {}  # çŠ¶æ€ -> ä»·å€¼çš„æ˜ å°„
        self.player = player
        self.initial_value = initial_value
        
    def get_value(self, state: tuple) -> float:
        """
        è·å–çŠ¶æ€çš„ä»·å€¼
        
        å¦‚æœæ˜¯æ–°çŠ¶æ€ï¼Œè¿”å›åˆå§‹å€¼
        è¿™å®ç°äº†"ä¹è§‚åˆå§‹å€¼"æ¢ç´¢ç­–ç•¥
        """
        if state not in self.values:
            self.values[state] = self.initial_value
        return self.values[state]
    
    def update_td(self, state: tuple, next_state: tuple, 
                  reward: float, alpha: float = 0.1):
        """
        æ—¶åºå·®åˆ†(TD)æ›´æ–° - å¼ºåŒ–å­¦ä¹ çš„æ ¸å¿ƒç®—æ³•ï¼
        
        TDæ›´æ–°å…¬å¼ï¼ˆä¹¦ä¸­æœ€é‡è¦çš„å…¬å¼ä¹‹ä¸€ï¼‰ï¼š
        V(S_t) â† V(S_t) + Î±[R_{t+1} + Î³V(S_{t+1}) - V(S_t)]
        
        å…¶ä¸­ï¼š
        - Î±: å­¦ä¹ ç‡ï¼ˆæˆ‘ä»¬å¤šç›¸ä¿¡æ–°ç»éªŒï¼‰
        - R_{t+1}: ç«‹å³å¥–åŠ±
        - Î³: æŠ˜æ‰£å› å­ï¼ˆæœªæ¥çš„é‡è¦æ€§ï¼‰
        - V(S_{t+1}): ä¸‹ä¸€çŠ¶æ€çš„ä»·å€¼ä¼°è®¡
        - V(S_t): å½“å‰çŠ¶æ€çš„ä»·å€¼ä¼°è®¡
        
        ç›´è§‰ç†è§£ï¼š
        å¦‚æœä¸‹ä¸€ä¸ªçŠ¶æ€æ¯”é¢„æœŸå¥½ â†’ æé«˜å½“å‰çŠ¶æ€ä»·å€¼
        å¦‚æœä¸‹ä¸€ä¸ªçŠ¶æ€æ¯”é¢„æœŸå·® â†’ é™ä½å½“å‰çŠ¶æ€ä»·å€¼
        """
        current_value = self.get_value(state)
        next_value = self.get_value(next_state) if next_state else 0
        
        # TDè¯¯å·® = (å¥–åŠ± + æœªæ¥ä»·å€¼) - å½“å‰ä¼°è®¡
        td_error = (reward + next_value) - current_value
        
        # æ›´æ–°ä»·å€¼
        self.values[state] = current_value + alpha * td_error
    
    def update_monte_carlo(self, episode_states: List[tuple], 
                           final_reward: float, alpha: float = 0.1):
        """
        è’™ç‰¹å¡æ´›æ›´æ–° - å¦ä¸€ç§å­¦ä¹ æ–¹å¼
        
        ä¸TDä¸åŒï¼ŒMCç­‰åˆ°æ¸¸æˆç»“æŸæ‰æ›´æ–°
        æ ¹æ®æœ€ç»ˆç»“æœæ›´æ–°æ•´æ¡è·¯å¾„ä¸Šçš„æ‰€æœ‰çŠ¶æ€
        
        è¿™å±•ç¤ºäº†RLä¸­çš„ä¸¤å¤§å­¦ä¹ èŒƒå¼ï¼š
        1. TD: è¾¹èµ°è¾¹å­¦ï¼ˆåœ¨çº¿å­¦ä¹ ï¼‰
        2. MC: èµ°å®Œå†å­¦ï¼ˆç¦»çº¿å­¦ä¹ ï¼‰
        """
        for state in episode_states:
            current_value = self.get_value(state)
            # å‘æœ€ç»ˆç»“æœé æ‹¢
            self.values[state] = current_value + alpha * (final_reward - current_value)


# ================================================================================
# ç¬¬1.4.3èŠ‚ï¼šå¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“
# Section 1.4.3: Reinforcement Learning Agent
# ================================================================================

class TicTacToeAgent:
    """
    äº•å­—æ£‹å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“
    
    è¿™ä¸ªæ™ºèƒ½ä½“å±•ç¤ºäº†å®Œæ•´çš„å¼ºåŒ–å­¦ä¹ å¾ªç¯ï¼š
    1. è§‚å¯ŸçŠ¶æ€ (Observe State)
    2. é€‰æ‹©åŠ¨ä½œ (Select Action) - åŸºäºä»·å€¼å‡½æ•°
    3. æ‰§è¡ŒåŠ¨ä½œ (Execute Action)
    4. è·å¾—å¥–åŠ± (Receive Reward)
    5. å­¦ä¹ æ›´æ–° (Learn and Update)
    
    å…³é”®ç»„ä»¶ï¼š
    - ä»·å€¼å‡½æ•°ï¼šè¯„ä¼°çŠ¶æ€å¥½å
    - æ¢ç´¢ç­–ç•¥ï¼šÎµ-è´ªå©ª
    - å­¦ä¹ ç®—æ³•ï¼šTDå­¦ä¹ 
    """
    
    def __init__(self, player: int = 1, epsilon: float = 0.1, 
                 alpha: float = 0.1, name: str = "RL_Agent"):
        """
        åˆå§‹åŒ–æ™ºèƒ½ä½“
        
        å‚æ•°è¯´æ˜ï¼š
        player: ç©å®¶æ ‡è¯† (1=X, -1=O)
        epsilon: æ¢ç´¢ç‡ï¼ˆæ¢ç´¢vsåˆ©ç”¨çš„å¹³è¡¡ï¼‰
        alpha: å­¦ä¹ ç‡ï¼ˆå­¦ä¹ é€Ÿåº¦ï¼‰
        name: æ™ºèƒ½ä½“åç§°
        
        è¿™äº›è¶…å‚æ•°çš„é€‰æ‹©è‰ºæœ¯ï¼š
        - Îµå¤ªå°ï¼šå¯èƒ½é”™è¿‡æ›´å¥½çš„ç­–ç•¥
        - Îµå¤ªå¤§ï¼šå­¦ä¹ å¤ªæ…¢ï¼Œæ€»åœ¨éšæœºæ¢ç´¢
        - Î±å¤ªå°ï¼šå­¦ä¹ å¤ªæ…¢
        - Î±å¤ªå¤§ï¼šä¸ç¨³å®šï¼Œæ–°ç»éªŒè¦†ç›–æ—§çŸ¥è¯†
        """
        self.player = player
        self.epsilon = epsilon
        self.alpha = alpha
        self.name = name
        self.value_function = ValueFunction(player)
        
        # è®°å½•å­¦ä¹ å†å²
        self.state_history = []  # ä¸€å±€æ¸¸æˆçš„çŠ¶æ€åºåˆ—
        self.win_history = []    # èƒœç‡è®°å½•
        
    def select_action(self, board: TicTacToeBoard, training: bool = True) -> int:
        """
        é€‰æ‹©åŠ¨ä½œ - Îµ-è´ªå©ªç­–ç•¥
        
        è¿™æ˜¯å¼ºåŒ–å­¦ä¹ çš„æ ¸å¿ƒå†³ç­–è¿‡ç¨‹ï¼
        
        è®­ç»ƒæ—¶ï¼š
        - æœ‰Îµæ¦‚ç‡éšæœºæ¢ç´¢ï¼ˆå°è¯•æ–°ç­–ç•¥ï¼‰
        - æœ‰1-Îµæ¦‚ç‡é€‰æ‹©æœ€ä¼˜ï¼ˆåˆ©ç”¨å·²çŸ¥çŸ¥è¯†ï¼‰
        
        æµ‹è¯•æ—¶ï¼š
        - æ€»æ˜¯é€‰æ‹©æœ€ä¼˜ï¼ˆå±•ç¤ºå­¦ä¹ æˆæœï¼‰
        """
        available_actions = board.get_available_actions()
        
        if training and np.random.random() < self.epsilon:
            # æ¢ç´¢ï¼šéšæœºé€‰æ‹©
            # å°±åƒå°æ˜å¶å°”ä¼šå°è¯•æ–°çš„ä¸‹æ³•
            return np.random.choice(available_actions)
        else:
            # åˆ©ç”¨ï¼šé€‰æ‹©ä»·å€¼æœ€é«˜çš„åŠ¨ä½œ
            # è¿™éœ€è¦"å‘å‰çœ‹ä¸€æ­¥"
            action_values = []
            
            for action in available_actions:
                # æƒ³è±¡ï¼šå¦‚æœæˆ‘ä¸‹è¿™ä¸€æ­¥ï¼Œæ£‹ç›˜ä¼šå˜æˆä»€ä¹ˆæ ·ï¼Ÿ
                future_board = self._imagine_move(board, action)
                future_state = future_board.get_state()
                
                # è¿™ä¸ªæœªæ¥çŠ¶æ€çš„ä»·å€¼æ˜¯å¤šå°‘ï¼Ÿ
                # æ³¨æ„ï¼šå¯¹æ‰‹çš„ä»·å€¼è¦å–åï¼
                if board.current_player == self.player:
                    value = self.value_function.get_value(future_state)
                else:
                    value = 1 - self.value_function.get_value(future_state)
                
                action_values.append((action, value))
            
            # é€‰æ‹©ä»·å€¼æœ€é«˜çš„åŠ¨ä½œ
            # å¦‚æœæœ‰å¤šä¸ªæœ€ä¼˜ï¼Œéšæœºé€‰ä¸€ä¸ª
            max_value = max(v for _, v in action_values)
            best_actions = [a for a, v in action_values if v == max_value]
            return np.random.choice(best_actions)
    
    def _imagine_move(self, board: TicTacToeBoard, action: int) -> TicTacToeBoard:
        """
        æƒ³è±¡ä¸‹ä¸€æ­¥åçš„æ£‹ç›˜
        
        è¿™æ˜¯"å‰å‘æ€è€ƒ"çš„èƒ½åŠ›
        ä¸å®é™…æ”¹å˜æ£‹ç›˜ï¼Œåªæ˜¯æƒ³è±¡ç»“æœ
        """
        imaginary_board = TicTacToeBoard()
        imaginary_board.board = board.board.copy()
        imaginary_board.current_player = board.current_player
        imaginary_board.board.flat[action] = board.current_player
        return imaginary_board
    
    def start_episode(self):
        """å¼€å§‹æ–°çš„ä¸€å±€æ¸¸æˆ"""
        self.state_history = []
    
    def observe_and_act(self, board: TicTacToeBoard, training: bool = True) -> int:
        """
        è§‚å¯ŸçŠ¶æ€å¹¶é‡‡å–è¡ŒåŠ¨
        
        å®Œæ•´çš„æ™ºèƒ½ä½“å†³ç­–æµç¨‹
        """
        # è®°å½•å½“å‰çŠ¶æ€
        state = board.get_state()
        self.state_history.append(state)
        
        # é€‰æ‹©å¹¶è¿”å›åŠ¨ä½œ
        return self.select_action(board, training)
    
    def learn_from_episode(self, final_reward: float):
        """
        ä»ä¸€å±€æ¸¸æˆä¸­å­¦ä¹ 
        
        æ¸¸æˆç»“æŸåï¼Œå›é¡¾æ•´å±€æ¸¸æˆï¼Œæ›´æ–°ä»·å€¼å‡½æ•°
        
        å…³é”®æ€æƒ³ï¼š
        - èµ¢äº†ï¼šæé«˜è·¯å¾„ä¸Šæ‰€æœ‰çŠ¶æ€çš„ä»·å€¼
        - è¾“äº†ï¼šé™ä½è·¯å¾„ä¸Šæ‰€æœ‰çŠ¶æ€çš„ä»·å€¼
        - å¹³å±€ï¼šå°å¹…è°ƒæ•´
        
        è¿™å°±æ˜¯"å»¶è¿Ÿå¥–åŠ±"çš„å­¦ä¹ ï¼
        """
        if not self.state_history:
            return
        
        # TDå­¦ä¹ ï¼šä»åå‘å‰æ›´æ–°
        # ä¸ºä»€ä¹ˆä»åå‘å‰ï¼Ÿå› ä¸ºéœ€è¦ç”¨åˆ°"ä¸‹ä¸€çŠ¶æ€"çš„ä»·å€¼
        for i in range(len(self.state_history) - 1, -1, -1):
            state = self.state_history[i]
            
            if i == len(self.state_history) - 1:
                # æœ€åä¸€ä¸ªçŠ¶æ€ï¼Œç›´æ¥ç”¨æœ€ç»ˆå¥–åŠ±
                next_state = None
                reward = final_reward
            else:
                # ä¸­é—´çŠ¶æ€ï¼Œå¥–åŠ±ä¸º0ï¼Œä½†æœ‰ä¸‹ä¸€çŠ¶æ€ä»·å€¼
                next_state = self.state_history[i + 1]
                reward = 0
            
            # TDæ›´æ–°
            self.value_function.update_td(state, next_state, reward, self.alpha)
        
        # æ¸…ç©ºå†å²ï¼Œå‡†å¤‡ä¸‹ä¸€å±€
        self.state_history = []
    
    def save_knowledge(self, filepath: str):
        """
        ä¿å­˜å­¦åˆ°çš„çŸ¥è¯†ï¼ˆä»·å€¼å‡½æ•°ï¼‰
        
        è®­ç»ƒåçš„ä»·å€¼å‡½æ•°å°±æ˜¯æ™ºèƒ½ä½“çš„"ç»éªŒ"
        å¯ä»¥ä¿å­˜ä¸‹æ¥ï¼Œä¸‹æ¬¡ç›´æ¥ä½¿ç”¨
        """
        with open(filepath, 'wb') as f:
            pickle.dump(self.value_function.values, f)
        print(f"{self.name}çš„çŸ¥è¯†å·²ä¿å­˜åˆ° {filepath}")
    
    def load_knowledge(self, filepath: str):
        """åŠ è½½ä¹‹å‰å­¦åˆ°çš„çŸ¥è¯†"""
        with open(filepath, 'rb') as f:
            self.value_function.values = pickle.load(f)
        print(f"{self.name}å·²åŠ è½½çŸ¥è¯†")


# ================================================================================
# ç¬¬1.4.4èŠ‚ï¼šè‡ªæˆ‘å¯¹å¼ˆè®­ç»ƒ
# Section 1.4.4: Self-Play Training
# ================================================================================

class SelfPlayTrainer:
    """
    è‡ªæˆ‘å¯¹å¼ˆè®­ç»ƒå™¨
    
    å¼ºåŒ–å­¦ä¹ çš„é­…åŠ›ä¹‹ä¸€ï¼šä¸éœ€è¦äººç±»å¯¹æ‰‹ï¼
    æ™ºèƒ½ä½“å¯ä»¥é€šè¿‡è‡ªæˆ‘å¯¹å¼ˆä¸æ–­æå‡
    
    å°±åƒAlphaGoé€šè¿‡è‡ªæˆ‘å¯¹å¼ˆæˆä¸ºä¸–ç•Œå† å†›
    """
    
    def __init__(self, episodes: int = 10000):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨
        
        episodes: è®­ç»ƒå±€æ•°
        æ›´å¤šçš„è®­ç»ƒ = æ›´å¼ºçš„æ™ºèƒ½ä½“
        """
        self.episodes = episodes
        self.board = TicTacToeBoard()
        
    def train(self, agent1: TicTacToeAgent, agent2: TicTacToeAgent, 
              verbose: bool = True) -> Dict:
        """
        è®­ç»ƒä¸¤ä¸ªæ™ºèƒ½ä½“
        
        é€šè¿‡å¤§é‡å¯¹å¼ˆï¼Œä¸¤ä¸ªæ™ºèƒ½ä½“äº’ç›¸å­¦ä¹ ï¼Œå…±åŒè¿›æ­¥
        è¿™å±•ç¤ºäº†"ååŒè¿›åŒ–"çš„æ¦‚å¿µ
        """
        results = {'agent1_wins': 0, 'agent2_wins': 0, 'draws': 0}
        win_rates = []
        
        for episode in range(self.episodes):
            # å¼€å§‹æ–°æ¸¸æˆ
            self.board.reset()
            agent1.start_episode()
            agent2.start_episode()
            
            # è½®æµä¸‹æ£‹ç›´åˆ°æ¸¸æˆç»“æŸ
            current_agent = agent1
            other_agent = agent2
            
            while True:
                # å½“å‰ç©å®¶è¡ŒåŠ¨
                action = current_agent.observe_and_act(self.board, training=True)
                reward, done = self.board.make_move(action)
                
                if done:
                    # æ¸¸æˆç»“æŸï¼ŒåŒæ–¹å­¦ä¹ 
                    if reward == agent1.player:
                        # agent1èµ¢
                        agent1.learn_from_episode(1)
                        agent2.learn_from_episode(-1)
                        results['agent1_wins'] += 1
                    elif reward == agent2.player:
                        # agent2èµ¢
                        agent1.learn_from_episode(-1)
                        agent2.learn_from_episode(1)
                        results['agent2_wins'] += 1
                    else:
                        # å¹³å±€
                        agent1.learn_from_episode(0)
                        agent2.learn_from_episode(0)
                        results['draws'] += 1
                    break
                
                # äº¤æ¢ç©å®¶
                current_agent, other_agent = other_agent, current_agent
            
            # å®šæœŸæŠ¥å‘Šè¿›åº¦
            if (episode + 1) % 1000 == 0:
                win_rate = results['agent1_wins'] / (episode + 1)
                win_rates.append(win_rate)
                
                if verbose:
                    print(f"è®­ç»ƒè¿›åº¦ Episode {episode + 1}/{self.episodes}")
                    print(f"  Agent1èƒœç‡: {win_rate:.2%}")
                    print(f"  Agent2èƒœç‡: {results['agent2_wins']/(episode+1):.2%}")
                    print(f"  å¹³å±€ç‡: {results['draws']/(episode+1):.2%}")
                    print(f"  Agent1å·²å­¦ä¹ {len(agent1.value_function.values)}ä¸ªçŠ¶æ€")
        
        return results, win_rates
    
    def demonstrate_learning(self):
        """
        æ¼”ç¤ºå­¦ä¹ è¿‡ç¨‹
        
        å±•ç¤ºæ™ºèƒ½ä½“å¦‚ä½•ä»éšæœºç©å®¶æˆé•¿ä¸ºé«˜æ‰‹
        """
        print("="*70)
        print("äº•å­—æ£‹è‡ªæˆ‘å¯¹å¼ˆå­¦ä¹ æ¼”ç¤º")
        print("Tic-Tac-Toe Self-Play Learning Demonstration")
        print("="*70)
        
        # åˆ›å»ºä¸¤ä¸ªæ™ºèƒ½ä½“
        agent_x = TicTacToeAgent(player=1, epsilon=0.3, alpha=0.1, name="Agent_X")
        agent_o = TicTacToeAgent(player=-1, epsilon=0.3, alpha=0.1, name="Agent_O")
        
        print("\nåˆå§‹çŠ¶æ€ï¼šä¸¤ä¸ªæ™ºèƒ½ä½“éƒ½æ˜¯æ–°æ‰‹")
        print("Initial: Both agents are beginners")
        print("-"*40)
        
        # æµ‹è¯•åˆå§‹æ°´å¹³
        self._test_against_random(agent_x, n_games=100, verbose=True)
        
        # å¼€å§‹è®­ç»ƒ
        print("\nå¼€å§‹è‡ªæˆ‘å¯¹å¼ˆè®­ç»ƒ...")
        print("Starting self-play training...")
        print("-"*40)
        
        trainer = SelfPlayTrainer(episodes=5000)
        results, win_rates = trainer.train(agent_x, agent_o, verbose=True)
        
        print("\nè®­ç»ƒå®Œæˆï¼")
        print("Training complete!")
        print("-"*40)
        
        # æµ‹è¯•è®­ç»ƒåæ°´å¹³
        print("\nè®­ç»ƒåæ°´å¹³æµ‹è¯•ï¼š")
        print("Post-training test:")
        self._test_against_random(agent_x, n_games=100, verbose=True)
        
        # å±•ç¤ºå­¦ä¹ æ›²çº¿
        self._plot_learning_curve(win_rates)
        
        return agent_x, agent_o
    
    def _test_against_random(self, agent: TicTacToeAgent, 
                             n_games: int = 100, verbose: bool = True):
        """æµ‹è¯•æ™ºèƒ½ä½“å¯¹æˆ˜éšæœºç©å®¶"""
        wins = 0
        draws = 0
        
        for _ in range(n_games):
            self.board.reset()
            agent.start_episode()
            
            # éšæœºå†³å®šè°å…ˆæ‰‹
            agent_first = np.random.choice([True, False])
            
            while True:
                if agent_first:
                    # æ™ºèƒ½ä½“å…ˆä¸‹
                    if self.board.current_player == agent.player:
                        action = agent.observe_and_act(self.board, training=False)
                    else:
                        # éšæœºç©å®¶
                        action = np.random.choice(self.board.get_available_actions())
                else:
                    # éšæœºç©å®¶å…ˆä¸‹
                    if self.board.current_player != agent.player:
                        action = np.random.choice(self.board.get_available_actions())
                    else:
                        action = agent.observe_and_act(self.board, training=False)
                
                reward, done = self.board.make_move(action)
                
                if done:
                    if reward == agent.player:
                        wins += 1
                    elif reward == 0:
                        draws += 1
                    break
        
        if verbose:
            print(f"{agent.name} vs éšæœºç©å®¶ ({n_games}å±€):")
            print(f"  èƒœç‡: {wins/n_games:.1%}")
            print(f"  å¹³å±€ç‡: {draws/n_games:.1%}")
            print(f"  è´¥ç‡: {(n_games-wins-draws)/n_games:.1%}")
    
    def _plot_learning_curve(self, win_rates: List[float]):
        """ç»˜åˆ¶å­¦ä¹ æ›²çº¿"""
        plt.figure(figsize=(10, 6))
        episodes = [i * 1000 for i in range(1, len(win_rates) + 1)]
        plt.plot(episodes, win_rates, 'b-', linewidth=2)
        plt.axhline(y=0.5, color='r', linestyle='--', alpha=0.5, label='50% baseline')
        plt.xlabel('è®­ç»ƒå±€æ•° Training Episodes')
        plt.ylabel('Agent1 èƒœç‡ Win Rate')
        plt.title('äº•å­—æ£‹è‡ªæˆ‘å¯¹å¼ˆå­¦ä¹ æ›²çº¿\nTic-Tac-Toe Self-Play Learning Curve')
        plt.grid(True, alpha=0.3)
        plt.legend()
        plt.show()


# ================================================================================
# ç¬¬1.4.5èŠ‚ï¼šäººæœºå¯¹æˆ˜
# Section 1.4.5: Human vs AI
# ================================================================================

class HumanVsAI:
    """
    äººæœºå¯¹æˆ˜ç•Œé¢
    
    æµ‹è¯•è®­ç»ƒå¥½çš„æ™ºèƒ½ä½“ï¼Œçœ‹çœ‹å®ƒå­¦åˆ°äº†ä»€ä¹ˆï¼
    """
    
    def __init__(self, agent: TicTacToeAgent):
        """åˆå§‹åŒ–å¯¹æˆ˜ç³»ç»Ÿ"""
        self.agent = agent
        self.board = TicTacToeBoard()
        
    def play(self):
        """å¼€å§‹äººæœºå¯¹æˆ˜"""
        print("\n" + "="*70)
        print("äº•å­—æ£‹äººæœºå¯¹æˆ˜")
        print("Tic-Tac-Toe: Human vs AI")
        print("="*70)
        
        print("\næ¸¸æˆè¯´æ˜:")
        print("è¾“å…¥ä½ç½®ç¼–å·(0-8)æ¥ä¸‹æ£‹ï¼š")
        print("  0 1 2")
        print("  3 4 5")
        print("  6 7 8")
        
        # éšæœºå†³å®šè°å…ˆæ‰‹
        human_first = np.random.choice([True, False])
        human_player = 1 if human_first else -1
        
        print(f"\nä½ æ˜¯ {'X' if human_player == 1 else 'O'}")
        print(f"{'ä½ ' if human_first else 'AI'}å…ˆæ‰‹")
        
        self.board.reset()
        self.agent.start_episode()
        
        while True:
            self.board.render()
            
            if self.board.current_player == human_player:
                # äººç±»å›åˆ
                while True:
                    try:
                        action = int(input("ä½ çš„é€‰æ‹© (0-8): "))
                        if action in self.board.get_available_actions():
                            break
                        else:
                            print("è¯¥ä½ç½®å·²æœ‰æ£‹å­ï¼Œè¯·é‡æ–°é€‰æ‹©")
                    except:
                        print("è¯·è¾“å…¥0-8ä¹‹é—´çš„æ•°å­—")
            else:
                # AIå›åˆ
                print("AIæ€è€ƒä¸­...")
                action = self.agent.observe_and_act(self.board, training=False)
                print(f"AIé€‰æ‹©: {action}")
            
            reward, done = self.board.make_move(action)
            
            if done:
                self.board.render()
                if reward == human_player:
                    print("\nğŸ‰ æ­å–œä½ èµ¢äº†ï¼")
                elif reward == -human_player:
                    print("\nğŸ˜” AIèµ¢äº†ï¼")
                else:
                    print("\nğŸ¤ å¹³å±€ï¼")
                
                # è¯¢é—®æ˜¯å¦å†æ¥ä¸€å±€
                again = input("\nå†æ¥ä¸€å±€ï¼Ÿ(y/n): ").lower()
                if again == 'y':
                    self.board.reset()
                    self.agent.start_episode()
                    human_first = not human_first
                    human_player = 1 if human_first else -1
                    print(f"\næ–°æ¸¸æˆï¼ä½ æ˜¯ {'X' if human_player == 1 else 'O'}")
                    print(f"{'ä½ ' if human_first else 'AI'}å…ˆæ‰‹")
                else:
                    break


# ================================================================================
# å®è·µï¼šå®Œæ•´çš„äº•å­—æ£‹å¼ºåŒ–å­¦ä¹ 
# Practice: Complete Tic-Tac-Toe Reinforcement Learning
# ================================================================================

def main():
    """ä¸»ç¨‹åºï¼šå±•ç¤ºå®Œæ•´çš„å¼ºåŒ–å­¦ä¹ è¿‡ç¨‹"""
    
    print("â•”" + "â•"*68 + "â•—")
    print("â•‘" + " "*15 + "ç¬¬1ç« ï¼šäº•å­—æ£‹å¼ºåŒ–å­¦ä¹ å®Œæ•´å®ç°".center(38) + " "*15 + "â•‘")
    print("â•‘" + " "*10 + "Chapter 1: Complete Tic-Tac-Toe RL Implementation".center(48) + " "*10 + "â•‘")
    print("â•š" + "â•"*68 + "â•")
    
    print("\næ¬¢è¿æ¥åˆ°äº•å­—æ£‹å¼ºåŒ–å­¦ä¹ ä¸–ç•Œï¼")
    print("Welcome to Tic-Tac-Toe Reinforcement Learning!")
    
    # 1. æ¼”ç¤ºå­¦ä¹ è¿‡ç¨‹
    print("\nã€ç¬¬1éƒ¨åˆ†ï¼šè‡ªæˆ‘å¯¹å¼ˆå­¦ä¹ ã€‘")
    print("[Part 1: Self-Play Learning]")
    print("="*70)
    
    trainer = SelfPlayTrainer()
    agent_x, agent_o = trainer.demonstrate_learning()
    
    # 2. åˆ†æå­¦åˆ°çš„ç­–ç•¥
    print("\nã€ç¬¬2éƒ¨åˆ†ï¼šç­–ç•¥åˆ†æã€‘")
    print("[Part 2: Strategy Analysis]")
    print("="*70)
    
    print("\nè®©æˆ‘ä»¬çœ‹çœ‹æ™ºèƒ½ä½“å­¦åˆ°äº†ä»€ä¹ˆç­–ç•¥ï¼š")
    print("Let's see what strategies the agent learned:")
    
    # åˆ†æä¸€äº›å…³é”®çŠ¶æ€çš„ä»·å€¼
    board = TicTacToeBoard()
    
    # åˆå§‹çŠ¶æ€
    print("\n1. åˆå§‹çŠ¶æ€ä»·å€¼:")
    initial_state = board.get_state()
    print(f"   V(empty board) = {agent_x.value_function.get_value(initial_state):.3f}")
    print("   (åº”è¯¥æ¥è¿‘0.5ï¼Œè¡¨ç¤ºåŒæ–¹æœºä¼šå‡ç­‰)")
    
    # ä¸­å¿ƒæ ¼å­
    board.board[1, 1] = 1
    center_state = board.get_state()
    print(f"\n2. å æ®ä¸­å¿ƒåçš„ä»·å€¼:")
    print(f"   V(X in center) = {agent_x.value_function.get_value(center_state):.3f}")
    print("   (åº”è¯¥>0.5ï¼Œä¸­å¿ƒæ˜¯å¥½ä½ç½®)")
    
    # å³å°†è·èƒœ
    board.reset()
    board.board[0, 0] = 1
    board.board[0, 1] = 1
    winning_state = board.get_state()
    print(f"\n3. å³å°†è¿çº¿çš„ä»·å€¼:")
    print(f"   V(two X in row) = {agent_x.value_function.get_value(winning_state):.3f}")
    print("   (åº”è¯¥æ¥è¿‘1.0ï¼Œé©¬ä¸Šå°±èµ¢äº†)")
    
    # 3. äººæœºå¯¹æˆ˜
    print("\nã€ç¬¬3éƒ¨åˆ†ï¼šäººæœºå¯¹æˆ˜ã€‘")
    print("[Part 3: Human vs AI]")
    print("="*70)
    
    play_vs_ai = input("\næƒ³è¦æŒ‘æˆ˜AIå—ï¼Ÿ(y/n): ").lower()
    if play_vs_ai == 'y':
        game = HumanVsAI(agent_x)
        game.play()
    
    # 4. æ€»ç»“
    print("\n" + "="*70)
    print("å­¦ä¹ æ€»ç»“ Learning Summary")
    print("="*70)
    print("""
    é€šè¿‡äº•å­—æ£‹ï¼Œæˆ‘ä»¬å­¦åˆ°äº†å¼ºåŒ–å­¦ä¹ çš„æ ¸å¿ƒæ¦‚å¿µï¼š
    
    1. ä»·å€¼å‡½æ•° Value Function V(s)
       - è¯„ä¼°æ¯ä¸ªçŠ¶æ€çš„å¥½å
       - é€šè¿‡ç»éªŒä¸æ–­æ›´æ–°
    
    2. æ—¶åºå·®åˆ†å­¦ä¹  TD Learning
       - V(s) â† V(s) + Î±[R + V(s') - V(s)]
       - ä»æ¯ä¸€æ­¥ä¸­å­¦ä¹ 
    
    3. æ¢ç´¢vsåˆ©ç”¨ Exploration vs Exploitation
       - Îµ-è´ªå©ªç­–ç•¥å¹³è¡¡ä¸¤è€…
       - æ—¢è¦å°è¯•æ–°ç­–ç•¥ï¼Œä¹Ÿè¦ç”¨å·²çŸ¥æœ€å¥½çš„
    
    4. è‡ªæˆ‘å¯¹å¼ˆ Self-Play
       - ä¸éœ€è¦äººç±»ä¸“å®¶
       - é€šè¿‡è‡ªæˆ‘æå‡è¾¾åˆ°é«˜æ°´å¹³
    
    è¿™äº›æ¦‚å¿µå°†è´¯ç©¿æ•´ä¸ªå¼ºåŒ–å­¦ä¹ ï¼
    These concepts will run through all of RL!
    
    ä¸‹ä¸€æ­¥ï¼šå­¦ä¹ æ›´å¤æ‚çš„é—®é¢˜å’Œç®—æ³•
    Next: Learn more complex problems and algorithms
    """)
    
    # ä¿å­˜è®­ç»ƒå¥½çš„æ™ºèƒ½ä½“
    save = input("\nä¿å­˜è®­ç»ƒå¥½çš„æ™ºèƒ½ä½“å—ï¼Ÿ(y/n): ").lower()
    if save == 'y':
        agent_x.save_knowledge("tictactoe_agent_x.pkl")
        agent_o.save_knowledge("tictactoe_agent_o.pkl")
        print("æ™ºèƒ½ä½“å·²ä¿å­˜ï¼")


if __name__ == "__main__":
    main()