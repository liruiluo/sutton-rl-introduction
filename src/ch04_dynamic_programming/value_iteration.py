"""
================================================================================
ç¬¬3.3èŠ‚ï¼šä»·å€¼è¿­ä»£ - ç›´æ¥å¯»æ‰¾æœ€ä¼˜ä»·å€¼å‡½æ•°
Section 3.3: Value Iteration - Finding Optimal Value Function Directly
================================================================================

ä»·å€¼è¿­ä»£æ˜¯å¦ä¸€ç§åŠ¨æ€è§„åˆ’ç®—æ³•ï¼Œå®ƒè·³è¿‡äº†æ˜¾å¼çš„ç­–ç•¥è¡¨ç¤ºï¼Œç›´æ¥å¯»æ‰¾æœ€ä¼˜ä»·å€¼å‡½æ•°ã€‚
Value Iteration is another DP algorithm that skips explicit policy representation
and finds optimal value function directly.

æ ¸å¿ƒæ€æƒ³ï¼šåå¤åº”ç”¨è´å°”æ›¼æœ€ä¼˜ç®—å­
Core idea: Repeatedly apply Bellman optimality operator
v_{k+1}(s) = max_a Î£_{s',r} p(s',r|s,a)[r + Î³v_k(s')]

è¿™å¯ä»¥çœ‹ä½œæ˜¯"æˆªæ–­çš„ç­–ç•¥è¿­ä»£"ï¼Œæ¯æ¬¡åªåšä¸€æ­¥ç­–ç•¥è¯„ä¼°å°±ç«‹å³æ”¹è¿›ã€‚
This can be viewed as "truncated policy iteration" with only one step of evaluation before improvement.

ä¸ºä»€ä¹ˆå«"ä»·å€¼è¿­ä»£"ï¼Ÿ
Why called "Value Iteration"?
å› ä¸ºæˆ‘ä»¬ç›´æ¥è¿­ä»£ä»·å€¼å‡½æ•°ï¼Œç­–ç•¥æ˜¯éšå«çš„ï¼ˆä»ä»·å€¼å‡½æ•°è´ªå©ªå¯¼å‡ºï¼‰ã€‚
Because we directly iterate value function, policy is implicit (derived greedily from values).

ä¼˜åŠ¿ vs ç­–ç•¥è¿­ä»£ï¼š
Advantages vs Policy Iteration:
- æ¯æ¬¡è¿­ä»£è®¡ç®—é‡æ›´å°ï¼ˆä¸éœ€è¦å®Œæ•´ç­–ç•¥è¯„ä¼°ï¼‰
  Less computation per iteration (no full policy evaluation)
- å®ç°æ›´ç®€å•ï¼ˆä¸éœ€è¦å­˜å‚¨ç­–ç•¥ï¼‰
  Simpler implementation (no need to store policy)
- å¯ä»¥éšæ—¶åœæ­¢å¾—åˆ°è¿‘ä¼¼è§£
  Can stop anytime to get approximation

åŠ£åŠ¿ï¼š
Disadvantages:
- éœ€è¦æ›´å¤šè¿­ä»£æ¬¡æ•°æ‰èƒ½æ”¶æ•›
  Needs more iterations to converge
- ä¸­é—´è¿‡ç¨‹æ²¡æœ‰æ˜ç¡®çš„ç­–ç•¥
  No explicit policy during process
"""

import numpy as np
from typing import Dict, List, Tuple, Optional, Any, Callable
from dataclasses import dataclass, field
import logging
import matplotlib.pyplot as plt
import matplotlib.animation as animation
from matplotlib.patches import Rectangle, FancyBboxPatch
import seaborn as sns
from collections import defaultdict
import time
from IPython.display import HTML

# å¯¼å…¥åŸºç¡€ç»„ä»¶
import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent))

from src.ch03_finite_mdp.mdp_framework import (
    State, Action, MDPEnvironment
)
from src.ch03_finite_mdp.policies_and_values import (
    Policy, StateValueFunction, ActionValueFunction,
    DeterministicPolicy
)
from .dp_foundations import BellmanOperator

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


# ================================================================================
# ç¬¬3.3.1èŠ‚ï¼šä»·å€¼è¿­ä»£ç®—æ³•
# Section 3.3.1: Value Iteration Algorithm
# ================================================================================

class ValueIteration:
    """
    ä»·å€¼è¿­ä»£ç®—æ³•
    Value Iteration Algorithm
    
    ç®—æ³•ä¼ªä»£ç ï¼š
    Algorithm Pseudocode:
    ```
    åˆå§‹åŒ– Initialize:
    å¯¹æ‰€æœ‰sâˆˆSï¼ŒV(s) = 0ï¼ˆé™¤äº†ç»ˆæ­¢çŠ¶æ€ï¼‰
    For all sâˆˆS, V(s) = 0 (except terminal states)
    
    é‡å¤ Repeat:
        Î” â† 0
        å¯¹æ¯ä¸ªsâˆˆSï¼š
        For each sâˆˆS:
            v â† V(s)
            V(s) â† max_a Î£_{s',r} p(s',r|s,a)[r + Î³V(s')]
            Î” â† max(Î”, |v - V(s)|)
    ç›´åˆ°Î” < Î¸
    until Î” < Î¸
    
    è¾“å‡ºç¡®å®šæ€§ç­–ç•¥ï¼š
    Output deterministic policy:
    Ï€(s) = argmax_a Î£_{s',r} p(s',r|s,a)[r + Î³V(s')]
    ```
    
    æ•°å­¦åŸç†ï¼š
    Mathematical Principle:
    - è´å°”æ›¼æœ€ä¼˜ç®—å­T*æ˜¯Î³-æ”¶ç¼©æ˜ å°„
      Bellman optimality operator T* is Î³-contraction
    - æœ‰å”¯ä¸€ä¸åŠ¨ç‚¹v*ï¼ˆæœ€ä¼˜ä»·å€¼å‡½æ•°ï¼‰
      Has unique fixed point v* (optimal value function)
    - ä»ä»»æ„åˆå§‹å€¼å¼€å§‹éƒ½ä¼šæ”¶æ•›åˆ°v*
      Converges to v* from any initial value
    
    æ”¶æ•›é€Ÿåº¦ï¼š
    Convergence Rate:
    ||v_{k+1} - v*||âˆ â‰¤ Î³||v_k - v*||âˆ
    
    è¿™æ„å‘³ç€è¯¯å·®ä»¥Î³çš„é€Ÿåº¦æŒ‡æ•°è¡°å‡ï¼
    This means error decays exponentially at rate Î³!
    """
    
    def __init__(self, mdp_env: MDPEnvironment, gamma: float = 0.99):
        """
        åˆå§‹åŒ–ä»·å€¼è¿­ä»£
        Initialize Value Iteration
        
        Args:
            mdp_env: MDPç¯å¢ƒ
            gamma: æŠ˜æ‰£å› å­
        
        ä¸ºä»€ä¹ˆgammaé‡è¦ï¼Ÿ
        Why is gamma important?
        - Î³æ¥è¿‘1ï¼šè€ƒè™‘é•¿è¿œï¼Œæ”¶æ•›æ…¢
          Î³ close to 1: long-term focus, slow convergence
        - Î³æ¥è¿‘0ï¼šçŸ­è§†ï¼Œæ”¶æ•›å¿«
          Î³ close to 0: myopic, fast convergence
        - Î³å†³å®šäº†æ”¶ç¼©é€Ÿåº¦å’Œæœ€ç»ˆä»·å€¼å¤§å°
          Î³ determines contraction rate and value magnitude
        """
        self.env = mdp_env
        self.gamma = gamma
        self.bellman_op = BellmanOperator(mdp_env, gamma)
        
        # è®°å½•è¿­ä»£å†å²
        self.iteration_history = []
        self.convergence_history = []
        
        # æ€§èƒ½ç»Ÿè®¡
        self.total_iterations = 0
        self.total_time = 0.0
        
        logger.info(f"åˆå§‹åŒ–ä»·å€¼è¿­ä»£ï¼ŒÎ³={gamma}")
        logger.info(f"çŠ¶æ€ç©ºé—´å¤§å°: {len(mdp_env.state_space)}")
        logger.info(f"åŠ¨ä½œç©ºé—´å¤§å°: {len(mdp_env.action_space)}")
    
    def solve(self,
             theta: float = 1e-6,
             max_iterations: int = 1000,
             initial_v: Optional[StateValueFunction] = None,
             verbose: bool = True) -> Tuple[Policy, StateValueFunction]:
        """
        è¿è¡Œä»·å€¼è¿­ä»£ç®—æ³•
        Run Value Iteration Algorithm
        
        ä¸ç­–ç•¥è¿­ä»£çš„å…³é”®åŒºåˆ«ï¼š
        Key difference from Policy Iteration:
        - ä¸ç»´æŠ¤æ˜¾å¼ç­–ç•¥ï¼Œåªç»´æŠ¤ä»·å€¼å‡½æ•°
          No explicit policy, only value function
        - æ¯æ¬¡è¿­ä»£éƒ½æ˜¯ä¸€æ­¥è´å°”æ›¼æœ€ä¼˜æ›´æ–°
          Each iteration is one Bellman optimality update
        - æœ€åä»ä»·å€¼å‡½æ•°æå–ç­–ç•¥
          Extract policy from values at the end
        
        Args:
            theta: æ”¶æ•›é˜ˆå€¼
                  Convergence threshold
            max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•°
                          Maximum iterations
            initial_v: åˆå§‹ä»·å€¼å‡½æ•°
                      Initial value function
            verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
                    Whether to print details
        
        Returns:
            (æœ€ä¼˜ç­–ç•¥, æœ€ä¼˜ä»·å€¼å‡½æ•°)
            (optimal policy, optimal value function)
        
        å®ç°ç»†èŠ‚ï¼š
        Implementation Details:
        1. åŒæ­¥æ›´æ–°ï¼šéœ€è¦ä¸¤ä¸ªæ•°ç»„å­˜å‚¨æ–°æ—§å€¼
           Synchronous update: need two arrays for old and new values
        2. åŸåœ°æ›´æ–°ä¹Ÿå¯ä»¥ï¼ˆGauss-Seidelé£æ ¼ï¼‰ï¼Œå¯èƒ½æ›´å¿«æ”¶æ•›
           In-place update also works (Gauss-Seidel style), may converge faster
        3. ç»ˆæ­¢çŠ¶æ€çš„ä»·å€¼å§‹ç»ˆä¸º0
           Terminal states always have value 0
        """
        # æ¸…ç©ºå†å²
        self.iteration_history = []
        self.convergence_history = []
        
        # å¼€å§‹è®¡æ—¶
        start_time = time.time()
        
        # åˆå§‹åŒ–ä»·å€¼å‡½æ•°
        if initial_v is None:
            V = StateValueFunction(self.env.state_space, initial_value=0.0)
        else:
            V = initial_v
        
        if verbose:
            print("\n" + "="*60)
            print("å¼€å§‹ä»·å€¼è¿­ä»£")
            print("Starting Value Iteration")
            print("="*60)
            print(f"æ”¶æ•›é˜ˆå€¼ Î¸ = {theta}")
            print(f"æŠ˜æ‰£å› å­ Î³ = {self.gamma}")
            print(f"çŠ¶æ€æ•°é‡ |S| = {len(self.env.state_space)}")
            print(f"åŠ¨ä½œæ•°é‡ |A| = {len(self.env.action_space)}")
        
        # ä¸»å¾ªç¯
        for iteration in range(max_iterations):
            # è®°å½•å½“å‰ä»·å€¼å‡½æ•°ï¼ˆæ·±æ‹·è´ï¼‰
            V_old = StateValueFunction(self.env.state_space)
            for state in self.env.state_space:
                V_old.set_value(state, V.get_value(state))
            
            # åº”ç”¨è´å°”æ›¼æœ€ä¼˜ç®—å­
            # Apply Bellman optimality operator
            V_new = self.bellman_op.bellman_optimality_operator(V)
            
            # è®¡ç®—æœ€å¤§å˜åŒ–ï¼ˆæ”¶æ•›åˆ¤æ–­ï¼‰
            # Calculate maximum change (convergence check)
            delta = 0.0
            state_changes = {}  # è®°å½•æ¯ä¸ªçŠ¶æ€çš„å˜åŒ–
            
            for state in self.env.state_space:
                old_value = V.get_value(state)
                new_value = V_new.get_value(state)
                change = abs(old_value - new_value)
                delta = max(delta, change)
                state_changes[state] = change
            
            # æ›´æ–°ä»·å€¼å‡½æ•°
            V = V_new
            
            # è®°å½•å†å²
            self.iteration_history.append({
                'iteration': iteration + 1,
                'value_function': V_old,  # è®°å½•æ›´æ–°å‰çš„å€¼
                'delta': delta,
                'max_change_state': max(state_changes, key=state_changes.get) if state_changes else None
            })
            self.convergence_history.append(delta)
            
            # æ‰“å°è¿›åº¦
            if verbose and (iteration % 10 == 0 or delta < theta):
                print(f"è¿­ä»£ {iteration + 1}: Î” = {delta:.2e}")
                
                # æ˜¾ç¤ºå˜åŒ–æœ€å¤§çš„çŠ¶æ€
                if state_changes:
                    max_change_state = max(state_changes, key=state_changes.get)
                    print(f"  å˜åŒ–æœ€å¤§çš„çŠ¶æ€: {max_change_state.id} "
                          f"(Î” = {state_changes[max_change_state]:.2e})")
                
                # æ˜¾ç¤ºå‡ ä¸ªçŠ¶æ€çš„ä»·å€¼
                if iteration % 50 == 0:
                    sample_states = self.env.state_space[:min(3, len(self.env.state_space))]
                    print("  ç¤ºä¾‹çŠ¶æ€ä»·å€¼:")
                    for s in sample_states:
                        print(f"    V({s.id}) = {V.get_value(s):.3f}")
            
            # æ£€æŸ¥æ”¶æ•›
            if delta < theta:
                self.total_iterations = iteration + 1
                if verbose:
                    print(f"\nâœ“ ä»·å€¼è¿­ä»£æ”¶æ•›ï¼")
                    print(f"  è¿­ä»£æ¬¡æ•°: {self.total_iterations}")
                    print(f"  æœ€ç»ˆ Î”: {delta:.2e}")
                
                logger.info(f"ä»·å€¼è¿­ä»£åœ¨ç¬¬{self.total_iterations}æ¬¡è¿­ä»£æ”¶æ•›")
                break
        else:
            # è¾¾åˆ°æœ€å¤§è¿­ä»£
            self.total_iterations = max_iterations
            logger.warning(f"è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•° {max_iterations}ï¼ŒÎ” = {delta:.2e}")
            if verbose:
                print(f"\nâš  è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œå¯èƒ½æœªå®Œå…¨æ”¶æ•›")
                print(f"  å½“å‰ Î” = {delta:.2e} > Î¸ = {theta}")
        
        # è®°å½•æ€»æ—¶é—´
        self.total_time = time.time() - start_time
        
        # ä»æœ€ä¼˜ä»·å€¼å‡½æ•°æå–æœ€ä¼˜ç­–ç•¥
        # Extract optimal policy from optimal value function
        if verbose:
            print("\næå–æœ€ä¼˜ç­–ç•¥...")
            print("Extracting optimal policy...")
        
        optimal_policy = self._extract_policy(V)
        
        if verbose:
            print(f"\næ€»è¿è¡Œæ—¶é—´: {self.total_time:.3f}ç§’")
            print(f"å¹³å‡æ¯æ¬¡è¿­ä»£: {self.total_time/self.total_iterations:.4f}ç§’")
            
            # ç†è®ºåˆ†æ
            self._print_theoretical_analysis()
        
        return optimal_policy, V
    
    def _extract_policy(self, V: StateValueFunction) -> Policy:
        """
        ä»ä»·å€¼å‡½æ•°æå–è´ªå©ªç­–ç•¥
        Extract greedy policy from value function
        
        Ï€*(s) = argmax_a Î£_{s',r} p(s',r|s,a)[r + Î³V(s')]
        
        è¿™æ˜¯ä»·å€¼è¿­ä»£çš„å…³é”®æ­¥éª¤ï¼
        This is the key step of value iteration!
        
        æ³¨æ„ï¼šåªæœ‰åœ¨Væ¥è¿‘v*æ—¶ï¼Œæå–çš„ç­–ç•¥æ‰æ¥è¿‘Ï€*
        Note: Only when V is close to v*, extracted policy is close to Ï€*
        """
        policy_map = {}
        P = self.bellman_op.P
        
        for state in self.env.state_space:
            if state.is_terminal:
                continue
            
            # è®¡ç®—æ¯ä¸ªåŠ¨ä½œçš„Qå€¼
            action_values = {}
            for action in self.env.action_space:
                q_value = self.bellman_op._compute_q_value(state, action, V)
                action_values[action] = q_value
            
            # é€‰æ‹©æœ€ä½³åŠ¨ä½œ
            if action_values:
                best_action = max(action_values, key=action_values.get)
                policy_map[state] = best_action
                
                logger.debug(f"State {state.id}: "
                           f"Q-values = {{{', '.join(f'{a.id}:{q:.2f}' for a, q in action_values.items())}}}, "
                           f"Best = {best_action.id}")
        
        return DeterministicPolicy(policy_map)
    
    def _print_theoretical_analysis(self):
        """
        æ‰“å°ç†è®ºåˆ†æ
        Print Theoretical Analysis
        
        å¸®åŠ©ç†è§£ç®—æ³•çš„æ”¶æ•›æ€§è´¨
        Helps understand convergence properties
        """
        print("\n" + "-"*40)
        print("ç†è®ºåˆ†æ Theoretical Analysis")
        print("-"*40)
        
        # ä¼°è®¡æ”¶æ•›é€Ÿåº¦
        if len(self.convergence_history) > 10:
            # è®¡ç®—å®é™…æ”¶ç¼©ç‡
            recent_deltas = self.convergence_history[-10:]
            ratios = [recent_deltas[i+1]/recent_deltas[i] 
                     for i in range(len(recent_deltas)-1) 
                     if recent_deltas[i] > 0]
            if ratios:
                avg_ratio = np.mean(ratios)
                print(f"å®é™…æ”¶ç¼©ç‡: {avg_ratio:.3f} (ç†è®ºä¸Šç•Œ: {self.gamma})")
                print(f"Actual contraction: {avg_ratio:.3f} (theoretical bound: {self.gamma})")
        
        # ä¼°è®¡åˆ°æœ€ä¼˜çš„è·ç¦»
        final_delta = self.convergence_history[-1] if self.convergence_history else 0
        if final_delta > 0:
            # ä½¿ç”¨è¯¯å·®ç•Œï¼š||v_k - v*|| â‰¤ Î³^k/(1-Î³) * ||v_1 - v_0||
            estimated_error = final_delta / (1 - self.gamma)
            print(f"ä¼°è®¡è¯¯å·®ä¸Šç•Œ: {estimated_error:.2e}")
            print(f"Estimated error bound: {estimated_error:.2e}")
        
        # è®¡ç®—æ•ˆç‡
        total_updates = self.total_iterations * len(self.env.state_space)
        print(f"æ€»çŠ¶æ€æ›´æ–°æ¬¡æ•°: {total_updates}")
        print(f"Total state updates: {total_updates}")
    
    def get_value_evolution(self, state_indices: List[int] = None) -> np.ndarray:
        """
        è·å–ä»·å€¼å‡½æ•°æ¼”åŒ–è½¨è¿¹
        Get value function evolution trajectory
        
        ç”¨äºå¯è§†åŒ–åˆ†æ
        For visualization and analysis
        
        Args:
            state_indices: è¦è·Ÿè¸ªçš„çŠ¶æ€ç´¢å¼•
        
        Returns:
            å½¢çŠ¶ä¸º (n_iterations, n_states) çš„æ•°ç»„
            Array of shape (n_iterations, n_states)
        """
        if not self.iteration_history:
            return np.array([])
        
        states = self.env.state_space
        if state_indices is not None:
            states = [states[i] for i in state_indices if i < len(states)]
        
        n_iterations = len(self.iteration_history)
        n_states = len(states)
        
        evolution = np.zeros((n_iterations, n_states))
        
        for i, hist in enumerate(self.iteration_history):
            V = hist['value_function']
            for j, state in enumerate(states):
                evolution[i, j] = V.get_value(state)
        
        return evolution


# ================================================================================
# ç¬¬3.3.2èŠ‚ï¼šå¼‚æ­¥ä»·å€¼è¿­ä»£
# Section 3.3.2: Asynchronous Value Iteration
# ================================================================================

class AsynchronousValueIteration(ValueIteration):
    """
    å¼‚æ­¥ä»·å€¼è¿­ä»£
    Asynchronous Value Iteration
    
    ä¸åŒæ­¥ç‰ˆæœ¬çš„åŒºåˆ«ï¼š
    Difference from synchronous version:
    - åŒæ­¥ï¼šæ‰€æœ‰çŠ¶æ€åŒæ—¶æ›´æ–°ï¼ˆéœ€è¦ä¸¤ä¸ªæ•°ç»„ï¼‰
      Synchronous: all states updated simultaneously (needs two arrays)
    - å¼‚æ­¥ï¼šçŠ¶æ€æŒ‰æŸç§é¡ºåºé€ä¸ªæ›´æ–°ï¼ˆåªéœ€ä¸€ä¸ªæ•°ç»„ï¼‰
      Asynchronous: states updated one by one in some order (needs one array)
    
    ä¼˜åŠ¿ï¼š
    Advantages:
    - å†…å­˜æ•ˆç‡æ›´é«˜ï¼ˆåªéœ€ä¸€ä¸ªæ•°ç»„ï¼‰
      More memory efficient (one array)
    - å¯èƒ½æ”¶æ•›æ›´å¿«ï¼ˆæ–°ä¿¡æ¯ç«‹å³ä¼ æ’­ï¼‰
      May converge faster (new info propagates immediately)
    - æ›´çµæ´»ï¼ˆå¯ä»¥ä¼˜å…ˆæ›´æ–°é‡è¦çŠ¶æ€ï¼‰
      More flexible (can prioritize important states)
    
    å˜ä½“ï¼š
    Variants:
    1. Gauss-Seidelï¼šå›ºå®šé¡ºåºæ›´æ–°
       Fixed order update
    2. éšæœºé€‰æ‹©ï¼šéšæœºé€‰æ‹©çŠ¶æ€æ›´æ–°
       Random selection
    3. ä¼˜å…ˆçº§æ‰«æï¼šä¼˜å…ˆæ›´æ–°å˜åŒ–å¤§çš„çŠ¶æ€
       Prioritized sweeping: update high-change states first
    """
    
    def __init__(self, mdp_env: MDPEnvironment, gamma: float = 0.99,
                 update_mode: str = 'random'):
        """
        åˆå§‹åŒ–å¼‚æ­¥ä»·å€¼è¿­ä»£
        
        Args:
            update_mode: æ›´æ–°æ¨¡å¼
                - 'sequential': é¡ºåºæ›´æ–°
                - 'random': éšæœºæ›´æ–°
                - 'prioritized': ä¼˜å…ˆçº§æ›´æ–°
        """
        super().__init__(mdp_env, gamma)
        self.update_mode = update_mode
        
        # ä¼˜å…ˆçº§é˜Ÿåˆ—ï¼ˆç”¨äºä¼˜å…ˆçº§æ‰«æï¼‰
        self.priority_queue = []
        
        logger.info(f"åˆå§‹åŒ–å¼‚æ­¥ä»·å€¼è¿­ä»£ï¼Œæ¨¡å¼: {update_mode}")
    
    def solve(self,
             theta: float = 1e-6,
             max_iterations: int = 10000,
             updates_per_iteration: int = None,
             verbose: bool = True) -> Tuple[Policy, StateValueFunction]:
        """
        è¿è¡Œå¼‚æ­¥ä»·å€¼è¿­ä»£
        Run Asynchronous Value Iteration
        
        æ³¨æ„ï¼šè¿­ä»£çš„å®šä¹‰ä¸åŒ
        Note: Different definition of iteration
        - åŒæ­¥ï¼šä¸€æ¬¡è¿­ä»£ = æ›´æ–°æ‰€æœ‰çŠ¶æ€
          Synchronous: one iteration = update all states
        - å¼‚æ­¥ï¼šä¸€æ¬¡è¿­ä»£ = æ›´æ–°ä¸€ä¸ªï¼ˆæˆ–å‡ ä¸ªï¼‰çŠ¶æ€
          Asynchronous: one iteration = update one (or few) states
        
        Args:
            updates_per_iteration: æ¯æ¬¡è¿­ä»£æ›´æ–°çš„çŠ¶æ€æ•°
                                  Number of states to update per iteration
        """
        # æ¯æ¬¡è¿­ä»£æ›´æ–°çš„çŠ¶æ€æ•°
        if updates_per_iteration is None:
            updates_per_iteration = len(self.env.state_space)
        
        # åˆå§‹åŒ–
        V = StateValueFunction(self.env.state_space, initial_value=0.0)
        start_time = time.time()
        
        if verbose:
            print("\n" + "="*60)
            print(f"å¼€å§‹å¼‚æ­¥ä»·å€¼è¿­ä»£ (æ¨¡å¼: {self.update_mode})")
            print(f"Starting Asynchronous Value Iteration (mode: {self.update_mode})")
            print("="*60)
        
        # åˆå§‹åŒ–ä¼˜å…ˆçº§ï¼ˆå¦‚æœä½¿ç”¨ä¼˜å…ˆçº§æ‰«æï¼‰
        if self.update_mode == 'prioritized':
            self._initialize_priorities(V)
        
        # è®°å½•æ›´æ–°æ¬¡æ•°
        total_updates = 0
        max_delta_history = []
        
        # ä¸»å¾ªç¯
        for iteration in range(max_iterations):
            iteration_delta = 0.0
            
            # é€‰æ‹©è¦æ›´æ–°çš„çŠ¶æ€
            states_to_update = self._select_states_to_update(
                V, updates_per_iteration
            )
            
            # æ›´æ–°é€‰ä¸­çš„çŠ¶æ€
            for state in states_to_update:
                if state.is_terminal:
                    continue
                
                # è®¡ç®—æ–°å€¼ï¼ˆè´å°”æ›¼æœ€ä¼˜æ›´æ–°ï¼‰
                old_value = V.get_value(state)
                
                # è®¡ç®—max_a Q(s,a)
                max_q_value = float('-inf')
                for action in self.env.action_space:
                    q_value = self.bellman_op._compute_q_value(state, action, V)
                    max_q_value = max(max_q_value, q_value)
                
                # åŸåœ°æ›´æ–°
                V.set_value(state, max_q_value)
                
                # è®°å½•å˜åŒ–
                delta = abs(old_value - max_q_value)
                iteration_delta = max(iteration_delta, delta)
                
                # æ›´æ–°ä¼˜å…ˆçº§ï¼ˆå¦‚æœä½¿ç”¨ï¼‰
                if self.update_mode == 'prioritized':
                    self._update_priority(state, delta)
                
                total_updates += 1
            
            max_delta_history.append(iteration_delta)
            
            # å®šæœŸæ£€æŸ¥æ”¶æ•›
            if iteration % 100 == 0:
                # è®¡ç®—æ‰€æœ‰çŠ¶æ€çš„æœ€å¤§å˜åŒ–
                global_delta = self._compute_global_delta(V)
                
                if verbose and iteration % 1000 == 0:
                    print(f"è¿­ä»£ {iteration}: "
                          f"å±€éƒ¨Î” = {iteration_delta:.2e}, "
                          f"å…¨å±€Î” = {global_delta:.2e}")
                
                # æ£€æŸ¥æ”¶æ•›
                if global_delta < theta:
                    if verbose:
                        print(f"\nâœ“ å¼‚æ­¥ä»·å€¼è¿­ä»£æ”¶æ•›ï¼")
                        print(f"  æ€»æ›´æ–°æ¬¡æ•°: {total_updates}")
                        print(f"  è¿­ä»£æ¬¡æ•°: {iteration + 1}")
                    break
        
        self.total_time = time.time() - start_time
        self.total_iterations = iteration + 1
        
        # æå–ç­–ç•¥
        optimal_policy = self._extract_policy(V)
        
        if verbose:
            print(f"\næ€»è¿è¡Œæ—¶é—´: {self.total_time:.3f}ç§’")
            print(f"æ€»çŠ¶æ€æ›´æ–°: {total_updates}")
            print(f"å¹³å‡æ¯ç§’æ›´æ–°: {total_updates/self.total_time:.0f}")
        
        return optimal_policy, V
    
    def _select_states_to_update(self, V: StateValueFunction, 
                                 n: int) -> List[State]:
        """
        é€‰æ‹©è¦æ›´æ–°çš„çŠ¶æ€
        Select states to update
        
        æ ¹æ®update_modeé€‰æ‹©ä¸åŒç­–ç•¥
        Different strategies based on update_mode
        """
        non_terminal_states = [s for s in self.env.state_space 
                              if not s.is_terminal]
        
        if self.update_mode == 'sequential':
            # å¾ªç¯é¡ºåºé€‰æ‹©
            if not hasattr(self, '_sequential_index'):
                self._sequential_index = 0
            
            states = []
            for _ in range(min(n, len(non_terminal_states))):
                states.append(non_terminal_states[self._sequential_index])
                self._sequential_index = (self._sequential_index + 1) % len(non_terminal_states)
            return states
            
        elif self.update_mode == 'random':
            # éšæœºé€‰æ‹©
            n = min(n, len(non_terminal_states))
            return np.random.choice(non_terminal_states, n, replace=False).tolist()
            
        elif self.update_mode == 'prioritized':
            # ä¼˜å…ˆçº§é€‰æ‹©
            return self._select_by_priority(n)
        
        else:
            raise ValueError(f"æœªçŸ¥çš„æ›´æ–°æ¨¡å¼: {self.update_mode}")
    
    def _initialize_priorities(self, V: StateValueFunction):
        """
        åˆå§‹åŒ–ä¼˜å…ˆçº§é˜Ÿåˆ—
        Initialize priority queue
        """
        self.priorities = {}
        for state in self.env.state_space:
            if not state.is_terminal:
                # åˆå§‹ä¼˜å…ˆçº§è®¾ä¸ºæ— ç©·å¤§ï¼ˆç¡®ä¿æ‰€æœ‰çŠ¶æ€è‡³å°‘æ›´æ–°ä¸€æ¬¡ï¼‰
                self.priorities[state] = float('inf')
    
    def _update_priority(self, state: State, delta: float):
        """
        æ›´æ–°çŠ¶æ€ä¼˜å…ˆçº§
        Update state priority
        """
        if self.update_mode == 'prioritized':
            self.priorities[state] = delta
    
    def _select_by_priority(self, n: int) -> List[State]:
        """
        æ ¹æ®ä¼˜å…ˆçº§é€‰æ‹©çŠ¶æ€
        Select states by priority
        """
        if not self.priorities:
            return []
        
        # é€‰æ‹©ä¼˜å…ˆçº§æœ€é«˜çš„nä¸ªçŠ¶æ€
        sorted_states = sorted(self.priorities.items(), 
                             key=lambda x: x[1], 
                             reverse=True)
        return [state for state, _ in sorted_states[:n]]
    
    def _compute_global_delta(self, V: StateValueFunction) -> float:
        """
        è®¡ç®—å…¨å±€æœ€å¤§å˜åŒ–
        Compute global maximum change
        
        ç”¨äºåˆ¤æ–­çœŸæ­£çš„æ”¶æ•›
        For determining true convergence
        """
        max_delta = 0.0
        
        for state in self.env.state_space:
            if state.is_terminal:
                continue
            
            old_value = V.get_value(state)
            
            # è®¡ç®—åº”è¯¥çš„æ–°å€¼
            max_q_value = float('-inf')
            for action in self.env.action_space:
                q_value = self.bellman_op._compute_q_value(state, action, V)
                max_q_value = max(max_q_value, q_value)
            
            delta = abs(old_value - max_q_value)
            max_delta = max(max_delta, delta)
        
        return max_delta


# ================================================================================
# ç¬¬3.3.3èŠ‚ï¼šä»·å€¼è¿­ä»£å¯è§†åŒ–
# Section 3.3.3: Value Iteration Visualization
# ================================================================================

class ValueIterationVisualizer:
    """
    ä»·å€¼è¿­ä»£å¯è§†åŒ–å™¨
    Value Iteration Visualizer
    
    å±•ç¤ºä»·å€¼è¿­ä»£çš„æ”¶æ•›è¿‡ç¨‹å’Œç‰¹æ€§
    Show convergence process and properties of value iteration
    """
    
    @staticmethod
    def visualize_convergence(vi: ValueIteration):
        """
        å¯è§†åŒ–æ”¶æ•›è¿‡ç¨‹
        Visualize Convergence Process
        
        å±•ç¤ºä»·å€¼è¿­ä»£å¦‚ä½•é€æ­¥æ”¶æ•›åˆ°æœ€ä¼˜
        Show how value iteration converges to optimal
        """
        if not vi.convergence_history:
            logger.warning("æ²¡æœ‰æ”¶æ•›å†å²å¯è§†åŒ–")
            return None
        
        fig, axes = plt.subplots(2, 2, figsize=(14, 10))
        
        # ========== å›¾1ï¼šæ”¶æ•›æ›²çº¿ï¼ˆå¯¹æ•°å°ºåº¦ï¼‰ ==========
        ax1 = axes[0, 0]
        iterations = range(1, len(vi.convergence_history) + 1)
        ax1.semilogy(iterations, vi.convergence_history, 'b-', linewidth=2)
        ax1.set_xlabel('Iteration / è¿­ä»£')
        ax1.set_ylabel('Max Change Î” (log scale) / æœ€å¤§å˜åŒ–ï¼ˆå¯¹æ•°å°ºåº¦ï¼‰')
        ax1.set_title('Convergence Rate / æ”¶æ•›é€Ÿåº¦')
        ax1.grid(True, alpha=0.3)
        
        # æ·»åŠ ç†è®ºç•Œé™
        if len(vi.convergence_history) > 1:
            initial_delta = vi.convergence_history[0]
            theoretical_bound = [initial_delta * (vi.gamma ** i) 
                               for i in range(len(vi.convergence_history))]
            ax1.semilogy(iterations, theoretical_bound, 'r--', 
                        alpha=0.5, label=f'Î³^k bound (Î³={vi.gamma})')
            ax1.legend()
        
        # æ ‡è®°æ”¶æ•›ç‚¹
        ax1.axhline(y=1e-6, color='g', linestyle='--', alpha=0.5, label='Î¸=1e-6')
        
        # ========== å›¾2ï¼šæ”¶ç¼©ç‡åˆ†æ ==========
        ax2 = axes[0, 1]
        if len(vi.convergence_history) > 1:
            # è®¡ç®—ç›¸é‚»è¿­ä»£çš„æ¯”ç‡
            ratios = []
            for i in range(1, len(vi.convergence_history)):
                if vi.convergence_history[i-1] > 0:
                    ratio = vi.convergence_history[i] / vi.convergence_history[i-1]
                    ratios.append(ratio)
            
            if ratios:
                ax2.plot(range(2, len(vi.convergence_history) + 1), ratios, 
                        'o-', markersize=4, alpha=0.7)
                ax2.axhline(y=vi.gamma, color='r', linestyle='--', 
                           label=f'Î³ = {vi.gamma}')
                ax2.set_xlabel('Iteration / è¿­ä»£')
                ax2.set_ylabel('Contraction Ratio / æ”¶ç¼©æ¯”ç‡')
                ax2.set_title('Actual vs Theoretical Contraction / å®é™…vsç†è®ºæ”¶ç¼©')
                ax2.legend()
                ax2.grid(True, alpha=0.3)
                ax2.set_ylim([0, 1])
        
        # ========== å›¾3ï¼šä»·å€¼å‡½æ•°æ¼”åŒ– ==========
        ax3 = axes[1, 0]
        
        # é€‰æ‹©å‡ ä¸ªçŠ¶æ€å±•ç¤º
        n_states_to_show = min(5, len(vi.env.state_space))
        state_indices = np.linspace(0, len(vi.env.state_space)-1, 
                                   n_states_to_show, dtype=int)
        
        evolution = vi.get_value_evolution(state_indices)
        
        if evolution.size > 0:
            for i, idx in enumerate(state_indices):
                state = vi.env.state_space[idx]
                ax3.plot(evolution[:, i], label=f'State {state.id}', alpha=0.7)
            
            ax3.set_xlabel('Iteration / è¿­ä»£')
            ax3.set_ylabel('State Value / çŠ¶æ€ä»·å€¼')
            ax3.set_title('Value Function Evolution / ä»·å€¼å‡½æ•°æ¼”åŒ–')
            ax3.legend()
            ax3.grid(True, alpha=0.3)
        
        # ========== å›¾4ï¼šç®—æ³•æ¯”è¾ƒ ==========
        ax4 = axes[1, 1]
        
        # åˆ›å»ºæ¯”è¾ƒè¡¨æ ¼
        comparison_data = {
            'Property': ['æ”¶æ•›é€Ÿåº¦\nConvergence', 'æ¯æ­¥è®¡ç®—\nPer Step', 
                        'å†…å­˜éœ€æ±‚\nMemory', 'å®ç°éš¾åº¦\nImplementation'],
            'Value Iteration': ['æ…¢ Slow\nO(Î³^k)', 'ä½ Low\nO(|S||A|)', 
                              'ä½ Low\nO(|S|)', 'ç®€å• Simple'],
            'Policy Iteration': ['å¿« Fast\n<10 iterations', 'é«˜ High\nO(|S|Â²|A|Ã—I)', 
                               'é«˜ High\nO(|S|+|A|)', 'å¤æ‚ Complex']
        }
        
        # æ¸…ç©ºåæ ‡è½´
        ax4.axis('tight')
        ax4.axis('off')
        
        # åˆ›å»ºè¡¨æ ¼
        table = ax4.table(cellText=[[comparison_data[col][i] 
                                    for col in comparison_data.keys()] 
                                   for i in range(len(comparison_data['Property']))],
                         colLabels=list(comparison_data.keys()),
                         cellLoc='center',
                         loc='center',
                         colWidths=[0.3, 0.35, 0.35])
        
        table.auto_set_font_size(False)
        table.set_fontsize(9)
        table.scale(1, 2)
        
        # è®¾ç½®è¡¨æ ¼æ ·å¼
        for (i, j), cell in table.get_celld().items():
            if i == 0:  # æ ‡é¢˜è¡Œ
                cell.set_facecolor('#40466e')
                cell.set_text_props(weight='bold', color='white')
            else:
                cell.set_facecolor('#f0f0f0' if j % 2 == 0 else '#ffffff')
        
        ax4.set_title('Algorithm Comparison / ç®—æ³•æ¯”è¾ƒ', pad=20)
        
        plt.suptitle('Value Iteration Analysis / ä»·å€¼è¿­ä»£åˆ†æ', 
                    fontsize=14, fontweight='bold')
        plt.tight_layout()
        
        return fig
    
    @staticmethod
    def create_animation(vi: ValueIteration, grid_env=None):
        """
        åˆ›å»ºä»·å€¼è¿­ä»£åŠ¨ç”»
        Create Value Iteration Animation
        
        åŠ¨æ€å±•ç¤ºä»·å€¼å‡½æ•°å¦‚ä½•ä¼ æ’­
        Dynamically show how value function propagates
        """
        if not vi.iteration_history or grid_env is None:
            logger.warning("éœ€è¦è¿­ä»£å†å²å’Œç½‘æ ¼ç¯å¢ƒåˆ›å»ºåŠ¨ç”»")
            return None
        
        fig, ax = plt.subplots(figsize=(8, 8))
        
        def animate(frame):
            ax.clear()
            
            # è·å–å½“å‰è¿­ä»£çš„ä»·å€¼å‡½æ•°
            if frame < len(vi.iteration_history):
                V = vi.iteration_history[frame]['value_function']
                iteration = vi.iteration_history[frame]['iteration']
                delta = vi.iteration_history[frame]['delta']
            else:
                return
            
            # ç»˜åˆ¶ç½‘æ ¼å’Œä»·å€¼
            ValueIterationVisualizer._draw_grid_values(ax, grid_env, V)
            
            ax.set_title(f'Value Iteration - Iteration {iteration}\n'
                        f'Î” = {delta:.2e}', fontsize=12)
        
        anim = animation.FuncAnimation(
            fig, animate,
            frames=len(vi.iteration_history),
            interval=200,  # æ¯å¸§200ms
            repeat=True
        )
        
        plt.close()  # é˜²æ­¢æ˜¾ç¤ºé™æ€å›¾
        return anim
    
    @staticmethod
    def _draw_grid_values(ax, grid_env, V: StateValueFunction):
        """
        åœ¨ç½‘æ ¼ä¸Šç»˜åˆ¶ä»·å€¼å‡½æ•°
        Draw value function on grid
        
        ä½¿ç”¨çƒ­åŠ›å›¾å±•ç¤ºä»·å€¼åˆ†å¸ƒ
        Use heatmap to show value distribution
        """
        rows, cols = grid_env.rows, grid_env.cols
        
        # åˆ›å»ºä»·å€¼çŸ©é˜µ
        value_matrix = np.zeros((rows, cols))
        
        for i in range(rows):
            for j in range(cols):
                pos = (i, j)
                if pos in grid_env.pos_to_state:
                    state = grid_env.pos_to_state[pos]
                    value_matrix[i, j] = V.get_value(state)
                elif pos in grid_env.obstacles:
                    value_matrix[i, j] = np.nan  # éšœç¢ç‰©
        
        # ç»˜åˆ¶çƒ­åŠ›å›¾
        im = ax.imshow(value_matrix, cmap='coolwarm', aspect='equal')
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for i in range(rows):
            for j in range(cols):
                pos = (i, j)
                if pos in grid_env.pos_to_state:
                    state = grid_env.pos_to_state[pos]
                    value = V.get_value(state)
                    
                    # æ ¹æ®å€¼çš„å¤§å°è°ƒæ•´æ–‡æœ¬é¢œè‰²
                    text_color = 'white' if value < np.nanmean(value_matrix) else 'black'
                    ax.text(j, i, f'{value:.1f}', ha='center', va='center',
                           color=text_color, fontweight='bold')
                
                # æ ‡è®°ç‰¹æ®Šä½ç½®
                if pos == grid_env.start_pos:
                    ax.add_patch(Rectangle((j-0.45, i-0.45), 0.9, 0.9,
                                         fill=False, edgecolor='green', linewidth=3))
                    ax.text(j, i-0.35, 'S', ha='center', va='center',
                           color='green', fontweight='bold', fontsize=12)
                elif pos == grid_env.goal_pos:
                    ax.add_patch(Rectangle((j-0.45, i-0.45), 0.9, 0.9,
                                         fill=False, edgecolor='red', linewidth=3))
                    ax.text(j, i-0.35, 'G', ha='center', va='center',
                           color='red', fontweight='bold', fontsize=12)
                elif pos in grid_env.obstacles:
                    ax.add_patch(Rectangle((j-0.5, i-0.5), 1, 1,
                                         facecolor='gray', alpha=0.8))
        
        # è®¾ç½®åæ ‡è½´
        ax.set_xlim(-0.5, cols - 0.5)
        ax.set_ylim(rows - 0.5, -0.5)
        ax.set_xticks(range(cols))
        ax.set_yticks(range(rows))
        ax.grid(True, color='black', linewidth=0.5)
        
        # æ·»åŠ é¢œè‰²æ¡
        plt.colorbar(im, ax=ax, label='State Value')


# ================================================================================
# ç¬¬3.3.4èŠ‚ï¼šä»·å€¼è¿­ä»£åˆ†æ
# Section 3.3.4: Value Iteration Analysis
# ================================================================================

class ValueIterationAnalysis:
    """
    ä»·å€¼è¿­ä»£ç†è®ºä¸å®éªŒåˆ†æ
    Value Iteration Theoretical and Experimental Analysis
    """
    
    @staticmethod
    def theoretical_analysis():
        """
        ç†è®ºåˆ†æ
        Theoretical Analysis
        """
        print("\n" + "="*80)
        print("ä»·å€¼è¿­ä»£ç†è®ºåˆ†æ")
        print("Value Iteration Theoretical Analysis")
        print("="*80)
        
        print("""
        ğŸ“š 1. æ”¶æ•›æ€§è¯æ˜
        Convergence Proof
        ================================
        
        è´å°”æ›¼æœ€ä¼˜ç®—å­T*çš„æ€§è´¨ï¼š
        Properties of Bellman optimality operator T*:
        
        (1) å•è°ƒæ€§ Monotonicity:
            v â‰¤ w âŸ¹ T*v â‰¤ T*w
        
        (2) æ”¶ç¼©æ€§ Contraction:
            ||T*v - T*w||âˆ â‰¤ Î³||v - w||âˆ
        
        ç”±Banachä¸åŠ¨ç‚¹å®šç†ï¼š
        By Banach fixed-point theorem:
        - T*æœ‰å”¯ä¸€ä¸åŠ¨ç‚¹v*
          T* has unique fixed point v*
        - ä»ä»»æ„v_0å¼€å§‹ï¼Œv_k â†’ v* as k â†’ âˆ
          Starting from any v_0, v_k â†’ v* as k â†’ âˆ
        - æ”¶æ•›é€Ÿåº¦ï¼š||v_k - v*|| â‰¤ Î³^k ||v_0 - v*||
          Convergence rate: ||v_k - v*|| â‰¤ Î³^k ||v_0 - v*||
        
        ğŸ“š 2. è¯¯å·®ç•Œ
        Error Bounds
        ================================
        
        kæ­¥åçš„è¯¯å·®ä¸Šç•Œï¼š
        Error bound after k steps:
        
        ||v_k - v*||âˆ â‰¤ Î³^k/(1-Î³) Â· ||v_1 - v_0||âˆ
        
        è¿™å‘Šè¯‰æˆ‘ä»¬ï¼š
        This tells us:
        - Î³è¶Šå°ï¼Œæ”¶æ•›è¶Šå¿«
          Smaller Î³, faster convergence
        - åˆå§‹å€¼çš„é€‰æ‹©å½±å“æœ‰é™
          Initial value choice has limited impact
        - å¯ä»¥é¢„ä¼°éœ€è¦çš„è¿­ä»£æ¬¡æ•°
          Can estimate required iterations
        
        è¦è¾¾åˆ°Îµç²¾åº¦ï¼Œéœ€è¦è¿­ä»£æ¬¡æ•°ï¼š
        To reach Îµ accuracy, need iterations:
        k â‰¥ log(Îµ(1-Î³)/||v_1-v_0||) / log(Î³)
        
        ğŸ“š 3. vs ç­–ç•¥è¿­ä»£
        vs Policy Iteration
        ================================
        
        ä»·å€¼è¿­ä»£ = ä¿®æ”¹çš„ç­–ç•¥è¿­ä»£(m=1)
        Value Iteration = Modified Policy Iteration (m=1)
        
        | æ–¹é¢ Aspect | ä»·å€¼è¿­ä»£ VI | ç­–ç•¥è¿­ä»£ PI |
        |------------|------------|-------------|
        | è¿­ä»£æ¬¡æ•°    | å¤š Many     | å°‘ Few      |
        | æ¯æ­¥è®¡ç®—    | å°‘ Less     | å¤š More     |
        | å†…å­˜éœ€æ±‚    | å° Small    | å¤§ Large    |
        | ä¸­é—´ç­–ç•¥    | æ—  None     | æœ‰ Yes      |
        | é€‚ç”¨åœºæ™¯    | Î³å° Small Î³ | Î³å¤§ Large Î³ |
        
        ğŸ“š 4. å®è·µæŠ€å·§
        Practical Tips
        ================================
        
        åŠ é€Ÿæ”¶æ•›ï¼š
        Speed up convergence:
        
        1. å¥½çš„åˆå§‹å€¼ï¼š
           Good initial values:
           - ä½¿ç”¨å¯å‘å¼ï¼ˆå¦‚æœ€çŸ­è·¯å¾„ï¼‰
             Use heuristics (e.g., shortest path)
           - ä»ç›¸ä¼¼é—®é¢˜çš„è§£å¼€å§‹
             Start from similar problem's solution
        
        2. å¼‚æ­¥æ›´æ–°ï¼š
           Asynchronous updates:
           - Gauss-Seidelæ¯”Jacobiå¿«
             Gauss-Seidel faster than Jacobi
           - ä¼˜å…ˆçº§æ‰«ææ›´é«˜æ•ˆ
             Prioritized sweeping more efficient
        
        3. æ—©åœï¼š
           Early stopping:
           - ä¸éœ€è¦å®Œå…¨æ”¶æ•›å°±èƒ½å¾—åˆ°å¥½ç­–ç•¥
             Don't need full convergence for good policy
           - Îµ-æœ€ä¼˜ç­–ç•¥å¯èƒ½å°±å¤Ÿäº†
             Îµ-optimal policy may be enough
        """)
    
    @staticmethod
    def compare_sync_async(env, n_runs: int = 5):
        """
        æ¯”è¾ƒåŒæ­¥å’Œå¼‚æ­¥ä»·å€¼è¿­ä»£
        Compare Synchronous and Asynchronous Value Iteration
        
        å®éªŒå±•ç¤ºä¸¤ç§æ–¹æ³•çš„æ€§èƒ½å·®å¼‚
        Experiment shows performance difference between two methods
        """
        print("\n" + "="*80)
        print("åŒæ­¥ vs å¼‚æ­¥ä»·å€¼è¿­ä»£")
        print("Synchronous vs Asynchronous Value Iteration")
        print("="*80)
        
        results = {
            'Synchronous': [],
            'Async-Random': [],
            'Async-Sequential': []
        }
        
        for run in range(n_runs):
            print(f"\nè¿è¡Œ {run + 1}/{n_runs}")
            
            # åŒæ­¥ç‰ˆæœ¬
            vi_sync = ValueIteration(env, gamma=0.9)
            _, _ = vi_sync.solve(theta=1e-4, verbose=False)
            results['Synchronous'].append({
                'iterations': vi_sync.total_iterations,
                'time': vi_sync.total_time,
                'updates': vi_sync.total_iterations * len(env.state_space)
            })
            
            # å¼‚æ­¥éšæœºç‰ˆæœ¬
            vi_async_random = AsynchronousValueIteration(env, gamma=0.9, 
                                                        update_mode='random')
            _, _ = vi_async_random.solve(theta=1e-4, verbose=False)
            results['Async-Random'].append({
                'iterations': vi_async_random.total_iterations,
                'time': vi_async_random.total_time,
                'updates': vi_async_random.total_iterations
            })
            
            # å¼‚æ­¥é¡ºåºç‰ˆæœ¬
            vi_async_seq = AsynchronousValueIteration(env, gamma=0.9,
                                                     update_mode='sequential')
            _, _ = vi_async_seq.solve(theta=1e-4, verbose=False)
            results['Async-Sequential'].append({
                'iterations': vi_async_seq.total_iterations,
                'time': vi_async_seq.total_time,
                'updates': vi_async_seq.total_iterations
            })
        
        # ç»Ÿè®¡å’Œå¯è§†åŒ–
        fig, axes = plt.subplots(1, 3, figsize=(15, 5))
        
        methods = list(results.keys())
        colors = ['steelblue', 'lightcoral', 'lightgreen']
        
        # å›¾1ï¼šè¿­ä»£æ¬¡æ•°
        ax1 = axes[0]
        avg_iterations = [np.mean([r['iterations'] for r in results[m]]) 
                         for m in methods]
        std_iterations = [np.std([r['iterations'] for r in results[m]]) 
                         for m in methods]
        
        bars1 = ax1.bar(methods, avg_iterations, yerr=std_iterations,
                       color=colors, alpha=0.7, capsize=5)
        ax1.set_ylabel('Iterations')
        ax1.set_title('Iterations to Converge')
        ax1.grid(True, alpha=0.3, axis='y')
        
        # æ·»åŠ æ•°å€¼
        for bar, val, std in zip(bars1, avg_iterations, std_iterations):
            ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height(),
                    f'{val:.0f}Â±{std:.0f}', ha='center', va='bottom', fontsize=9)
        
        # å›¾2ï¼šè¿è¡Œæ—¶é—´
        ax2 = axes[1]
        avg_times = [np.mean([r['time'] for r in results[m]]) for m in methods]
        std_times = [np.std([r['time'] for r in results[m]]) for m in methods]
        
        bars2 = ax2.bar(methods, avg_times, yerr=std_times,
                       color=colors, alpha=0.7, capsize=5)
        ax2.set_ylabel('Time (seconds)')
        ax2.set_title('Runtime')
        ax2.grid(True, alpha=0.3, axis='y')
        
        for bar, val, std in zip(bars2, avg_times, std_times):
            ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height(),
                    f'{val:.3f}Â±{std:.3f}', ha='center', va='bottom', fontsize=9)
        
        # å›¾3ï¼šæ€»æ›´æ–°æ¬¡æ•°
        ax3 = axes[2]
        avg_updates = [np.mean([r['updates'] for r in results[m]]) for m in methods]
        
        bars3 = ax3.bar(methods, avg_updates, color=colors, alpha=0.7)
        ax3.set_ylabel('Total State Updates')
        ax3.set_title('Total Updates')
        ax3.grid(True, alpha=0.3, axis='y')
        
        for bar, val in zip(bars3, avg_updates):
            ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height(),
                    f'{val:.0f}', ha='center', va='bottom')
        
        plt.suptitle('Synchronous vs Asynchronous Comparison', fontweight='bold')
        plt.tight_layout()
        
        # æ‰“å°æ€»ç»“
        print("\nå®éªŒæ€»ç»“ Experiment Summary:")
        print("-" * 40)
        for method in methods:
            print(f"\n{method}:")
            print(f"  å¹³å‡è¿­ä»£: {np.mean([r['iterations'] for r in results[method]]):.1f}")
            print(f"  å¹³å‡æ—¶é—´: {np.mean([r['time'] for r in results[method]]):.3f}s")
            print(f"  å¹³å‡æ›´æ–°: {np.mean([r['updates'] for r in results[method]]):.0f}")
        
        return fig


# ================================================================================
# ä¸»å‡½æ•°ï¼šæ¼”ç¤ºä»·å€¼è¿­ä»£
# Main Function: Demonstrate Value Iteration
# ================================================================================

def main():
    """
    è¿è¡Œä»·å€¼è¿­ä»£å®Œæ•´æ¼”ç¤º
    Run Complete Value Iteration Demo
    """
    print("\n" + "="*80)
    print("ç¬¬3.3èŠ‚ï¼šä»·å€¼è¿­ä»£")
    print("Section 3.3: Value Iteration")
    print("="*80)
    
    # åˆ›å»ºæµ‹è¯•ç¯å¢ƒ
    from src.ch03_finite_mdp.gridworld import GridWorld
    
    # åˆ›å»º4x4ç½‘æ ¼ä¸–ç•Œ
    env = GridWorld(
        rows=4,
        cols=4,
        start_pos=(0, 0),
        goal_pos=(3, 3),
        obstacles={(1, 1), (2, 2)}
    )
    
    print(f"\nåˆ›å»º {env.rows}Ã—{env.cols} ç½‘æ ¼ä¸–ç•Œ")
    print(f"Create {env.rows}Ã—{env.cols} Grid World")
    
    # 1. è¿è¡Œæ ‡å‡†ä»·å€¼è¿­ä»£
    print("\n" + "="*60)
    print("1. æ ‡å‡†ï¼ˆåŒæ­¥ï¼‰ä»·å€¼è¿­ä»£")
    print("1. Standard (Synchronous) Value Iteration")
    print("="*60)
    
    vi = ValueIteration(env, gamma=0.9)
    optimal_policy, optimal_V = vi.solve(theta=1e-6, verbose=True)
    
    # 2. å¯è§†åŒ–æ”¶æ•›è¿‡ç¨‹
    print("\n2. å¯è§†åŒ–æ”¶æ•›è¿‡ç¨‹")
    print("2. Visualize Convergence Process")
    visualizer = ValueIterationVisualizer()
    fig1 = visualizer.visualize_convergence(vi)
    
    # 3. åˆ›å»ºåŠ¨ç”»ï¼ˆå¦‚æœå¯èƒ½ï¼‰
    print("\n3. åˆ›å»ºä»·å€¼ä¼ æ’­åŠ¨ç”»")
    print("3. Create Value Propagation Animation")
    anim = visualizer.create_animation(vi, env)
    if anim:
        print("åŠ¨ç”»åˆ›å»ºæˆåŠŸï¼ˆåœ¨Jupyterä¸­å¯ä»¥æ’­æ”¾ï¼‰")
        print("Animation created successfully (can play in Jupyter)")
    
    # 4. ç†è®ºåˆ†æ
    ValueIterationAnalysis.theoretical_analysis()
    
    # 5. æ¯”è¾ƒåŒæ­¥å’Œå¼‚æ­¥ç‰ˆæœ¬
    print("\n5. æ¯”è¾ƒåŒæ­¥å’Œå¼‚æ­¥ç‰ˆæœ¬")
    print("5. Compare Synchronous and Asynchronous Versions")
    fig2 = ValueIterationAnalysis.compare_sync_async(env, n_runs=3)
    
    # 6. å±•ç¤ºæœ€ä¼˜ç­–ç•¥
    print("\n" + "="*60)
    print("æœ€ä¼˜ç­–ç•¥å’Œä»·å€¼")
    print("Optimal Policy and Values")
    print("="*60)
    
    # æ˜¾ç¤ºå…³é”®ä½ç½®çš„ä»·å€¼å’ŒåŠ¨ä½œ
    key_positions = [
        (0, 0),  # èµ·ç‚¹
        (0, 2),  # å³ä¸Š
        (2, 0),  # å·¦ä¸‹
        (2, 3),  # ç›®æ ‡é™„è¿‘
        (3, 2)   # ç›®æ ‡é™„è¿‘
    ]
    
    print("\nå…³é”®ä½ç½®çš„æœ€ä¼˜å†³ç­–:")
    print("Optimal decisions at key positions:")
    for pos in key_positions:
        if pos in env.pos_to_state:
            state = env.pos_to_state[pos]
            value = optimal_V.get_value(state)
            
            if isinstance(optimal_policy, DeterministicPolicy) and state in optimal_policy.policy_map:
                action = optimal_policy.policy_map[state]
                print(f"  ä½ç½® {pos}: {action.id} (V={value:.2f})")
    
    # æ¯”è¾ƒæ”¶æ•›é€Ÿåº¦
    print(f"\næ”¶æ•›ç»Ÿè®¡ Convergence Statistics:")
    print(f"  ä»·å€¼è¿­ä»£è¿­ä»£æ¬¡æ•°: {vi.total_iterations}")
    print(f"  æ€»çŠ¶æ€æ›´æ–°æ¬¡æ•°: {vi.total_iterations * len(env.state_space)}")
    print(f"  å¹³å‡æ”¶ç¼©ç‡: {np.mean(vi.convergence_history[i+1]/vi.convergence_history[i] for i in range(len(vi.convergence_history)-1) if vi.convergence_history[i] > 0):.3f}")
    
    print("\n" + "="*80)
    print("ä»·å€¼è¿­ä»£æ¼”ç¤ºå®Œæˆï¼")
    print("Value Iteration Demo Complete!")
    print("\nå…³é”®è¦ç‚¹ Key Takeaways:")
    print("1. ä»·å€¼è¿­ä»£ç›´æ¥å¯»æ‰¾æœ€ä¼˜ä»·å€¼å‡½æ•°")
    print("   Value iteration directly finds optimal value function")
    print("2. æ¯æ¬¡è¿­ä»£åº”ç”¨è´å°”æ›¼æœ€ä¼˜ç®—å­")
    print("   Each iteration applies Bellman optimality operator")
    print("3. æ”¶æ•›é€Ÿåº¦å–å†³äºÎ³ï¼ˆæŒ‡æ•°æ”¶æ•›ï¼‰")
    print("   Convergence speed depends on Î³ (exponential)")
    print("4. å¼‚æ­¥ç‰ˆæœ¬å¯èƒ½æ›´é«˜æ•ˆ")
    print("   Asynchronous version may be more efficient")
    print("5. é€‚åˆéœ€è¦è¿‘ä¼¼è§£çš„åœºæ™¯ï¼ˆå¯éšæ—¶åœæ­¢ï¼‰")
    print("   Good for scenarios needing approximation (can stop anytime)")
    print("="*80)
    
    plt.show()
    
    return optimal_policy, optimal_V


if __name__ == "__main__":
    main()