"""
================================================================================
ç¬¬4.2èŠ‚ï¼šç­–ç•¥è¿­ä»£ - é€šè¿‡è¯„ä¼°å’Œæ”¹è¿›æ‰¾åˆ°æœ€ä¼˜ç­–ç•¥
Section 4.2: Policy Iteration - Finding Optimal Policy through Evaluation and Improvement
================================================================================

ç­–ç•¥è¿­ä»£æ˜¯åŠ¨æ€è§„åˆ’çš„ç»å…¸ç®—æ³•ä¹‹ä¸€ï¼Œå®ƒé€šè¿‡ä¸æ–­äº¤æ›¿è¿›è¡Œç­–ç•¥è¯„ä¼°å’Œç­–ç•¥æ”¹è¿›æ¥æ‰¾åˆ°æœ€ä¼˜ç­–ç•¥ã€‚
Policy Iteration is one of the classic DP algorithms, finding optimal policy by alternating 
between policy evaluation and policy improvement.

ç®—æ³•æµç¨‹å°±åƒçˆ¬å±±ï¼š
The algorithm is like hill climbing:
1. è¯„ä¼°å½“å‰ä½ç½®çš„é«˜åº¦ï¼ˆç­–ç•¥è¯„ä¼°ï¼‰
   Evaluate current position height (policy evaluation)
2. æ‰¾åˆ°æ›´é«˜çš„æ–¹å‘ï¼ˆç­–ç•¥æ”¹è¿›ï¼‰
   Find higher direction (policy improvement)
3. ç§»åŠ¨åˆ°æ–°ä½ç½®ï¼ˆæ›´æ–°ç­–ç•¥ï¼‰
   Move to new position (update policy)
4. é‡å¤ç›´åˆ°åˆ°è¾¾å±±é¡¶ï¼ˆæœ€ä¼˜ç­–ç•¥ï¼‰
   Repeat until reach peak (optimal policy)

ä¸ºä»€ä¹ˆè¿™ä¸ªæ–¹æ³•æœ‰æ•ˆï¼Ÿ
Why does this work?
- ç­–ç•¥è¯„ä¼°ç»™å‡ºå‡†ç¡®çš„v_Ï€
  Policy evaluation gives exact v_Ï€
- ç­–ç•¥æ”¹è¿›ä¿è¯æ–°ç­–ç•¥ä¸æ›´å·®
  Policy improvement guarantees new policy not worse
- æœ‰é™MDPåªæœ‰æœ‰é™ä¸ªç­–ç•¥ï¼Œå¿…ç„¶æ”¶æ•›
  Finite MDP has finite policies, must converge
"""

import numpy as np
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass, field
import logging
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from matplotlib.patches import FancyBboxPatch
import seaborn as sns
from collections import defaultdict
import time

# å¯¼å…¥åŸºç¡€ç»„ä»¶
import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent))

from src.ch03_finite_mdp.mdp_framework import (
    State, Action, MDPEnvironment
)
from src.ch03_finite_mdp.policies_and_values import (
    Policy, StateValueFunction, ActionValueFunction,
    DeterministicPolicy, StochasticPolicy
)
from .dp_foundations import (
    PolicyEvaluationDP, PolicyImprovementDP,
    BellmanOperator
)

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


# ================================================================================
# ç¬¬4.2.1èŠ‚ï¼šç­–ç•¥è¿­ä»£ç®—æ³•
# Section 4.2.1: Policy Iteration Algorithm
# ================================================================================

class PolicyIteration:
    """
    ç­–ç•¥è¿­ä»£ç®—æ³•
    Policy Iteration Algorithm
    
    è¿™æ˜¯æ±‚è§£MDPçš„ç¬¬ä¸€ä¸ªå®Œæ•´ç®—æ³•ï¼
    This is the first complete algorithm for solving MDPs!
    
    ç®—æ³•ä¼ªä»£ç ï¼š
    Algorithm Pseudocode:
    ```
    1. åˆå§‹åŒ–
       Initialization
       å¯¹æ‰€æœ‰sâˆˆSï¼Œä»»æ„åˆå§‹åŒ–V(s)å’ŒÏ€(s)
       For all sâˆˆS, arbitrarily initialize V(s) and Ï€(s)
    
    2. ç­–ç•¥è¯„ä¼°ï¼ˆPolicy Evaluationï¼‰
       é‡å¤
       Repeat
         Î” â† 0
         å¯¹æ¯ä¸ªsâˆˆSï¼š
         For each sâˆˆS:
           v â† V(s)
           V(s) â† Î£_a Ï€(a|s) Î£_{s',r} p(s',r|s,a)[r + Î³V(s')]
           Î” â† max(Î”, |v - V(s)|)
       ç›´åˆ°Î” < Î¸ï¼ˆä¸€ä¸ªå°çš„é˜ˆå€¼ï¼‰
       until Î” < Î¸ (a small threshold)
    
    3. ç­–ç•¥æ”¹è¿›ï¼ˆPolicy Improvementï¼‰
       policy_stable â† true
       å¯¹æ¯ä¸ªsâˆˆSï¼š
       For each sâˆˆS:
         old_action â† Ï€(s)
         Ï€(s) â† argmax_a Î£_{s',r} p(s',r|s,a)[r + Î³V(s')]
         å¦‚æœold_action â‰  Ï€(s)ï¼Œåˆ™policy_stable â† false
         If old_action â‰  Ï€(s), then policy_stable â† false
       
    4. å¦‚æœpolicy_stableï¼Œåˆ™åœæ­¢ï¼›å¦åˆ™å›åˆ°2
       If policy_stable, then stop; else go to 2
    ```
    
    å…³é”®æ€§è´¨ï¼š
    Key Properties:
    1. æœ‰é™æ­¥æ”¶æ•›åˆ°æœ€ä¼˜ç­–ç•¥
       Converges to optimal policy in finite steps
    2. æ¯æ¬¡è¿­ä»£ç­–ç•¥ä¸¥æ ¼æ”¹è¿›ï¼ˆé™¤éå·²æœ€ä¼˜ï¼‰
       Each iteration strictly improves policy (unless optimal)
    3. è®¡ç®—é‡å¤§ä½†ç²¾ç¡®
       Computationally expensive but exact
    """
    
    def __init__(self, mdp_env: MDPEnvironment, gamma: float = 0.99):
        """
        åˆå§‹åŒ–ç­–ç•¥è¿­ä»£
        Initialize Policy Iteration
        
        Args:
            mdp_env: MDPç¯å¢ƒï¼ˆéœ€è¦å®Œæ•´æ¨¡å‹ï¼‰
                    MDP environment (needs complete model)
            gamma: æŠ˜æ‰£å› å­
                  Discount factor
        
        è®¾è®¡è€ƒè™‘ï¼š
        Design Considerations:
        - ä½¿ç”¨ç»„åˆè€Œéç»§æ‰¿ï¼Œä¿æŒæ¨¡å—ç‹¬ç«‹
          Use composition over inheritance, keep modules independent
        - è®°å½•è¯¦ç»†å†å²ç”¨äºåˆ†æå’Œå¯è§†åŒ–
          Record detailed history for analysis and visualization
        """
        self.env = mdp_env
        self.gamma = gamma
        
        # åˆ›å»ºè¯„ä¼°å™¨å’Œæ”¹è¿›å™¨
        self.evaluator = PolicyEvaluationDP(mdp_env, gamma)
        self.improver = PolicyImprovementDP(mdp_env, gamma)
        
        # è®°å½•è¿­ä»£å†å²
        self.iteration_history = []
        
        # æ€§èƒ½ç»Ÿè®¡
        self.total_evaluations = 0
        self.total_improvements = 0
        self.total_time = 0.0
        
        logger.info(f"åˆå§‹åŒ–ç­–ç•¥è¿­ä»£ï¼ŒÎ³={gamma}")
    
    def solve(self, 
             initial_policy: Optional[Policy] = None,
             theta: float = 1e-6,
             max_iterations: int = 100,
             verbose: bool = True) -> Tuple[Policy, StateValueFunction]:
        """
        è¿è¡Œç­–ç•¥è¿­ä»£ç®—æ³•
        Run Policy Iteration Algorithm
        
        è¿™æ˜¯ç®—æ³•çš„ä¸»å…¥å£ï¼Œåè°ƒè¯„ä¼°å’Œæ”¹è¿›çš„å¾ªç¯
        This is the main entry point, coordinating evaluation and improvement loop
        
        Args:
            initial_policy: åˆå§‹ç­–ç•¥ï¼ˆNoneåˆ™ä½¿ç”¨éšæœºç­–ç•¥ï¼‰
                          Initial policy (None for random policy)
            theta: ç­–ç•¥è¯„ä¼°çš„æ”¶æ•›é˜ˆå€¼
                  Convergence threshold for policy evaluation
            max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼ˆé˜²æ­¢æ— é™å¾ªç¯ï¼‰
                          Maximum iterations (prevent infinite loop)
            verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
                    Whether to print detailed info
        
        Returns:
            (æœ€ä¼˜ç­–ç•¥, æœ€ä¼˜ä»·å€¼å‡½æ•°)
            (optimal policy, optimal value function)
        
        ç®—æ³•å¤æ‚åº¦åˆ†æï¼š
        Complexity Analysis:
        - æ¯æ¬¡ç­–ç•¥è¯„ä¼°: O(|S|Â²|A|) Ã— æ”¶æ•›æ‰€éœ€è¿­ä»£æ¬¡æ•°
          Each evaluation: O(|S|Â²|A|) Ã— iterations to converge
        - æ¯æ¬¡ç­–ç•¥æ”¹è¿›: O(|S||A|)
          Each improvement: O(|S||A|)
        - æ€»è¿­ä»£æ¬¡æ•°: é€šå¸¸å¾ˆå°‘ï¼ˆ<10ï¼‰å¯¹äºå°é—®é¢˜
          Total iterations: Usually few (<10) for small problems
        
        æ•™å­¦è¦ç‚¹ï¼š
        Teaching Points:
        1. æ³¨æ„ç­–ç•¥æ˜¯å¦‚ä½•é€æ­¥æ”¹è¿›çš„
           Notice how policy gradually improves
        2. è¯„ä¼°éœ€è¦å¤šæ¬¡è¿­ä»£ï¼Œæ”¹è¿›åªéœ€ä¸€æ¬¡æ‰«æ
           Evaluation needs many iterations, improvement needs one sweep
        3. å½“ç­–ç•¥ä¸å†æ”¹å˜æ—¶ï¼Œæˆ‘ä»¬æ‰¾åˆ°äº†æœ€ä¼˜ç­–ç•¥
           When policy stops changing, we found optimal policy
        """
        # æ¸…ç©ºå†å²è®°å½•
        self.iteration_history = []
        self.total_evaluations = 0
        self.total_improvements = 0
        
        # å¼€å§‹è®¡æ—¶
        start_time = time.time()
        
        # åˆå§‹åŒ–ç­–ç•¥
        if initial_policy is None:
            # ä½¿ç”¨å‡åŒ€éšæœºç­–ç•¥ä½œä¸ºåˆå§‹ç­–ç•¥
            from src.ch03_finite_mdp.policies_and_values import UniformRandomPolicy
            policy = UniformRandomPolicy(self.env.action_space)
            logger.info("ä½¿ç”¨å‡åŒ€éšæœºç­–ç•¥åˆå§‹åŒ–")
        else:
            policy = initial_policy
            logger.info("ä½¿ç”¨æä¾›çš„åˆå§‹ç­–ç•¥")
        
        # åˆå§‹åŒ–ä»·å€¼å‡½æ•°
        V = StateValueFunction(self.env.state_space, initial_value=0.0)
        
        if verbose:
            print("\n" + "="*60)
            print("å¼€å§‹ç­–ç•¥è¿­ä»£")
            print("Starting Policy Iteration")
            print("="*60)
        
        # ä¸»å¾ªç¯ï¼šäº¤æ›¿è¿›è¡Œè¯„ä¼°å’Œæ”¹è¿›
        for iteration in range(max_iterations):
            if verbose:
                print(f"\n--- è¿­ä»£ {iteration + 1} ---")
                print(f"--- Iteration {iteration + 1} ---")
            
            # ============ æ­¥éª¤1ï¼šç­–ç•¥è¯„ä¼° ============
            # Step 1: Policy Evaluation
            if verbose:
                print("æ‰§è¡Œç­–ç•¥è¯„ä¼°...")
                print("Performing policy evaluation...")
            
            eval_start = time.time()
            V = self.evaluator.evaluate(policy, theta=theta)
            eval_time = time.time() - eval_start
            self.total_evaluations += 1
            
            if verbose:
                print(f"  è¯„ä¼°å®Œæˆï¼Œç”¨æ—¶ {eval_time:.3f}ç§’")
                print(f"  Evaluation done in {eval_time:.3f}s")
                
                # æ˜¾ç¤ºä¸€äº›çŠ¶æ€çš„ä»·å€¼
                sample_states = self.env.state_space[:min(3, len(self.env.state_space))]
                for state in sample_states:
                    print(f"    V({state.id}) = {V.get_value(state):.3f}")
            
            # ============ æ­¥éª¤2ï¼šç­–ç•¥æ”¹è¿› ============
            # Step 2: Policy Improvement
            if verbose:
                print("æ‰§è¡Œç­–ç•¥æ”¹è¿›...")
                print("Performing policy improvement...")
            
            improve_start = time.time()
            
            # è®°å½•æ—§ç­–ç•¥ï¼ˆç”¨äºæ¯”è¾ƒï¼‰
            old_policy_actions = {}
            if isinstance(policy, DeterministicPolicy):
                old_policy_actions = policy.policy_map.copy()
            
            # æ”¹è¿›ç­–ç•¥
            new_policy, policy_changed = self.improver.improve(V)
            improve_time = time.time() - improve_start
            self.total_improvements += 1
            
            # æ£€æŸ¥ç­–ç•¥æ˜¯å¦æ”¹å˜
            policy_stable = True
            changes_count = 0
            
            for state in self.env.state_space:
                if state.is_terminal:
                    continue
                
                # æ¯”è¾ƒæ–°æ—§ç­–ç•¥
                if isinstance(new_policy, DeterministicPolicy) and isinstance(policy, DeterministicPolicy):
                    if state in new_policy.policy_map and state in old_policy_actions:
                        if new_policy.policy_map[state] != old_policy_actions[state]:
                            policy_stable = False
                            changes_count += 1
                            
                            if verbose and changes_count <= 3:  # åªæ˜¾ç¤ºå‰3ä¸ªå˜åŒ–
                                old_action = old_policy_actions[state]
                                new_action = new_policy.policy_map[state]
                                print(f"    çŠ¶æ€ {state.id}: {old_action.id} â†’ {new_action.id}")
            
            if verbose:
                print(f"  æ”¹è¿›å®Œæˆï¼Œç”¨æ—¶ {improve_time:.3f}ç§’")
                print(f"  Improvement done in {improve_time:.3f}s")
                print(f"  ç­–ç•¥å˜åŒ–: {changes_count}ä¸ªçŠ¶æ€")
                print(f"  Policy changes: {changes_count} states")
            
            # è®°å½•æœ¬æ¬¡è¿­ä»£
            iteration_data = {
                'iteration': iteration + 1,
                'value_function': V,
                'policy': new_policy,
                'policy_stable': policy_stable,
                'changes_count': changes_count,
                'eval_time': eval_time,
                'improve_time': improve_time
            }
            self.iteration_history.append(iteration_data)
            
            # æ›´æ–°ç­–ç•¥
            policy = new_policy
            
            # ============ æ­¥éª¤3ï¼šæ£€æŸ¥æ”¶æ•› ============
            # Step 3: Check Convergence
            if policy_stable:
                if verbose:
                    print("\n" + "="*60)
                    print(f"âœ“ ç­–ç•¥è¿­ä»£æ”¶æ•›ï¼")
                    print(f"âœ“ Policy Iteration Converged!")
                    print(f"  æ€»è¿­ä»£æ¬¡æ•°: {iteration + 1}")
                    print(f"  Total iterations: {iteration + 1}")
                    print("="*60)
                
                logger.info(f"ç­–ç•¥è¿­ä»£åœ¨ç¬¬{iteration + 1}æ¬¡è¿­ä»£æ”¶æ•›")
                break
        else:
            # è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°
            logger.warning(f"è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•° {max_iterations}")
            if verbose:
                print(f"\nâš  è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•° {max_iterations}ï¼Œå¯èƒ½æœªå®Œå…¨æ”¶æ•›")
                print(f"âš  Reached max iterations {max_iterations}, may not fully converged")
        
        # è®°å½•æ€»æ—¶é—´
        self.total_time = time.time() - start_time
        
        # æ‰“å°æœ€ç»ˆç»Ÿè®¡
        if verbose:
            self._print_statistics()
        
        return policy, V
    
    def _print_statistics(self):
        """
        æ‰“å°ç®—æ³•ç»Ÿè®¡ä¿¡æ¯
        Print Algorithm Statistics
        
        å¸®åŠ©ç†è§£ç®—æ³•çš„è®¡ç®—æˆæœ¬
        Helps understand computational cost of algorithm
        """
        print("\nç®—æ³•ç»Ÿè®¡ Algorithm Statistics:")
        print("-" * 40)
        print(f"æ€»è¿è¡Œæ—¶é—´: {self.total_time:.3f}ç§’")
        print(f"Total runtime: {self.total_time:.3f}s")
        print(f"ç­–ç•¥è¯„ä¼°æ¬¡æ•°: {self.total_evaluations}")
        print(f"Policy evaluations: {self.total_evaluations}")
        print(f"ç­–ç•¥æ”¹è¿›æ¬¡æ•°: {self.total_improvements}")
        print(f"Policy improvements: {self.total_improvements}")
        
        if self.iteration_history:
            total_eval_time = sum(it['eval_time'] for it in self.iteration_history)
            total_improve_time = sum(it['improve_time'] for it in self.iteration_history)
            print(f"è¯„ä¼°æ€»æ—¶é—´: {total_eval_time:.3f}ç§’ ({total_eval_time/self.total_time*100:.1f}%)")
            print(f"æ”¹è¿›æ€»æ—¶é—´: {total_improve_time:.3f}ç§’ ({total_improve_time/self.total_time*100:.1f}%)")


# ================================================================================
# ç¬¬4.2.2èŠ‚ï¼šç­–ç•¥è¿­ä»£å¯è§†åŒ–
# Section 4.2.2: Policy Iteration Visualization
# ================================================================================

class PolicyIterationVisualizer:
    """
    ç­–ç•¥è¿­ä»£å¯è§†åŒ–å™¨
    Policy Iteration Visualizer
    
    å¯è§†åŒ–æ˜¯ç†è§£ç®—æ³•çš„å…³é”®ï¼
    Visualization is key to understanding algorithms!
    
    å±•ç¤ºå†…å®¹ï¼š
    What to show:
    1. ç­–ç•¥æ¼”åŒ–ï¼šç­–ç•¥å¦‚ä½•é€æ­¥æ”¹è¿›
       Policy evolution: how policy improves step by step
    2. ä»·å€¼å‡½æ•°å˜åŒ–ï¼šV(s)å¦‚ä½•æ”¶æ•›
       Value function changes: how V(s) converges
    3. æ”¶æ•›è¿‡ç¨‹ï¼šè¿­ä»£æ¬¡æ•°ä¸æ”¹è¿›å…³ç³»
       Convergence process: iterations vs improvements
    """
    
    @staticmethod
    def visualize_convergence(policy_iter: PolicyIteration):
        """
        å¯è§†åŒ–æ”¶æ•›è¿‡ç¨‹
        Visualize Convergence Process
        
        å±•ç¤ºç­–ç•¥è¿­ä»£çš„æ”¶æ•›ç‰¹æ€§
        Show convergence characteristics of policy iteration
        """
        if not policy_iter.iteration_history:
            logger.warning("æ²¡æœ‰è¿­ä»£å†å²å¯è§†åŒ–")
            return None
        
        history = policy_iter.iteration_history
        
        # åˆ›å»ºå›¾è¡¨
        fig, axes = plt.subplots(2, 2, figsize=(14, 10))
        
        # ========== å›¾1ï¼šç­–ç•¥å˜åŒ–æ•°é‡ ==========
        # Chart 1: Number of Policy Changes
        ax1 = axes[0, 0]
        iterations = [h['iteration'] for h in history]
        changes = [h['changes_count'] for h in history]
        
        ax1.bar(iterations, changes, color='steelblue', alpha=0.7)
        ax1.set_xlabel('Iteration / è¿­ä»£')
        ax1.set_ylabel('Policy Changes / ç­–ç•¥å˜åŒ–æ•°')
        ax1.set_title('Policy Changes per Iteration / æ¯æ¬¡è¿­ä»£çš„ç­–ç•¥å˜åŒ–')
        ax1.grid(True, alpha=0.3)
        
        # æ ‡æ³¨æ”¶æ•›ç‚¹
        if changes[-1] == 0:
            ax1.axvline(x=iterations[-1], color='red', linestyle='--', 
                       label='Converged / æ”¶æ•›')
            ax1.legend()
        
        # ========== å›¾2ï¼šè®¡ç®—æ—¶é—´åˆ†å¸ƒ ==========
        # Chart 2: Computation Time Distribution
        ax2 = axes[0, 1]
        eval_times = [h['eval_time'] for h in history]
        improve_times = [h['improve_time'] for h in history]
        
        width = 0.35
        x_pos = np.arange(len(iterations))
        
        bars1 = ax2.bar(x_pos - width/2, eval_times, width, 
                       label='Evaluation / è¯„ä¼°', color='lightblue')
        bars2 = ax2.bar(x_pos + width/2, improve_times, width,
                       label='Improvement / æ”¹è¿›', color='lightcoral')
        
        ax2.set_xlabel('Iteration / è¿­ä»£')
        ax2.set_ylabel('Time (seconds) / æ—¶é—´ï¼ˆç§’ï¼‰')
        ax2.set_title('Computation Time per Step / æ¯æ­¥è®¡ç®—æ—¶é—´')
        ax2.set_xticks(x_pos)
        ax2.set_xticklabels(iterations)
        ax2.legend()
        ax2.grid(True, alpha=0.3, axis='y')
        
        # ========== å›¾3ï¼šä»·å€¼å‡½æ•°æ¼”åŒ–ï¼ˆçƒ­åŠ›å›¾ï¼‰ ==========
        # Chart 3: Value Function Evolution (Heatmap)
        ax3 = axes[1, 0]
        
        # æ”¶é›†æ‰€æœ‰è¿­ä»£çš„ä»·å€¼å‡½æ•°
        states = policy_iter.env.state_space
        n_states = min(10, len(states))  # æœ€å¤šæ˜¾ç¤º10ä¸ªçŠ¶æ€
        sample_states = states[:n_states]
        
        value_matrix = np.zeros((len(history), n_states))
        for i, h in enumerate(history):
            V = h['value_function']
            for j, state in enumerate(sample_states):
                value_matrix[i, j] = V.get_value(state)
        
        # ç»˜åˆ¶çƒ­åŠ›å›¾
        im = ax3.imshow(value_matrix.T, aspect='auto', cmap='coolwarm')
        ax3.set_xlabel('Iteration / è¿­ä»£')
        ax3.set_ylabel('State / çŠ¶æ€')
        ax3.set_title('Value Function Evolution / ä»·å€¼å‡½æ•°æ¼”åŒ–')
        ax3.set_xticks(range(len(history)))
        ax3.set_xticklabels([h['iteration'] for h in history])
        ax3.set_yticks(range(n_states))
        ax3.set_yticklabels([s.id for s in sample_states])
        
        # æ·»åŠ é¢œè‰²æ¡
        plt.colorbar(im, ax=ax3, label='State Value / çŠ¶æ€ä»·å€¼')
        
        # ========== å›¾4ï¼šç®—æ³•æµç¨‹ç¤ºæ„ ==========
        # Chart 4: Algorithm Flow Diagram
        ax4 = axes[1, 1]
        ax4.axis('off')
        
        # ç»˜åˆ¶æµç¨‹å›¾
        PolicyIterationVisualizer._draw_flow_diagram(ax4)
        
        plt.suptitle('Policy Iteration Analysis / ç­–ç•¥è¿­ä»£åˆ†æ', 
                    fontsize=14, fontweight='bold')
        plt.tight_layout()
        
        return fig
    
    @staticmethod
    def _draw_flow_diagram(ax):
        """
        ç»˜åˆ¶ç®—æ³•æµç¨‹å›¾
        Draw Algorithm Flow Diagram
        
        å¸®åŠ©ç†è§£ç®—æ³•çš„é€»è¾‘æµç¨‹
        Helps understand algorithm logic flow
        """
        # è®¾ç½®åæ ‡èŒƒå›´
        ax.set_xlim(0, 10)
        ax.set_ylim(0, 10)
        
        # å®šä¹‰æ–¹æ¡†æ ·å¼
        box_style = "round,pad=0.3"
        
        # 1. åˆå§‹åŒ–
        init_box = FancyBboxPatch((1, 8), 3, 1,
                                  boxstyle=box_style,
                                  facecolor='lightgreen',
                                  edgecolor='black',
                                  linewidth=2)
        ax.add_patch(init_box)
        ax.text(2.5, 8.5, 'Initialize Ï€', ha='center', va='center', fontweight='bold')
        
        # 2. ç­–ç•¥è¯„ä¼°
        eval_box = FancyBboxPatch((1, 5.5), 3, 1,
                                  boxstyle=box_style,
                                  facecolor='lightblue',
                                  edgecolor='black',
                                  linewidth=2)
        ax.add_patch(eval_box)
        ax.text(2.5, 6, 'Policy\nEvaluation', ha='center', va='center', fontweight='bold')
        
        # 3. ç­–ç•¥æ”¹è¿›
        improve_box = FancyBboxPatch((6, 5.5), 3, 1,
                                     boxstyle=box_style,
                                     facecolor='lightcoral',
                                     edgecolor='black',
                                     linewidth=2)
        ax.add_patch(improve_box)
        ax.text(7.5, 6, 'Policy\nImprovement', ha='center', va='center', fontweight='bold')
        
        # 4. æ£€æŸ¥æ”¶æ•›
        check_box = FancyBboxPatch((3.5, 2.5), 3, 1,
                                   boxstyle=box_style,
                                   facecolor='lightyellow',
                                   edgecolor='black',
                                   linewidth=2)
        ax.add_patch(check_box)
        ax.text(5, 3, 'Converged?', ha='center', va='center', fontweight='bold')
        
        # 5. è¾“å‡º
        output_box = FancyBboxPatch((3.5, 0.5), 3, 0.8,
                                    boxstyle=box_style,
                                    facecolor='lightgreen',
                                    edgecolor='black',
                                    linewidth=2)
        ax.add_patch(output_box)
        ax.text(5, 0.9, 'Output Ï€*', ha='center', va='center', fontweight='bold')
        
        # ç»˜åˆ¶ç®­å¤´
        # åˆå§‹åŒ– -> è¯„ä¼°
        ax.arrow(2.5, 7.9, 0, -1.3, head_width=0.2, head_length=0.1, 
                fc='black', ec='black')
        
        # è¯„ä¼° -> æ”¹è¿›
        ax.arrow(4.1, 6, 1.8, 0, head_width=0.2, head_length=0.1,
                fc='black', ec='black')
        
        # æ”¹è¿› -> æ£€æŸ¥
        ax.arrow(7.5, 5.4, -2, -1.8, head_width=0.2, head_length=0.1,
                fc='black', ec='black')
        
        # æ£€æŸ¥ -> è¯„ä¼°ï¼ˆå¾ªç¯ï¼‰
        ax.arrow(3.4, 3, -1, 2.4, head_width=0.2, head_length=0.1,
                fc='blue', ec='blue')
        ax.text(2, 4.2, 'No', color='blue', fontweight='bold')
        
        # æ£€æŸ¥ -> è¾“å‡ºï¼ˆæ”¶æ•›ï¼‰
        ax.arrow(5, 2.4, 0, -1, head_width=0.2, head_length=0.1,
                fc='green', ec='green')
        ax.text(5.5, 1.7, 'Yes', color='green', fontweight='bold')
        
        # æ·»åŠ æ ‡é¢˜
        ax.text(5, 9.5, 'Policy Iteration Flow', ha='center', fontsize=12, fontweight='bold')
        ax.text(5, 9, 'ç­–ç•¥è¿­ä»£æµç¨‹', ha='center', fontsize=10)
    
    @staticmethod
    def visualize_policy_evolution(policy_iter: PolicyIteration, 
                                  grid_env=None):
        """
        å¯è§†åŒ–ç­–ç•¥æ¼”åŒ–ï¼ˆé€‚ç”¨äºç½‘æ ¼ä¸–ç•Œï¼‰
        Visualize Policy Evolution (for Grid World)
        
        å±•ç¤ºç­–ç•¥åœ¨ç½‘æ ¼ä¸–ç•Œä¸­å¦‚ä½•é€æ­¥æ”¹è¿›
        Show how policy improves step by step in grid world
        """
        if not policy_iter.iteration_history:
            logger.warning("æ²¡æœ‰è¿­ä»£å†å²å¯è§†åŒ–")
            return None
        
        if grid_env is None:
            logger.warning("éœ€è¦ç½‘æ ¼ä¸–ç•Œç¯å¢ƒè¿›è¡Œç­–ç•¥å¯è§†åŒ–")
            return None
        
        history = policy_iter.iteration_history
        n_iterations = len(history)
        
        # åˆ›å»ºå­å›¾
        n_cols = min(4, n_iterations)
        n_rows = (n_iterations + n_cols - 1) // n_cols
        
        fig, axes = plt.subplots(n_rows, n_cols, figsize=(4*n_cols, 4*n_rows))
        if n_iterations == 1:
            axes = [axes]
        else:
            axes = axes.flatten()
        
        # ä¸ºæ¯ä¸ªè¿­ä»£ç»˜åˆ¶ç­–ç•¥
        for idx, h in enumerate(history):
            ax = axes[idx] if n_iterations > 1 else axes[0]
            
            # ç»˜åˆ¶ç½‘æ ¼
            PolicyIterationVisualizer._draw_grid_policy(
                ax, grid_env, h['policy'], h['value_function']
            )
            
            ax.set_title(f'Iteration {h["iteration"]}\n'
                        f'Changes: {h["changes_count"]}')
        
        # éšè—å¤šä½™çš„å­å›¾
        for idx in range(n_iterations, len(axes)):
            axes[idx].axis('off')
        
        plt.suptitle('Policy Evolution / ç­–ç•¥æ¼”åŒ–', fontsize=14, fontweight='bold')
        plt.tight_layout()
        
        return fig
    
    @staticmethod
    def _draw_grid_policy(ax, grid_env, policy, value_function):
        """
        åœ¨ç½‘æ ¼ä¸Šç»˜åˆ¶ç­–ç•¥
        Draw Policy on Grid
        
        ä½¿ç”¨ç®­å¤´è¡¨ç¤ºåŠ¨ä½œï¼Œé¢œè‰²è¡¨ç¤ºä»·å€¼
        Use arrows for actions, colors for values
        """
        rows, cols = grid_env.rows, grid_env.cols
        
        # è®¾ç½®åæ ‡
        ax.set_xlim(-0.5, cols - 0.5)
        ax.set_ylim(-0.5, rows - 0.5)
        ax.set_aspect('equal')
        ax.invert_yaxis()
        
        # ç»˜åˆ¶ç½‘æ ¼çº¿
        for i in range(rows + 1):
            ax.axhline(y=i - 0.5, color='gray', linewidth=0.5)
        for j in range(cols + 1):
            ax.axvline(x=j - 0.5, color='gray', linewidth=0.5)
        
        # åŠ¨ä½œåˆ°ç®­å¤´çš„æ˜ å°„
        action_arrows = {
            'up': (0, -0.3),
            'down': (0, 0.3),
            'left': (-0.3, 0),
            'right': (0.3, 0)
        }
        
        # ç»˜åˆ¶æ¯ä¸ªæ ¼å­
        for i in range(rows):
            for j in range(cols):
                pos = (i, j)
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯éšœç¢ç‰©
                if pos in grid_env.obstacles:
                    rect = patches.Rectangle((j-0.5, i-0.5), 1, 1,
                                           facecolor='gray', alpha=0.8)
                    ax.add_patch(rect)
                    continue
                
                # è·å–çŠ¶æ€
                if pos in grid_env.pos_to_state:
                    state = grid_env.pos_to_state[pos]
                    
                    # è·å–ä»·å€¼å¹¶ç€è‰²
                    value = value_function.get_value(state)
                    # å½’ä¸€åŒ–ä»·å€¼ç”¨äºç€è‰²
                    norm_value = (value - value_function.V.values().min()) / \
                                (value_function.V.values().max() - value_function.V.values().min() + 1e-10)
                    color = plt.cm.coolwarm(norm_value)
                    
                    rect = patches.Rectangle((j-0.5, i-0.5), 1, 1,
                                           facecolor=color, alpha=0.3)
                    ax.add_patch(rect)
                    
                    # æ˜¾ç¤ºä»·å€¼
                    ax.text(j, i-0.3, f'{value:.1f}', 
                           ha='center', va='center', fontsize=8)
                    
                    # ç»˜åˆ¶ç­–ç•¥ç®­å¤´
                    if not state.is_terminal and isinstance(policy, DeterministicPolicy):
                        if state in policy.policy_map:
                            action = policy.policy_map[state]
                            if action.id in action_arrows:
                                dx, dy = action_arrows[action.id]
                                ax.arrow(j, i, dx, dy, head_width=0.15, 
                                       head_length=0.1, fc='black', ec='black')
                
                # æ ‡è®°ç‰¹æ®Šä½ç½®
                if pos == grid_env.start_pos:
                    ax.text(j, i+0.3, 'S', ha='center', va='center',
                           fontweight='bold', color='green')
                elif pos == grid_env.goal_pos:
                    ax.text(j, i+0.3, 'G', ha='center', va='center',
                           fontweight='bold', color='red')
        
        ax.set_xticks(range(cols))
        ax.set_yticks(range(rows))
        ax.grid(True, alpha=0.3)


# ================================================================================
# ç¬¬4.2.3èŠ‚ï¼šç­–ç•¥è¿­ä»£åˆ†æ
# Section 4.2.3: Policy Iteration Analysis  
# ================================================================================

class PolicyIterationAnalysis:
    """
    ç­–ç•¥è¿­ä»£ç†è®ºåˆ†æ
    Policy Iteration Theoretical Analysis
    
    æ·±å…¥åˆ†æç®—æ³•çš„æ€§è´¨å’Œæ€§èƒ½
    Deep analysis of algorithm properties and performance
    """
    
    @staticmethod
    def analyze_convergence():
        """
        åˆ†ææ”¶æ•›æ€§
        Analyze Convergence
        
        ç­–ç•¥è¿­ä»£çš„æ”¶æ•›æ€§åˆ†æ
        Convergence analysis of policy iteration
        """
        print("\n" + "="*80)
        print("ç­–ç•¥è¿­ä»£æ”¶æ•›æ€§åˆ†æ")
        print("Policy Iteration Convergence Analysis")
        print("="*80)
        
        print("""
        ğŸ“Š 1. æ”¶æ•›æ€§ä¿è¯
        Convergence Guarantee
        ================================
        
        å®šç†ï¼šå¯¹äºæœ‰é™MDPï¼Œç­–ç•¥è¿­ä»£åœ¨æœ‰é™æ­¥å†…æ”¶æ•›åˆ°æœ€ä¼˜ç­–ç•¥ã€‚
        Theorem: For finite MDPs, policy iteration converges to optimal policy in finite steps.
        
        è¯æ˜è¦ç‚¹ Proof Outline:
        1. æ¯æ¬¡æ”¹è¿›è¦ä¹ˆä¸¥æ ¼æ”¹è¿›ç­–ç•¥ï¼Œè¦ä¹ˆç­–ç•¥ä¸å˜
           Each improvement either strictly improves or keeps policy unchanged
        2. æœ‰é™MDPåªæœ‰æœ‰é™ä¸ªç¡®å®šæ€§ç­–ç•¥ï¼š|A|^|S|
           Finite MDP has finite deterministic policies: |A|^|S|
        3. ä¸ä¼šé‡å¤è®¿é—®åŒä¸€ç­–ç•¥ï¼ˆå•è°ƒæ”¹è¿›ï¼‰
           Won't revisit same policy (monotonic improvement)
        4. å› æ­¤å¿…åœ¨æœ‰é™æ­¥å†…è¾¾åˆ°æœ€ä¼˜
           Therefore must reach optimal in finite steps
        
        ğŸ“Š 2. æ”¶æ•›é€Ÿåº¦
        Convergence Speed
        ================================
        
        å®è·µè§‚å¯Ÿï¼š
        Empirical Observations:
        - é€šå¸¸æ”¶æ•›å¾ˆå¿«ï¼ˆ<10æ¬¡è¿­ä»£ï¼‰
          Usually converges quickly (<10 iterations)
        - è¿œå°‘äºç­–ç•¥æ€»æ•°|A|^|S|
          Much less than total policies |A|^|S|
        
        åŸå›  Reasons:
        1. ç­–ç•¥æ”¹è¿›é€šå¸¸æ”¹å˜å¤šä¸ªçŠ¶æ€çš„åŠ¨ä½œ
           Policy improvement usually changes actions for multiple states
        2. å‘æœ€ä¼˜ç­–ç•¥çš„"æ·å¾„"
           "Shortcuts" toward optimal policy
        3. å¥½çš„åˆå§‹ç­–ç•¥åŠ é€Ÿæ”¶æ•›
           Good initial policy speeds convergence
        
        ğŸ“Š 3. è®¡ç®—å¤æ‚åº¦
        Computational Complexity
        ================================
        
        æ¯æ¬¡è¿­ä»£ï¼š
        Per iteration:
        - ç­–ç•¥è¯„ä¼°: O(|S|Â²|A|) Ã— è¯„ä¼°è¿­ä»£æ¬¡æ•°
          Policy evaluation: O(|S|Â²|A|) Ã— evaluation iterations
        - ç­–ç•¥æ”¹è¿›: O(|S||A||S|) = O(|S|Â²|A|)
          Policy improvement: O(|S||A||S|) = O(|S|Â²|A|)
        
        æ€»å¤æ‚åº¦ï¼š
        Total complexity:
        O(K Ã— |S|Â²|A| Ã— I)
        å…¶ä¸­ where:
        - K: ç­–ç•¥è¿­ä»£æ¬¡æ•°ï¼ˆé€šå¸¸å¾ˆå°ï¼‰
          K: policy iterations (usually small)
        - I: æ¯æ¬¡ç­–ç•¥è¯„ä¼°çš„è¿­ä»£æ¬¡æ•°
          I: iterations per policy evaluation
        
        ğŸ“Š 4. vs å…¶ä»–ç®—æ³•
        vs Other Algorithms
        ================================
        
        | ç®—æ³• Algorithm | æ¯æ­¥è®¡ç®— Per Step | æ”¶æ•›é€Ÿåº¦ Convergence | ç²¾ç¡®æ€§ Exactness |
        |----------------|------------------|---------------------|-----------------|
        | ç­–ç•¥è¿­ä»£ PI     | é«˜ High          | å¿« Fast             | ç²¾ç¡® Exact      |
        | ä»·å€¼è¿­ä»£ VI     | ä¸­ Medium        | æ…¢ Slow             | ç²¾ç¡® Exact      |
        | ä¿®æ”¹çš„PI       | ä½ Low           | ä¸­ Medium           | ç²¾ç¡® Exact      |
        | Q-å­¦ä¹          | ä½ Low           | æ…¢ Slow             | è¿‘ä¼¼ Approx     |
        
        ç­–ç•¥è¿­ä»£çš„ä¼˜åŠ¿ï¼š
        Advantages of Policy Iteration:
        âœ“ æ”¶æ•›æ­¥æ•°å°‘
          Few convergence steps
        âœ“ æ¯æ­¥éƒ½æœ‰æ˜ç¡®çš„ç­–ç•¥
          Clear policy at each step
        âœ“ ç†è®ºä¿è¯å¼º
          Strong theoretical guarantees
        
        åŠ£åŠ¿ï¼š
        Disadvantages:
        âœ— æ¯æ­¥è®¡ç®—é‡å¤§ï¼ˆå®Œæ•´ç­–ç•¥è¯„ä¼°ï¼‰
          High computation per step (full evaluation)
        âœ— éœ€è¦å®Œæ•´æ¨¡å‹
          Needs complete model
        âœ— ä¸é€‚åˆå¤§çŠ¶æ€ç©ºé—´
          Not suitable for large state spaces
        """)
    
    @staticmethod
    def compare_with_initial_policies(env):
        """
        æ¯”è¾ƒä¸åŒåˆå§‹ç­–ç•¥çš„å½±å“
        Compare Impact of Different Initial Policies
        
        å±•ç¤ºåˆå§‹ç­–ç•¥å¦‚ä½•å½±å“æ”¶æ•›é€Ÿåº¦
        Show how initial policy affects convergence speed
        """
        print("\n" + "="*80)
        print("åˆå§‹ç­–ç•¥å¯¹æ¯”å®éªŒ")
        print("Initial Policy Comparison Experiment")
        print("="*80)
        
        from src.ch03_finite_mdp.policies_and_values import UniformRandomPolicy
        
        # ä¸åŒçš„åˆå§‹ç­–ç•¥
        initial_policies = []
        
        # 1. éšæœºç­–ç•¥
        random_policy = UniformRandomPolicy(env.action_space)
        initial_policies.append(("Random", random_policy))
        
        # 2. æ€»æ˜¯å‘å³çš„ç­–ç•¥ï¼ˆå¯èƒ½ä¸é”™çš„å¯å‘å¼ï¼‰
        right_policy_map = {}
        right_action = None
        for action in env.action_space:
            if action.id == 'right':
                right_action = action
                break
        
        if right_action:
            for state in env.state_space:
                if not state.is_terminal:
                    right_policy_map[state] = right_action
            right_policy = DeterministicPolicy(right_policy_map)
            initial_policies.append(("Always Right", right_policy))
        
        # 3. æ€»æ˜¯å‘ä¸Šçš„ç­–ç•¥ï¼ˆå¯èƒ½è¾ƒå·®çš„å¯å‘å¼ï¼‰
        up_policy_map = {}
        up_action = None
        for action in env.action_space:
            if action.id == 'up':
                up_action = action
                break
        
        if up_action:
            for state in env.state_space:
                if not state.is_terminal:
                    up_policy_map[state] = up_action
            up_policy = DeterministicPolicy(up_policy_map)
            initial_policies.append(("Always Up", up_policy))
        
        # è¿è¡Œå®éªŒ
        results = []
        
        for name, init_policy in initial_policies:
            print(f"\næµ‹è¯•åˆå§‹ç­–ç•¥: {name}")
            print(f"Testing initial policy: {name}")
            
            # è¿è¡Œç­–ç•¥è¿­ä»£
            pi = PolicyIteration(env, gamma=0.9)
            policy, V = pi.solve(initial_policy=init_policy, verbose=False)
            
            result = {
                'name': name,
                'iterations': len(pi.iteration_history),
                'total_time': pi.total_time,
                'final_value': sum(V.get_value(s) for s in env.state_space) / len(env.state_space)
            }
            results.append(result)
            
            print(f"  æ”¶æ•›è¿­ä»£æ•°: {result['iterations']}")
            print(f"  æ€»æ—¶é—´: {result['total_time']:.3f}ç§’")
            print(f"  å¹³å‡ä»·å€¼: {result['final_value']:.3f}")
        
        # å¯è§†åŒ–ç»“æœ
        fig, axes = plt.subplots(1, 3, figsize=(12, 4))
        
        names = [r['name'] for r in results]
        iterations = [r['iterations'] for r in results]
        times = [r['total_time'] for r in results]
        values = [r['final_value'] for r in results]
        
        # è¿­ä»£æ¬¡æ•°
        ax1 = axes[0]
        bars1 = ax1.bar(names, iterations, color='steelblue', alpha=0.7)
        ax1.set_ylabel('Iterations to Converge')
        ax1.set_title('Convergence Speed')
        ax1.grid(True, alpha=0.3, axis='y')
        
        # æ ‡æ³¨æ•°å€¼
        for bar, val in zip(bars1, iterations):
            ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height(),
                    f'{val}', ha='center', va='bottom')
        
        # è¿è¡Œæ—¶é—´
        ax2 = axes[1]
        bars2 = ax2.bar(names, times, color='lightcoral', alpha=0.7)
        ax2.set_ylabel('Time (seconds)')
        ax2.set_title('Runtime')
        ax2.grid(True, alpha=0.3, axis='y')
        
        for bar, val in zip(bars2, times):
            ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height(),
                    f'{val:.3f}', ha='center', va='bottom')
        
        # æœ€ç»ˆä»·å€¼
        ax3 = axes[2]
        bars3 = ax3.bar(names, values, color='lightgreen', alpha=0.7)
        ax3.set_ylabel('Average State Value')
        ax3.set_title('Final Policy Quality')
        ax3.grid(True, alpha=0.3, axis='y')
        
        for bar, val in zip(bars3, values):
            ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height(),
                    f'{val:.2f}', ha='center', va='bottom')
        
        plt.suptitle('Impact of Initial Policy / åˆå§‹ç­–ç•¥çš„å½±å“', fontweight='bold')
        plt.tight_layout()
        
        return fig


# ================================================================================
# ä¸»å‡½æ•°ï¼šæ¼”ç¤ºç­–ç•¥è¿­ä»£
# Main Function: Demonstrate Policy Iteration
# ================================================================================

def main():
    """
    è¿è¡Œç­–ç•¥è¿­ä»£å®Œæ•´æ¼”ç¤º
    Run Complete Policy Iteration Demo
    """
    print("\n" + "="*80)
    print("ç¬¬4.2èŠ‚ï¼šç­–ç•¥è¿­ä»£")
    print("Section 4.2: Policy Iteration")
    print("="*80)
    
    # åˆ›å»ºæµ‹è¯•ç¯å¢ƒ
    from src.ch03_finite_mdp.gridworld import GridWorld
    
    # åˆ›å»º4x4ç½‘æ ¼ä¸–ç•Œï¼ˆç¨å¤§ä¸€äº›å±•ç¤ºç®—æ³•ï¼‰
    env = GridWorld(
        rows=4, 
        cols=4,
        start_pos=(0, 0),
        goal_pos=(3, 3),
        obstacles={(1, 1), (2, 2)}  # æ·»åŠ ä¸€äº›éšœç¢ç‰©
    )
    
    print(f"\nåˆ›å»º {env.rows}Ã—{env.cols} ç½‘æ ¼ä¸–ç•Œ")
    print(f"èµ·ç‚¹: {env.start_pos}, ç»ˆç‚¹: {env.goal_pos}")
    print(f"éšœç¢ç‰©: {env.obstacles}")
    
    # 1. è¿è¡Œç­–ç•¥è¿­ä»£
    print("\n" + "="*60)
    print("è¿è¡Œç­–ç•¥è¿­ä»£ç®—æ³•")
    print("Running Policy Iteration Algorithm")
    print("="*60)
    
    pi = PolicyIteration(env, gamma=0.9)
    optimal_policy, optimal_V = pi.solve(verbose=True)
    
    # 2. å¯è§†åŒ–æ”¶æ•›è¿‡ç¨‹
    print("\nå¯è§†åŒ–æ”¶æ•›è¿‡ç¨‹...")
    visualizer = PolicyIterationVisualizer()
    fig1 = visualizer.visualize_convergence(pi)
    
    # 3. å¯è§†åŒ–ç­–ç•¥æ¼”åŒ–
    print("\nå¯è§†åŒ–ç­–ç•¥æ¼”åŒ–...")
    fig2 = visualizer.visualize_policy_evolution(pi, env)
    
    # 4. ç†è®ºåˆ†æ
    PolicyIterationAnalysis.analyze_convergence()
    
    # 5. åˆå§‹ç­–ç•¥å¯¹æ¯”
    print("\nè¿è¡Œåˆå§‹ç­–ç•¥å¯¹æ¯”å®éªŒ...")
    fig3 = PolicyIterationAnalysis.compare_with_initial_policies(env)
    
    # æ˜¾ç¤ºæœ€ä¼˜ç­–ç•¥çš„ä¸€äº›ä¿¡æ¯
    print("\n" + "="*60)
    print("æœ€ä¼˜ç­–ç•¥åˆ†æ")
    print("Optimal Policy Analysis")
    print("="*60)
    
    # æ˜¾ç¤ºå‡ ä¸ªçŠ¶æ€çš„æœ€ä¼˜åŠ¨ä½œ
    print("\néƒ¨åˆ†çŠ¶æ€çš„æœ€ä¼˜åŠ¨ä½œ:")
    print("Optimal actions for some states:")
    
    sample_positions = [(0, 0), (0, 1), (1, 0), (2, 1), (3, 2)]
    for pos in sample_positions:
        if pos in env.pos_to_state:
            state = env.pos_to_state[pos]
            if isinstance(optimal_policy, DeterministicPolicy) and state in optimal_policy.policy_map:
                action = optimal_policy.policy_map[state]
                value = optimal_V.get_value(state)
                print(f"  ä½ç½® {pos}: {action.id} (V={value:.2f})")
    
    print("\n" + "="*80)
    print("ç­–ç•¥è¿­ä»£æ¼”ç¤ºå®Œæˆï¼")
    print("Policy Iteration Demo Complete!")
    print("\nå…³é”®è¦ç‚¹ Key Takeaways:")
    print("1. ç­–ç•¥è¿­ä»£äº¤æ›¿è¿›è¡Œè¯„ä¼°å’Œæ”¹è¿›")
    print("   Policy iteration alternates evaluation and improvement")
    print("2. é€šå¸¸æ”¶æ•›å¾ˆå¿«ï¼ˆ<10æ¬¡è¿­ä»£ï¼‰")
    print("   Usually converges quickly (<10 iterations)")
    print("3. æ¯æ¬¡è¿­ä»£éƒ½ä¿è¯ä¸ä¼šå˜å·®")
    print("   Each iteration guaranteed not worse")
    print("4. é€‚åˆå°åˆ°ä¸­ç­‰è§„æ¨¡çš„é—®é¢˜")
    print("   Suitable for small to medium problems")
    print("="*80)
    
    plt.show()


if __name__ == "__main__":
    main()