"""
================================================================================
ç¬¬3.4èŠ‚ï¼šå¹¿ä¹‰ç­–ç•¥è¿­ä»£ - å¼ºåŒ–å­¦ä¹ çš„æ ¸å¿ƒæ¨¡å¼
Section 3.4: Generalized Policy Iteration - The Core Pattern of RL
================================================================================

å¹¿ä¹‰ç­–ç•¥è¿­ä»£(GPI)æ˜¯å‡ ä¹æ‰€æœ‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•çš„åº•å±‚æ¨¡å¼ã€‚
Generalized Policy Iteration (GPI) is the underlying pattern of almost all RL methods.

æ ¸å¿ƒæ€æƒ³ï¼šåŒæ—¶ç»´æŠ¤ä¸€ä¸ªç­–ç•¥Ï€å’Œä¸€ä¸ªä»·å€¼å‡½æ•°Vï¼Œä¸¤è€…ç›¸äº’æ”¹è¿›
Core idea: Maintain both a policy Ï€ and a value function V, improving each other

ä¸¤ä¸ªè¿‡ç¨‹çš„äº¤äº’ï¼š
Interaction of two processes:
1. ç­–ç•¥è¯„ä¼°ï¼šä½¿Væ¥è¿‘v_Ï€ï¼ˆè®©ä»·å€¼å‡½æ•°æ›´å‡†ç¡®ï¼‰
   Policy Evaluation: Make V closer to v_Ï€ (make value function more accurate)
2. ç­–ç•¥æ”¹è¿›ï¼šä½¿Ï€å¯¹Vè´ªå©ªï¼ˆè®©ç­–ç•¥æ›´å¥½ï¼‰
   Policy Improvement: Make Ï€ greedy w.r.t. V (make policy better)

è¿™ä¸¤ä¸ªè¿‡ç¨‹æ—¢ç«äº‰åˆåˆä½œï¼š
These two processes both compete and cooperate:
- ç«äº‰ï¼šä¸€ä¸ªçš„æ”¹å˜ä¼šè®©å¦ä¸€ä¸ªä¸å‡†ç¡®
  Competition: Change in one makes the other inaccurate
- åˆä½œï¼šå…±åŒå‘æœ€ä¼˜è§£å‰è¿›
  Cooperation: Together they move toward optimal solution

æ¯”å–»ï¼šåƒä¸¤ä¸ªç™»å±±è€…ç”¨ç»³å­ç›¸è¿
Analogy: Like two climbers connected by a rope
- ä¸€ä¸ªæ‰¾æ›´é«˜çš„è·¯ï¼ˆç­–ç•¥æ”¹è¿›ï¼‰
  One finds higher path (policy improvement)
- ä¸€ä¸ªç¨³å®šä½ç½®ï¼ˆç­–ç•¥è¯„ä¼°ï¼‰
  One stabilizes position (policy evaluation)
- æœ€ç»ˆéƒ½åˆ°è¾¾å±±é¡¶ï¼ˆæœ€ä¼˜ç­–ç•¥ï¼‰
  Eventually both reach peak (optimal policy)
"""

import numpy as np
from typing import Dict, List, Tuple, Optional, Callable, Any, Union
from dataclasses import dataclass, field
from enum import Enum
import logging
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from matplotlib.patches import FancyBboxPatch, FancyArrowPatch
import matplotlib.animation as animation
import seaborn as sns
from collections import defaultdict, deque
import time
from abc import ABC, abstractmethod

# å¯¼å…¥åŸºç¡€ç»„ä»¶
import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent))

from src.ch03_finite_mdp.mdp_framework import (
    State, Action, MDPEnvironment
)
from src.ch03_finite_mdp.policies_and_values import (
    Policy, StateValueFunction, ActionValueFunction,
    DeterministicPolicy, StochasticPolicy
)
from dp_foundations import (
    BellmanOperator, PolicyEvaluationDP, PolicyImprovementDP
)
from policy_iteration import PolicyIteration
from value_iteration import ValueIteration

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


# ================================================================================
# ç¬¬3.4.1èŠ‚ï¼šGPIæ¨¡å¼å®šä¹‰
# Section 3.4.1: GPI Pattern Definition
# ================================================================================

class GPIPattern(Enum):
    """
    GPIçš„ä¸åŒæ¨¡å¼
    Different Patterns of GPI
    
    è¿™äº›æ˜¯GPIçš„ç‰¹æ®Šæƒ…å†µï¼Œå±•ç¤ºäº†è¯„ä¼°å’Œæ”¹è¿›çš„ä¸åŒå¹³è¡¡
    These are special cases of GPI, showing different balance of evaluation and improvement
    """
    
    # ç»å…¸æ¨¡å¼
    POLICY_ITERATION = "policy_iteration"  # å®Œå…¨è¯„ä¼° + è´ªå©ªæ”¹è¿›
                                           # Full evaluation + greedy improvement
    VALUE_ITERATION = "value_iteration"    # ä¸€æ­¥è¯„ä¼° + è´ªå©ªæ”¹è¿›
                                           # One-step evaluation + greedy improvement
    
    # ä¿®æ”¹çš„ç­–ç•¥è¿­ä»£
    MODIFIED_PI_2 = "modified_pi_2"        # 2æ­¥è¯„ä¼° + è´ªå©ªæ”¹è¿›
                                           # 2-step evaluation + greedy improvement
    MODIFIED_PI_K = "modified_pi_k"        # kæ­¥è¯„ä¼° + è´ªå©ªæ”¹è¿›
                                           # k-step evaluation + greedy improvement
    
    # å¼‚æ­¥æ¨¡å¼
    ASYNC_IN_PLACE = "async_in_place"      # åŸåœ°å¼‚æ­¥æ›´æ–°
                                           # In-place asynchronous update
    PRIORITIZED = "prioritized"            # ä¼˜å…ˆçº§æ‰«æ
                                           # Prioritized sweeping
    
    # è¿‘ä¼¼æ¨¡å¼ï¼ˆä¸ºåç»­ç« èŠ‚é¢„ç•™ï¼‰
    APPROXIMATE = "approximate"             # å‡½æ•°è¿‘ä¼¼ä¸‹çš„GPI
                                           # GPI with function approximation


@dataclass
class GPIState:
    """
    GPIè¿‡ç¨‹çš„çŠ¶æ€
    State of GPI Process
    
    è®°å½•GPIåœ¨æŸä¸€æ—¶åˆ»çš„å®Œæ•´çŠ¶æ€
    Records complete state of GPI at a moment
    
    è¿™ä¸ªæ•°æ®ç»“æ„å¸®åŠ©æˆ‘ä»¬ç†è§£GPIçš„åŠ¨æ€è¿‡ç¨‹
    This data structure helps us understand GPI dynamics
    """
    iteration: int                          # å½“å‰è¿­ä»£æ¬¡æ•°
                                           # Current iteration number
    policy: Policy                         # å½“å‰ç­–ç•¥
                                           # Current policy
    value_function: StateValueFunction      # å½“å‰ä»·å€¼å‡½æ•°
                                           # Current value function
    evaluation_error: float                 # è¯„ä¼°è¯¯å·® ||V - v_Ï€||
                                           # Evaluation error
    improvement_delta: int                  # ç­–ç•¥æ”¹å˜çš„çŠ¶æ€æ•°
                                           # Number of states with policy change
    is_optimal: bool = False               # æ˜¯å¦å·²è¾¾åˆ°æœ€ä¼˜
                                           # Whether optimal reached
    
    # æ€§èƒ½æŒ‡æ ‡
    evaluation_steps: int = 0              # è¯„ä¼°æ­¥æ•°
                                           # Evaluation steps
    improvement_steps: int = 0             # æ”¹è¿›æ­¥æ•°
                                           # Improvement steps
    computation_time: float = 0.0          # è®¡ç®—æ—¶é—´
                                           # Computation time


# ================================================================================
# ç¬¬3.4.2èŠ‚ï¼šå¹¿ä¹‰ç­–ç•¥è¿­ä»£ç®—æ³•
# Section 3.4.2: Generalized Policy Iteration Algorithm
# ================================================================================

class GeneralizedPolicyIteration:
    """
    å¹¿ä¹‰ç­–ç•¥è¿­ä»£ - RLçš„ç»Ÿä¸€æ¡†æ¶
    Generalized Policy Iteration - Unified Framework of RL
    
    è¿™ä¸ªç±»å±•ç¤ºäº†æ‰€æœ‰DPç®—æ³•éƒ½æ˜¯GPIçš„ç‰¹ä¾‹
    This class shows all DP algorithms are special cases of GPI
    
    å…³é”®æ´å¯Ÿï¼š
    Key Insights:
    1. ä¸éœ€è¦å®Œå…¨è¯„ä¼°å°±å¯ä»¥æ”¹è¿›ç­–ç•¥
       Don't need full evaluation to improve policy
    2. ä¸éœ€è¦å®Œå…¨è´ªå©ªå°±å¯ä»¥æ”¹è¿›ç­–ç•¥
       Don't need full greediness to improve policy
    3. è¯„ä¼°å’Œæ”¹è¿›å¯ä»¥äº¤ç»‡è¿›è¡Œ
       Evaluation and improvement can be interleaved
    
    æ•°å­¦åŸºç¡€ï¼š
    Mathematical Foundation:
    - å•è°ƒæ€§ï¼šæ¯æ¬¡æ”¹è¿›ä¸ä¼šå˜å·®
      Monotonicity: Each improvement not worse
    - æ”¶æ•›æ€§ï¼šæœ€ç»ˆæ”¶æ•›åˆ°æœ€ä¼˜
      Convergence: Eventually converges to optimal
    - çµæ´»æ€§ï¼šå¯ä»¥åœ¨ä»»ä½•æ—¶å€™åœæ­¢
      Flexibility: Can stop at any time
    """
    
    def __init__(self, mdp_env: MDPEnvironment, gamma: float = 0.99):
        """
        åˆå§‹åŒ–GPI
        Initialize GPI
        
        Args:
            mdp_env: MDPç¯å¢ƒ
            gamma: æŠ˜æ‰£å› å­
        
        è®¾è®¡æ€è€ƒï¼š
        Design Consideration:
        å°†GPIè®¾è®¡æˆå¯é…ç½®çš„æ¡†æ¶ï¼Œæ”¯æŒä¸åŒçš„è¯„ä¼°å’Œæ”¹è¿›ç­–ç•¥
        Design GPI as configurable framework supporting different evaluation and improvement strategies
        """
        self.env = mdp_env
        self.gamma = gamma
        
        # æ ¸å¿ƒç»„ä»¶
        self.bellman_op = BellmanOperator(mdp_env, gamma)
        self.evaluator = PolicyEvaluationDP(mdp_env, gamma)
        self.improver = PolicyImprovementDP(mdp_env, gamma)
        
        # GPIå†å²è®°å½•
        self.gpi_history: List[GPIState] = []
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.total_iterations = 0
        self.total_eval_steps = 0
        self.total_improve_steps = 0
        self.total_time = 0.0
        
        logger.info(f"åˆå§‹åŒ–å¹¿ä¹‰ç­–ç•¥è¿­ä»£ï¼ŒÎ³={gamma}")
    
    def solve(self,
             pattern: GPIPattern = GPIPattern.POLICY_ITERATION,
             initial_policy: Optional[Policy] = None,
             evaluation_steps: Union[int, str] = "full",
             improvement_type: str = "greedy",
             theta: float = 1e-6,
             max_iterations: int = 1000,
             verbose: bool = True) -> Tuple[Policy, StateValueFunction]:
        """
        è¿è¡ŒGPIç®—æ³•
        Run GPI Algorithm
        
        è¿™æ˜¯GPIçš„æ ¸å¿ƒå®ç°ï¼Œå±•ç¤ºäº†ä¸åŒç®—æ³•å¦‚ä½•ä½œä¸ºå‚æ•°é…ç½®
        This is the core implementation of GPI, showing how different algorithms are parameter configurations
        
        Args:
            pattern: GPIæ¨¡å¼
                    GPI pattern
            initial_policy: åˆå§‹ç­–ç•¥
                          Initial policy
            evaluation_steps: è¯„ä¼°æ­¥æ•°
                            - "full": å®Œå…¨è¯„ä¼°ï¼ˆç­–ç•¥è¿­ä»£ï¼‰
                            - 1: ä¸€æ­¥è¯„ä¼°ï¼ˆä»·å€¼è¿­ä»£ï¼‰
                            - k: kæ­¥è¯„ä¼°ï¼ˆä¿®æ”¹çš„ç­–ç•¥è¿­ä»£ï¼‰
                            Evaluation steps
                            - "full": full evaluation (policy iteration)
                            - 1: one step (value iteration)
                            - k: k steps (modified policy iteration)
            improvement_type: æ”¹è¿›ç±»å‹
                            - "greedy": å®Œå…¨è´ªå©ª
                            - "epsilon_greedy": Îµ-è´ªå©ª
                            - "soft": è½¯æ”¹è¿›
                            Improvement type
            theta: æ”¶æ•›é˜ˆå€¼
                  Convergence threshold
            max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•°
                          Maximum iterations
            verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
                    Whether to print details
        
        Returns:
            (æœ€ä¼˜ç­–ç•¥, æœ€ä¼˜ä»·å€¼å‡½æ•°)
            (optimal policy, optimal value function)
        
        ç®—æ³•æ¡†æ¶ï¼š
        Algorithm Framework:
        ```
        åˆå§‹åŒ– Ï€, V
        Initialize Ï€, V
        é‡å¤ï¼š
        Repeat:
            éƒ¨åˆ†è¯„ä¼°ï¼šV â†’ v_Ï€çš„æ–¹å‘ç§»åŠ¨
            Partial evaluation: V moves toward v_Ï€
            éƒ¨åˆ†æ”¹è¿›ï¼šÏ€ â†’ Ï€'ä½¿å¾—Ï€'å¯¹Væ›´è´ªå©ª
            Partial improvement: Ï€ â†’ Ï€' to be more greedy w.r.t. V
        ç›´åˆ°æ”¶æ•›
        Until convergence
        ```
        """
        # æ¸…ç©ºå†å²
        self.gpi_history = []
        
        # å¼€å§‹è®¡æ—¶
        start_time = time.time()
        
        # åˆå§‹åŒ–ç­–ç•¥
        if initial_policy is None:
            from src.ch03_finite_mdp.policies_and_values import UniformRandomPolicy
            policy = UniformRandomPolicy(self.env.action_space)
        else:
            policy = initial_policy
        
        # åˆå§‹åŒ–ä»·å€¼å‡½æ•°
        V = StateValueFunction(self.env.state_space, initial_value=0.0)
        
        if verbose:
            print("\n" + "="*80)
            print(f"å¼€å§‹å¹¿ä¹‰ç­–ç•¥è¿­ä»£ (æ¨¡å¼: {pattern.value})")
            print(f"Starting Generalized Policy Iteration (pattern: {pattern.value})")
            print("="*80)
            print(f"è¯„ä¼°æ­¥æ•°: {evaluation_steps}")
            print(f"æ”¹è¿›ç±»å‹: {improvement_type}")
            print(f"æ”¶æ•›é˜ˆå€¼: {theta}")
        
        # æ ¹æ®æ¨¡å¼é…ç½®å‚æ•°
        eval_steps, improve_type = self._configure_pattern(pattern, evaluation_steps, improvement_type)
        
        # GPIä¸»å¾ªç¯
        for iteration in range(max_iterations):
            iter_start = time.time()
            
            if verbose and iteration % 10 == 0:
                print(f"\n--- GPIè¿­ä»£ {iteration + 1} ---")
            
            # ========== ç­–ç•¥è¯„ä¼°é˜¶æ®µ ==========
            # Policy Evaluation Phase
            V_old = StateValueFunction(self.env.state_space)
            for state in self.env.state_space:
                V_old.set_value(state, V.get_value(state))
            
            # æ‰§è¡Œéƒ¨åˆ†è¯„ä¼°
            V, eval_error, eval_steps_taken = self._partial_evaluation(
                policy, V, eval_steps, theta
            )
            self.total_eval_steps += eval_steps_taken
            
            if verbose and iteration % 10 == 0:
                print(f"  è¯„ä¼°: {eval_steps_taken}æ­¥, è¯¯å·®={eval_error:.2e}")
            
            # ========== ç­–ç•¥æ”¹è¿›é˜¶æ®µ ==========
            # Policy Improvement Phase
            new_policy, improvement_delta = self._policy_improvement(
                V, policy, improve_type
            )
            self.total_improve_steps += 1
            
            if verbose and iteration % 10 == 0:
                print(f"  æ”¹è¿›: {improvement_delta}ä¸ªçŠ¶æ€æ”¹å˜")
            
            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€ä¼˜
            is_optimal = (improvement_delta == 0 and eval_error < theta)
            
            # è®°å½•GPIçŠ¶æ€
            gpi_state = GPIState(
                iteration=iteration + 1,
                policy=new_policy,
                value_function=V,
                evaluation_error=eval_error,
                improvement_delta=improvement_delta,
                is_optimal=is_optimal,
                evaluation_steps=eval_steps_taken,
                improvement_steps=1,
                computation_time=time.time() - iter_start
            )
            self.gpi_history.append(gpi_state)
            
            # æ›´æ–°ç­–ç•¥
            policy = new_policy
            
            # æ£€æŸ¥æ”¶æ•›
            if is_optimal:
                self.total_iterations = iteration + 1
                if verbose:
                    print(f"\nâœ“ GPIæ”¶æ•›ï¼")
                    print(f"  è¿­ä»£æ¬¡æ•°: {self.total_iterations}")
                    print(f"  æ€»è¯„ä¼°æ­¥æ•°: {self.total_eval_steps}")
                    print(f"  æ€»æ”¹è¿›æ­¥æ•°: {self.total_improve_steps}")
                break
            
            # æ—©åœæ£€æŸ¥ï¼ˆå¦‚æœç­–ç•¥ç¨³å®šä½†è¯„ä¼°æœªå®Œå…¨æ”¶æ•›ï¼‰
            if improvement_delta == 0 and eval_error < theta * 10:
                if verbose:
                    print(f"\nâœ“ ç­–ç•¥ç¨³å®šï¼Œæå‰åœæ­¢")
                self.total_iterations = iteration + 1
                break
        else:
            # è¾¾åˆ°æœ€å¤§è¿­ä»£
            self.total_iterations = max_iterations
            if verbose:
                print(f"\nâš  è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•° {max_iterations}")
        
        self.total_time = time.time() - start_time
        
        if verbose:
            self._print_statistics()
            self._analyze_convergence()
        
        return policy, V
    
    def _configure_pattern(self, pattern: GPIPattern, 
                          eval_steps: Union[int, str],
                          improve_type: str) -> Tuple[Union[int, str], str]:
        """
        æ ¹æ®GPIæ¨¡å¼é…ç½®å‚æ•°
        Configure parameters based on GPI pattern
        
        å±•ç¤ºä¸åŒç®—æ³•å¦‚ä½•æ˜ å°„åˆ°GPIå‚æ•°
        Shows how different algorithms map to GPI parameters
        """
        if pattern == GPIPattern.POLICY_ITERATION:
            # ç­–ç•¥è¿­ä»£ï¼šå®Œå…¨è¯„ä¼° + è´ªå©ªæ”¹è¿›
            return "full", "greedy"
        elif pattern == GPIPattern.VALUE_ITERATION:
            # ä»·å€¼è¿­ä»£ï¼šä¸€æ­¥è¯„ä¼° + è´ªå©ªæ”¹è¿›
            return 1, "greedy"
        elif pattern == GPIPattern.MODIFIED_PI_2:
            # ä¿®æ”¹çš„ç­–ç•¥è¿­ä»£(m=2)
            return 2, "greedy"
        elif pattern == GPIPattern.MODIFIED_PI_K:
            # ä¿®æ”¹çš„ç­–ç•¥è¿­ä»£(m=k)
            return eval_steps if isinstance(eval_steps, int) else 5, "greedy"
        elif pattern == GPIPattern.ASYNC_IN_PLACE:
            # å¼‚æ­¥åŸåœ°æ›´æ–°
            return 1, "greedy"
        elif pattern == GPIPattern.PRIORITIZED:
            # ä¼˜å…ˆçº§æ‰«æ
            return 1, "greedy"
        else:
            # é»˜è®¤ä½¿ç”¨æä¾›çš„å‚æ•°
            return eval_steps, improve_type
    
    def _partial_evaluation(self, policy: Policy, V: StateValueFunction,
                           steps: Union[int, str], theta: float) -> Tuple[StateValueFunction, float, int]:
        """
        éƒ¨åˆ†ç­–ç•¥è¯„ä¼°
        Partial Policy Evaluation
        
        è¿™æ˜¯GPIçš„å…³é”®ï¼šä¸éœ€è¦å®Œå…¨è¯„ä¼°ï¼
        This is key to GPI: Don't need full evaluation!
        
        Args:
            policy: å½“å‰ç­–ç•¥
                   Current policy
            V: å½“å‰ä»·å€¼å‡½æ•°
              Current value function
            steps: è¯„ä¼°æ­¥æ•°
                  Evaluation steps
            theta: æ”¶æ•›é˜ˆå€¼
                  Convergence threshold
        
        Returns:
            (æ›´æ–°çš„ä»·å€¼å‡½æ•°, è¯„ä¼°è¯¯å·®, å®é™…æ­¥æ•°)
            (updated value function, evaluation error, actual steps)
        """
        if steps == "full":
            # å®Œå…¨è¯„ä¼°ï¼ˆç­–ç•¥è¿­ä»£æ¨¡å¼ï¼‰
            V_new = self.evaluator.evaluate(policy, theta=theta)
            steps_taken = len(self.evaluator.evaluation_history)
            
            # è®¡ç®—è¯¯å·®
            error = 0.0
            for state in self.env.state_space:
                error = max(error, abs(V_new.get_value(state) - V.get_value(state)))
            
            return V_new, error, steps_taken
        
        else:
            # éƒ¨åˆ†è¯„ä¼°ï¼ˆkæ­¥ï¼‰
            steps_taken = 0
            max_error = 0.0
            
            for _ in range(steps):
                # åº”ç”¨ä¸€æ¬¡è´å°”æ›¼æœŸæœ›ç®—å­
                V_new = self.bellman_op.bellman_expectation_operator(V, policy)
                
                # è®¡ç®—è¿™ä¸€æ­¥çš„è¯¯å·®
                step_error = 0.0
                for state in self.env.state_space:
                    old_val = V.get_value(state)
                    new_val = V_new.get_value(state)
                    step_error = max(step_error, abs(new_val - old_val))
                
                max_error = max(max_error, step_error)
                V = V_new
                steps_taken += 1
                
                # å¦‚æœå·²ç»æ”¶æ•›ï¼Œæå‰åœæ­¢
                if step_error < theta:
                    break
            
            return V, max_error, steps_taken
    
    def _policy_improvement(self, V: StateValueFunction, 
                           current_policy: Policy,
                           improve_type: str) -> Tuple[Policy, int]:
        """
        ç­–ç•¥æ”¹è¿›
        Policy Improvement
        
        æ ¹æ®æ”¹è¿›ç±»å‹æ‰§è¡Œä¸åŒçš„ç­–ç•¥æ›´æ–°
        Execute different policy updates based on improvement type
        
        Args:
            V: ä»·å€¼å‡½æ•°
              Value function
            current_policy: å½“å‰ç­–ç•¥
                          Current policy
            improve_type: æ”¹è¿›ç±»å‹
                        Improvement type
        
        Returns:
            (æ–°ç­–ç•¥, æ”¹å˜çš„çŠ¶æ€æ•°)
            (new policy, number of changed states)
        """
        if improve_type == "greedy":
            # å®Œå…¨è´ªå©ªæ”¹è¿›
            new_policy, _ = self.improver.improve(V)
            
            # è®¡ç®—æ”¹å˜çš„çŠ¶æ€æ•°
            changes = 0
            if isinstance(new_policy, DeterministicPolicy) and isinstance(current_policy, DeterministicPolicy):
                for state in self.env.state_space:
                    if not state.is_terminal:
                        if state in new_policy.policy_map and state in current_policy.policy_map:
                            if new_policy.policy_map[state] != current_policy.policy_map[state]:
                                changes += 1
            
            return new_policy, changes
        
        elif improve_type == "epsilon_greedy":
            # Îµ-è´ªå©ªæ”¹è¿›ï¼ˆä¸ºåç»­ç« èŠ‚é¢„ç•™ï¼‰
            # è¿™é‡Œç®€åŒ–ä¸ºè´ªå©ªæ”¹è¿›
            return self._policy_improvement(V, current_policy, "greedy")
        
        elif improve_type == "soft":
            # è½¯æ”¹è¿›ï¼ˆä¸ºåç»­ç« èŠ‚é¢„ç•™ï¼‰
            # è¿™é‡Œç®€åŒ–ä¸ºè´ªå©ªæ”¹è¿›
            return self._policy_improvement(V, current_policy, "greedy")
        
        else:
            raise ValueError(f"æœªçŸ¥çš„æ”¹è¿›ç±»å‹: {improve_type}")
    
    def _print_statistics(self):
        """
        æ‰“å°GPIç»Ÿè®¡ä¿¡æ¯
        Print GPI Statistics
        
        å¸®åŠ©ç†è§£ä¸åŒGPIå˜ä½“çš„æ€§èƒ½ç‰¹å¾
        Helps understand performance characteristics of different GPI variants
        """
        print("\n" + "-"*40)
        print("GPIç»Ÿè®¡ä¿¡æ¯")
        print("GPI Statistics")
        print("-"*40)
        
        print(f"æ€»è¿­ä»£æ¬¡æ•°: {self.total_iterations}")
        print(f"æ€»è¯„ä¼°æ­¥æ•°: {self.total_eval_steps}")
        print(f"æ€»æ”¹è¿›æ­¥æ•°: {self.total_improve_steps}")
        print(f"æ€»è¿è¡Œæ—¶é—´: {self.total_time:.3f}ç§’")
        
        if self.total_eval_steps > 0:
            print(f"å¹³å‡è¯„ä¼°æ­¥æ•°/è¿­ä»£: {self.total_eval_steps/self.total_iterations:.1f}")
        
        print(f"å¹³å‡æ—¶é—´/è¿­ä»£: {self.total_time/self.total_iterations:.4f}ç§’")
    
    def _analyze_convergence(self):
        """
        åˆ†ææ”¶æ•›è¿‡ç¨‹
        Analyze Convergence Process
        
        å±•ç¤ºGPIçš„æ”¶æ•›ç‰¹æ€§
        Show convergence characteristics of GPI
        """
        if not self.gpi_history:
            return
        
        print("\n" + "-"*40)
        print("æ”¶æ•›åˆ†æ")
        print("Convergence Analysis")
        print("-"*40)
        
        # åˆ†æè¯„ä¼°è¯¯å·®ä¸‹é™
        eval_errors = [state.evaluation_error for state in self.gpi_history]
        print(f"åˆå§‹è¯„ä¼°è¯¯å·®: {eval_errors[0]:.2e}")
        print(f"æœ€ç»ˆè¯„ä¼°è¯¯å·®: {eval_errors[-1]:.2e}")
        
        # åˆ†æç­–ç•¥æ”¹å˜
        policy_changes = [state.improvement_delta for state in self.gpi_history]
        stable_iteration = None
        for i, changes in enumerate(policy_changes):
            if changes == 0:
                stable_iteration = i + 1
                break
        
        if stable_iteration:
            print(f"ç­–ç•¥ç¨³å®šäºè¿­ä»£: {stable_iteration}")
        
        # åˆ†æè®¡ç®—æ•ˆç‡
        total_comp_time = sum(state.computation_time for state in self.gpi_history)
        eval_time = sum(state.computation_time * state.evaluation_steps / 
                       (state.evaluation_steps + state.improvement_steps)
                       for state in self.gpi_history)
        improve_time = total_comp_time - eval_time
        
        print(f"è¯„ä¼°æ—¶é—´å æ¯”: {eval_time/total_comp_time*100:.1f}%")
        print(f"æ”¹è¿›æ—¶é—´å æ¯”: {improve_time/total_comp_time*100:.1f}%")


# ================================================================================
# ç¬¬3.4.3èŠ‚ï¼šGPIå¯è§†åŒ–
# Section 3.4.3: GPI Visualization
# ================================================================================

class GPIVisualizer:
    """
    GPIå¯è§†åŒ–å™¨
    GPI Visualizer
    
    å±•ç¤ºGPIçš„åŠ¨æ€è¿‡ç¨‹å’Œæ”¶æ•›ç‰¹æ€§
    Show dynamics and convergence of GPI
    
    å¯è§†åŒ–å¸®åŠ©ç†è§£ï¼š
    Visualization helps understand:
    1. è¯„ä¼°å’Œæ”¹è¿›çš„ç›¸äº’ä½œç”¨
       Interaction between evaluation and improvement
    2. ä¸åŒGPIå˜ä½“çš„æ”¶æ•›é€Ÿåº¦
       Convergence speed of different GPI variants
    3. ç­–ç•¥å’Œä»·å€¼å‡½æ•°çš„ååŒæ¼”åŒ–
       Co-evolution of policy and value function
    """
    
    @staticmethod
    def visualize_gpi_process(gpi: GeneralizedPolicyIteration):
        """
        å¯è§†åŒ–GPIè¿‡ç¨‹
        Visualize GPI Process
        
        å±•ç¤ºè¯„ä¼°å’Œæ”¹è¿›çš„äº¤æ›¿è¿‡ç¨‹
        Show alternating process of evaluation and improvement
        """
        if not gpi.gpi_history:
            logger.warning("æ²¡æœ‰GPIå†å²å¯è§†åŒ–")
            return None
        
        fig, axes = plt.subplots(2, 3, figsize=(16, 10))
        
        # ========== å›¾1ï¼šè¯„ä¼°è¯¯å·®å’Œç­–ç•¥å˜åŒ– ==========
        ax1 = axes[0, 0]
        
        iterations = [state.iteration for state in gpi.gpi_history]
        eval_errors = [state.evaluation_error for state in gpi.gpi_history]
        policy_changes = [state.improvement_delta for state in gpi.gpi_history]
        
        # åŒYè½´
        ax1_twin = ax1.twinx()
        
        # è¯„ä¼°è¯¯å·®ï¼ˆå¯¹æ•°å°ºåº¦ï¼‰
        line1 = ax1.semilogy(iterations, eval_errors, 'b-', 
                            label='Evaluation Error', linewidth=2)
        ax1.set_xlabel('Iteration / è¿­ä»£')
        ax1.set_ylabel('Evaluation Error (log) / è¯„ä¼°è¯¯å·®ï¼ˆå¯¹æ•°ï¼‰', color='b')
        ax1.tick_params(axis='y', labelcolor='b')
        
        # ç­–ç•¥å˜åŒ–
        line2 = ax1_twin.plot(iterations, policy_changes, 'r--', 
                             label='Policy Changes', linewidth=2, alpha=0.7)
        ax1_twin.set_ylabel('Policy Changes / ç­–ç•¥å˜åŒ–', color='r')
        ax1_twin.tick_params(axis='y', labelcolor='r')
        
        ax1.set_title('GPI Convergence / GPIæ”¶æ•›')
        ax1.grid(True, alpha=0.3)
        
        # åˆå¹¶å›¾ä¾‹
        lines = line1 + line2
        labels = [l.get_label() for l in lines]
        ax1.legend(lines, labels, loc='upper right')
        
        # ========== å›¾2ï¼šè¯„ä¼°æ­¥æ•°åˆ†å¸ƒ ==========
        ax2 = axes[0, 1]
        
        eval_steps = [state.evaluation_steps for state in gpi.gpi_history]
        
        ax2.bar(iterations, eval_steps, color='lightblue', alpha=0.7)
        ax2.set_xlabel('Iteration / è¿­ä»£')
        ax2.set_ylabel('Evaluation Steps / è¯„ä¼°æ­¥æ•°')
        ax2.set_title('Evaluation Effort / è¯„ä¼°å·¥ä½œé‡')
        ax2.grid(True, alpha=0.3, axis='y')
        
        # æ·»åŠ å¹³å‡çº¿
        avg_steps = np.mean(eval_steps)
        ax2.axhline(y=avg_steps, color='red', linestyle='--', 
                   label=f'Average: {avg_steps:.1f}')
        ax2.legend()
        
        # ========== å›¾3ï¼šè®¡ç®—æ—¶é—´åˆ†æ ==========
        ax3 = axes[0, 2]
        
        comp_times = [state.computation_time for state in gpi.gpi_history]
        cumulative_times = np.cumsum(comp_times)
        
        ax3.plot(iterations, cumulative_times, 'g-', linewidth=2)
        ax3.fill_between(iterations, 0, cumulative_times, alpha=0.3, color='green')
        ax3.set_xlabel('Iteration / è¿­ä»£')
        ax3.set_ylabel('Cumulative Time (s) / ç´¯ç§¯æ—¶é—´ï¼ˆç§’ï¼‰')
        ax3.set_title('Computational Cost / è®¡ç®—æˆæœ¬')
        ax3.grid(True, alpha=0.3)
        
        # ========== å›¾4ï¼šGPIæ¦‚å¿µå›¾ ==========
        ax4 = axes[1, 0]
        ax4.axis('off')
        
        GPIVisualizer._draw_gpi_diagram(ax4)
        
        # ========== å›¾5ï¼šç®—æ³•æ¯”è¾ƒ ==========
        ax5 = axes[1, 1]
        ax5.axis('off')
        
        # åˆ›å»ºæ¯”è¾ƒè¡¨æ ¼
        comparison_data = {
            'Algorithm': ['Policy Iteration', 'Value Iteration', 'Modified PI', 'Async GPI'],
            'Eval Steps': ['Full (~100)', '1', 'k (2-10)', '1'],
            'Convergence': ['Fast (<10)', 'Slow (>50)', 'Medium', 'Variable'],
            'Memory': ['High', 'Low', 'Medium', 'Low'],
            'Stability': ['Stable', 'Stable', 'Stable', 'Less Stable']
        }
        
        # è½¬ç½®æ•°æ®ç”¨äºè¡¨æ ¼
        table_data = []
        headers = list(comparison_data.keys())
        for i in range(len(comparison_data['Algorithm'])):
            row = [comparison_data[key][i] for key in headers]
            table_data.append(row)
        
        table = ax5.table(cellText=table_data,
                         colLabels=headers,
                         cellLoc='center',
                         loc='center',
                         colWidths=[0.25, 0.2, 0.2, 0.15, 0.2])
        
        table.auto_set_font_size(False)
        table.set_fontsize(9)
        table.scale(1, 1.5)
        
        # è®¾ç½®è¡¨æ ¼æ ·å¼
        for (i, j), cell in table.get_celld().items():
            if i == 0:  # æ ‡é¢˜è¡Œ
                cell.set_facecolor('#40466e')
                cell.set_text_props(weight='bold', color='white')
            else:
                cell.set_facecolor('#f0f0f0' if i % 2 == 0 else '#ffffff')
        
        ax5.set_title('GPI Variants Comparison / GPIå˜ä½“æ¯”è¾ƒ', pad=20)
        
        # ========== å›¾6ï¼šæ”¶æ•›è½¨è¿¹ ==========
        ax6 = axes[1, 2]
        
        # åœ¨ç­–ç•¥-ä»·å€¼ç©ºé—´ä¸­ç»˜åˆ¶è½¨è¿¹
        # è¿™é‡Œç”¨ä¸€ä¸ªç®€åŒ–çš„2DæŠ•å½±è¡¨ç¤º
        
        # è®¡ç®—ç­–ç•¥"è´¨é‡"ï¼ˆç”¨æ”¹å˜æ•°çš„åå‘ä½œä¸ºä»£ç†ï¼‰
        max_states = len(gpi.env.state_space)
        policy_quality = [1 - (state.improvement_delta / max_states) 
                         for state in gpi.gpi_history]
        
        # è®¡ç®—ä»·å€¼å‡½æ•°"å‡†ç¡®åº¦"ï¼ˆç”¨è¯¯å·®çš„åå‘ä½œä¸ºä»£ç†ï¼‰
        max_error = max(eval_errors) if eval_errors else 1.0
        value_accuracy = [1 - (err / max_error) for err in eval_errors]
        
        # ç»˜åˆ¶è½¨è¿¹
        ax6.plot(value_accuracy, policy_quality, 'o-', markersize=4, alpha=0.7)
        
        # æ ‡è®°èµ·ç‚¹å’Œç»ˆç‚¹
        ax6.plot(value_accuracy[0], policy_quality[0], 'go', markersize=10, 
                label='Start')
        ax6.plot(value_accuracy[-1], policy_quality[-1], 'r*', markersize=15, 
                label='End (Optimal)')
        
        ax6.set_xlabel('Value Accuracy / ä»·å€¼å‡†ç¡®åº¦')
        ax6.set_ylabel('Policy Quality / ç­–ç•¥è´¨é‡')
        ax6.set_title('GPI Trajectory / GPIè½¨è¿¹')
        ax6.legend()
        ax6.grid(True, alpha=0.3)
        
        # æ·»åŠ ç†æƒ³è·¯å¾„ï¼ˆå¯¹è§’çº¿ï¼‰
        ax6.plot([0, 1], [0, 1], 'k--', alpha=0.3, label='Ideal Path')
        
        plt.suptitle('Generalized Policy Iteration Analysis / å¹¿ä¹‰ç­–ç•¥è¿­ä»£åˆ†æ',
                    fontsize=14, fontweight='bold')
        plt.tight_layout()
        
        return fig
    
    @staticmethod
    def _draw_gpi_diagram(ax):
        """
        ç»˜åˆ¶GPIæ¦‚å¿µå›¾
        Draw GPI Conceptual Diagram
        
        å±•ç¤ºè¯„ä¼°å’Œæ”¹è¿›çš„ç›¸äº’ä½œç”¨
        Show interaction between evaluation and improvement
        """
        # è®¾ç½®åæ ‡èŒƒå›´
        ax.set_xlim(0, 10)
        ax.set_ylim(0, 10)
        
        # ç­–ç•¥ç©ºé—´
        policy_box = FancyBboxPatch((1, 6), 3, 2,
                                   boxstyle="round,pad=0.1",
                                   facecolor='lightblue',
                                   edgecolor='blue',
                                   linewidth=2)
        ax.add_patch(policy_box)
        ax.text(2.5, 7, 'Policy Ï€', ha='center', va='center', 
               fontweight='bold', fontsize=12)
        
        # ä»·å€¼ç©ºé—´
        value_box = FancyBboxPatch((6, 6), 3, 2,
                                  boxstyle="round,pad=0.1",
                                  facecolor='lightgreen',
                                  edgecolor='green',
                                  linewidth=2)
        ax.add_patch(value_box)
        ax.text(7.5, 7, 'Value V', ha='center', va='center',
               fontweight='bold', fontsize=12)
        
        # è¯„ä¼°ç®­å¤´
        eval_arrow = FancyArrowPatch((4, 7.5), (6, 7.5),
                                   connectionstyle="arc3,rad=0",
                                   arrowstyle="->",
                                   mutation_scale=20,
                                   linewidth=2,
                                   color='blue')
        ax.add_patch(eval_arrow)
        ax.text(5, 8, 'Evaluation', ha='center', fontsize=10)
        ax.text(5, 7.8, 'V â†’ v_Ï€', ha='center', fontsize=9, style='italic')
        
        # æ”¹è¿›ç®­å¤´
        improve_arrow = FancyArrowPatch((6, 6.5), (4, 6.5),
                                      connectionstyle="arc3,rad=0",
                                      arrowstyle="->",
                                      mutation_scale=20,
                                      linewidth=2,
                                      color='green')
        ax.add_patch(improve_arrow)
        ax.text(5, 6, 'Improvement', ha='center', fontsize=10)
        ax.text(5, 5.8, 'Ï€ â†’ greedy(V)', ha='center', fontsize=9, style='italic')
        
        # æœ€ä¼˜ç‚¹
        optimal_point = plt.Circle((5, 3), 0.5, color='red', alpha=0.7)
        ax.add_patch(optimal_point)
        ax.text(5, 3, 'Ï€* = v*', ha='center', va='center',
               color='white', fontweight='bold')
        
        # æ·»åŠ èºæ—‹è½¨è¿¹è¡¨ç¤ºæ”¶æ•›
        theta_spiral = np.linspace(0, 4*np.pi, 100)
        r_spiral = np.linspace(2, 0.5, 100)
        x_spiral = 5 + r_spiral * np.cos(theta_spiral)
        y_spiral = 3 + r_spiral * np.sin(theta_spiral) * 0.5
        ax.plot(x_spiral, y_spiral, 'k--', alpha=0.3, linewidth=1)
        
        # æ·»åŠ è¯´æ˜
        ax.text(5, 9.5, 'GPI: Competing & Cooperating', 
               ha='center', fontsize=11, fontweight='bold')
        ax.text(5, 9, 'ç«äº‰ä¸åˆä½œ', ha='center', fontsize=10)
        
        ax.text(5, 1, 'Eventually converge to optimal\næœ€ç»ˆæ”¶æ•›åˆ°æœ€ä¼˜',
               ha='center', fontsize=9, style='italic')
    
    @staticmethod
    def compare_gpi_variants(env: MDPEnvironment, n_runs: int = 3):
        """
        æ¯”è¾ƒä¸åŒGPIå˜ä½“
        Compare Different GPI Variants
        
        å®éªŒå±•ç¤ºä¸åŒå¹³è¡¡ç‚¹çš„æ•ˆæœ
        Experiment shows effects of different balance points
        """
        print("\n" + "="*80)
        print("GPIå˜ä½“æ¯”è¾ƒå®éªŒ")
        print("GPI Variants Comparison Experiment")
        print("="*80)
        
        patterns = [
            (GPIPattern.POLICY_ITERATION, "full", "Policy Iteration"),
            (GPIPattern.VALUE_ITERATION, 1, "Value Iteration"),
            (GPIPattern.MODIFIED_PI_2, 2, "Modified PI (m=2)"),
            (GPIPattern.MODIFIED_PI_K, 5, "Modified PI (m=5)"),
        ]
        
        results = {name: [] for _, _, name in patterns}
        
        for run in range(n_runs):
            print(f"\nè¿è¡Œ {run + 1}/{n_runs}")
            
            for pattern, eval_steps, name in patterns:
                print(f"  æµ‹è¯• {name}...")
                
                gpi = GeneralizedPolicyIteration(env, gamma=0.9)
                
                # é…ç½®è¯„ä¼°æ­¥æ•°
                if pattern == GPIPattern.POLICY_ITERATION:
                    eval_steps_config = "full"
                else:
                    eval_steps_config = eval_steps
                
                # è¿è¡ŒGPI
                policy, V = gpi.solve(
                    pattern=pattern,
                    evaluation_steps=eval_steps_config,
                    max_iterations=200,
                    verbose=False
                )
                
                # è®°å½•ç»“æœ
                results[name].append({
                    'iterations': gpi.total_iterations,
                    'eval_steps': gpi.total_eval_steps,
                    'time': gpi.total_time
                })
                
                print(f"    å®Œæˆ: {gpi.total_iterations}æ¬¡è¿­ä»£, "
                     f"{gpi.total_eval_steps}æ¬¡è¯„ä¼°æ­¥")
        
        # å¯è§†åŒ–æ¯”è¾ƒç»“æœ
        fig, axes = plt.subplots(1, 3, figsize=(15, 5))
        
        names = list(results.keys())
        colors = ['steelblue', 'lightcoral', 'lightgreen', 'gold']
        
        # å›¾1ï¼šè¿­ä»£æ¬¡æ•°
        ax1 = axes[0]
        avg_iterations = [np.mean([r['iterations'] for r in results[name]]) 
                         for name in names]
        bars1 = ax1.bar(range(len(names)), avg_iterations, color=colors, alpha=0.7)
        ax1.set_xticks(range(len(names)))
        ax1.set_xticklabels(names, rotation=45, ha='right')
        ax1.set_ylabel('Iterations to Converge')
        ax1.set_title('Convergence Speed')
        ax1.grid(True, alpha=0.3, axis='y')
        
        for bar, val in zip(bars1, avg_iterations):
            ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height(),
                    f'{val:.0f}', ha='center', va='bottom')
        
        # å›¾2ï¼šæ€»è¯„ä¼°æ­¥æ•°
        ax2 = axes[1]
        avg_eval_steps = [np.mean([r['eval_steps'] for r in results[name]])
                         for name in names]
        bars2 = ax2.bar(range(len(names)), avg_eval_steps, color=colors, alpha=0.7)
        ax2.set_xticks(range(len(names)))
        ax2.set_xticklabels(names, rotation=45, ha='right')
        ax2.set_ylabel('Total Evaluation Steps')
        ax2.set_title('Evaluation Effort')
        ax2.grid(True, alpha=0.3, axis='y')
        
        for bar, val in zip(bars2, avg_eval_steps):
            ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height(),
                    f'{val:.0f}', ha='center', va='bottom')
        
        # å›¾3ï¼šè¿è¡Œæ—¶é—´
        ax3 = axes[2]
        avg_times = [np.mean([r['time'] for r in results[name]])
                    for name in names]
        bars3 = ax3.bar(range(len(names)), avg_times, color=colors, alpha=0.7)
        ax3.set_xticks(range(len(names)))
        ax3.set_xticklabels(names, rotation=45, ha='right')
        ax3.set_ylabel('Time (seconds)')
        ax3.set_title('Runtime')
        ax3.grid(True, alpha=0.3, axis='y')
        
        for bar, val in zip(bars3, avg_times):
            ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height(),
                    f'{val:.3f}', ha='center', va='bottom')
        
        plt.suptitle('GPI Variants Performance Comparison', fontsize=14, fontweight='bold')
        plt.tight_layout()
        
        # æ‰“å°æ€»ç»“
        print("\n" + "="*60)
        print("å®éªŒæ€»ç»“")
        print("="*60)
        
        print("\næ€§èƒ½æ’åï¼ˆæŒ‰è¿è¡Œæ—¶é—´ï¼‰ï¼š")
        time_ranking = sorted([(name, np.mean([r['time'] for r in results[name]])) 
                             for name in names], key=lambda x: x[1])
        for i, (name, time) in enumerate(time_ranking, 1):
            print(f"{i}. {name}: {time:.3f}ç§’")
        
        print("\nå…³é”®å‘ç°ï¼š")
        print("1. ç­–ç•¥è¿­ä»£ï¼šè¿­ä»£å°‘ä½†æ¯æ­¥è®¡ç®—å¤š")
        print("2. ä»·å€¼è¿­ä»£ï¼šè¿­ä»£å¤šä½†æ¯æ­¥è®¡ç®—å°‘")
        print("3. ä¿®æ”¹çš„ç­–ç•¥è¿­ä»£ï¼šåœ¨ä¸¤è€…é—´å–å¾—å¹³è¡¡")
        print("4. æœ€ä¼˜çš„må€¼å–å†³äºå…·ä½“é—®é¢˜")
        
        return fig


# ================================================================================
# ç¬¬3.4.4èŠ‚ï¼šGPIç†è®ºåˆ†æ
# Section 3.4.4: GPI Theoretical Analysis
# ================================================================================

class GPITheory:
    """
    GPIç†è®ºåˆ†æ
    GPI Theoretical Analysis
    
    æ·±å…¥ç†è§£GPIçš„æ•°å­¦æ€§è´¨
    Deep understanding of GPI mathematical properties
    """
    
    @staticmethod
    def explain_gpi_theory():
        """
        è§£é‡ŠGPIç†è®º
        Explain GPI Theory
        """
        print("\n" + "="*80)
        print("å¹¿ä¹‰ç­–ç•¥è¿­ä»£ç†è®º")
        print("Generalized Policy Iteration Theory")
        print("="*80)
        
        print("""
        ğŸ“š 1. GPIçš„æ•°å­¦åŸºç¡€
        Mathematical Foundation of GPI
        ================================
        
        GPIç»´æŠ¤ä¸¤ä¸ªè¿‘ä¼¼ï¼š
        GPI maintains two approximations:
        - ä»·å€¼å‡½æ•°V â‰ˆ v_Ï€
          Value function V â‰ˆ v_Ï€
        - ç­–ç•¥Ï€ â‰ˆ greedy(V)
          Policy Ï€ â‰ˆ greedy(V)
        
        ç¨³å®šæ¡ä»¶ï¼ˆGPIä¸åŠ¨ç‚¹ï¼‰ï¼š
        Stability condition (GPI fixed point):
        V = v_Ï€ ä¸” Ï€ = greedy(V)
        
        è¿™æ°å¥½æ˜¯è´å°”æ›¼æœ€ä¼˜æ–¹ç¨‹çš„æ¡ä»¶ï¼
        This is exactly the Bellman optimality condition!
        
        ğŸ“š 2. æ”¶æ•›æ€§è¯æ˜è¦ç‚¹
        Convergence Proof Outline
        ================================
        
        å®šç†ï¼šGPIæ”¶æ•›åˆ°æœ€ä¼˜ç­–ç•¥Ï€*å’Œæœ€ä¼˜ä»·å€¼v*
        Theorem: GPI converges to optimal policy Ï€* and optimal value v*
        
        è¯æ˜æ€è·¯ï¼š
        Proof idea:
        1. æ¯æ¬¡æ”¹è¿›å•è°ƒä¸å‡ï¼šv_{Ï€_{k+1}} â‰¥ v_{Ï€_k}
           Each improvement is monotonic
        2. ä»·å€¼å‡½æ•°æœ‰ä¸Šç•Œï¼šv_Ï€ â‰¤ v* for all Ï€
           Value functions are bounded
        3. å•è°ƒæœ‰ç•Œåºåˆ—å¿…æ”¶æ•›
           Monotonic bounded sequence converges
        4. æ”¶æ•›ç‚¹æ»¡è¶³è´å°”æ›¼æœ€ä¼˜æ¡ä»¶
           Convergence point satisfies Bellman optimality
        
        ğŸ“š 3. GPIçš„æ™®éæ€§
        Universality of GPI
        ================================
        
        å‡ ä¹æ‰€æœ‰RLæ–¹æ³•éƒ½æ˜¯GPIçš„å®ä¾‹ï¼š
        Almost all RL methods are instances of GPI:
        
        | æ–¹æ³• Method | è¯„ä¼° Evaluation | æ”¹è¿› Improvement |
        |------------|----------------|-----------------|
        | DP | è´å°”æ›¼æœŸæœ›ç®—å­ | è´ªå©ª |
        | MC | é‡‡æ ·å¹³å‡ | è´ªå©ª |
        | TD | è‡ªä¸¾æ›´æ–° | Îµ-è´ªå©ª |
        | Q-Learning | TD(0) | è´ªå©ª |
        | Actor-Critic | Critic | Actor |
        
        ğŸ“š 4. è¯„ä¼°-æ”¹è¿›çš„æƒè¡¡
        Evaluation-Improvement Tradeoff
        ================================
        
        å…³é”®æ´å¯Ÿï¼šä¸éœ€è¦å®Œç¾ï¼
        Key insight: Don't need perfection!
        
        - å®Œå…¨è¯„ä¼°ï¼ˆç­–ç•¥è¿­ä»£ï¼‰ï¼š
          Full evaluation (Policy Iteration):
          âœ“ ç¨³å®šï¼Œè¿­ä»£å°‘
          âœ“ Stable, few iterations
          âœ— æ¯æ­¥è®¡ç®—é‡å¤§
          âœ— High computation per step
        
        - æœ€å°è¯„ä¼°ï¼ˆä»·å€¼è¿­ä»£ï¼‰ï¼š
          Minimal evaluation (Value Iteration):
          âœ“ æ¯æ­¥è®¡ç®—é‡å°
          âœ“ Low computation per step
          âœ— éœ€è¦æ›´å¤šè¿­ä»£
          âœ— Need more iterations
        
        - å¹³è¡¡ç‚¹ï¼ˆä¿®æ”¹çš„ç­–ç•¥è¿­ä»£ï¼‰ï¼š
          Balance (Modified Policy Iteration):
          âœ“ å¯è°ƒèŠ‚çš„æƒè¡¡
          âœ“ Adjustable tradeoff
          âœ“ é€šå¸¸æœ€å®ç”¨
          âœ“ Often most practical
        
        ğŸ“š 5. GPIä¸å¼ºåŒ–å­¦ä¹ çš„ç»Ÿä¸€è§†è§’
        GPI as Unifying View of RL
        ================================
        
        GPIæä¾›äº†ç†è§£æ‰€æœ‰RLç®—æ³•çš„æ¡†æ¶ï¼š
        GPI provides framework to understand all RL:
        
        ç»´åº¦1ï¼šè¯„ä¼°æ–¹æ³•
        Dimension 1: Evaluation method
        - åŸºäºæ¨¡å‹ï¼ˆDPï¼‰
          Model-based (DP)
        - æ— æ¨¡å‹ï¼ˆMC, TDï¼‰
          Model-free (MC, TD)
        - å‡½æ•°è¿‘ä¼¼
          Function approximation
        
        ç»´åº¦2ï¼šæ”¹è¿›æ–¹æ³•
        Dimension 2: Improvement method
        - è´ªå©ª
          Greedy
        - Îµ-è´ªå©ª
          Îµ-greedy
        - è½¯ç­–ç•¥
          Soft policy
        - ç­–ç•¥æ¢¯åº¦
          Policy gradient
        
        ç»´åº¦3ï¼šäº¤æ›¿æ¨¡å¼
        Dimension 3: Alternation pattern
        - å®Œå…¨äº¤æ›¿
          Full alternation
        - éƒ¨åˆ†äº¤æ›¿
          Partial alternation
        - å¼‚æ­¥æ›´æ–°
          Asynchronous update
        
        è¿™ä¸ªç»Ÿä¸€è§†è§’æ˜¯ç†è§£æ•´ä¸ªRLé¢†åŸŸçš„å…³é”®ï¼
        This unified view is key to understanding all of RL!
        """)


# ================================================================================
# ä¸»å‡½æ•°ï¼šæ¼”ç¤ºGPI
# Main Function: Demonstrate GPI
# ================================================================================

def main():
    """
    è¿è¡Œå¹¿ä¹‰ç­–ç•¥è¿­ä»£æ¼”ç¤º
    Run Generalized Policy Iteration Demo
    """
    print("\n" + "="*80)
    print("ç¬¬3.4èŠ‚ï¼šå¹¿ä¹‰ç­–ç•¥è¿­ä»£")
    print("Section 3.4: Generalized Policy Iteration")
    print("="*80)
    
    # åˆ›å»ºæµ‹è¯•ç¯å¢ƒ
    from src.ch03_finite_mdp.gridworld import GridWorld
    
    # åˆ›å»º4x4ç½‘æ ¼ä¸–ç•Œ
    env = GridWorld(
        rows=4,
        cols=4,
        start_pos=(0, 0),
        goal_pos=(3, 3),
        obstacles={(1, 1), (2, 2)}
    )
    
    print(f"\nåˆ›å»º {env.rows}Ã—{env.cols} ç½‘æ ¼ä¸–ç•Œ")
    print(f"Create {env.rows}Ã—{env.cols} Grid World")
    
    # 1. æ¼”ç¤ºæ ‡å‡†GPIï¼ˆç­–ç•¥è¿­ä»£æ¨¡å¼ï¼‰
    print("\n" + "="*60)
    print("1. æ ‡å‡†GPIï¼ˆç­–ç•¥è¿­ä»£æ¨¡å¼ï¼‰")
    print("1. Standard GPI (Policy Iteration Mode)")
    print("="*60)
    
    gpi = GeneralizedPolicyIteration(env, gamma=0.9)
    policy_pi, V_pi = gpi.solve(
        pattern=GPIPattern.POLICY_ITERATION,
        verbose=True
    )
    
    # 2. æ¼”ç¤ºGPIï¼ˆä»·å€¼è¿­ä»£æ¨¡å¼ï¼‰
    print("\n" + "="*60)
    print("2. GPIï¼ˆä»·å€¼è¿­ä»£æ¨¡å¼ï¼‰")
    print("2. GPI (Value Iteration Mode)")
    print("="*60)
    
    gpi_vi = GeneralizedPolicyIteration(env, gamma=0.9)
    policy_vi, V_vi = gpi_vi.solve(
        pattern=GPIPattern.VALUE_ITERATION,
        verbose=True
    )
    
    # 3. æ¼”ç¤ºä¿®æ”¹çš„ç­–ç•¥è¿­ä»£
    print("\n" + "="*60)
    print("3. ä¿®æ”¹çš„ç­–ç•¥è¿­ä»£ (m=3)")
    print("3. Modified Policy Iteration (m=3)")
    print("="*60)
    
    gpi_mod = GeneralizedPolicyIteration(env, gamma=0.9)
    policy_mod, V_mod = gpi_mod.solve(
        pattern=GPIPattern.MODIFIED_PI_K,
        evaluation_steps=3,
        verbose=True
    )
    
    # 4. å¯è§†åŒ–GPIè¿‡ç¨‹
    print("\n4. å¯è§†åŒ–GPIè¿‡ç¨‹")
    print("4. Visualize GPI Process")
    visualizer = GPIVisualizer()
    fig1 = visualizer.visualize_gpi_process(gpi)
    
    # 5. æ¯”è¾ƒä¸åŒGPIå˜ä½“
    print("\n5. æ¯”è¾ƒGPIå˜ä½“")
    print("5. Compare GPI Variants")
    fig2 = GPIVisualizer.compare_gpi_variants(env, n_runs=3)
    
    # 6. ç†è®ºåˆ†æ
    GPITheory.explain_gpi_theory()
    
    # 7. éªŒè¯æ‰€æœ‰æ–¹æ³•æ”¶æ•›åˆ°ç›¸åŒçš„æœ€ä¼˜ç­–ç•¥
    print("\n" + "="*60)
    print("éªŒè¯æ”¶æ•›ç»“æœ")
    print("Verify Convergence Results")
    print("="*60)
    
    # æ¯”è¾ƒä»·å€¼å‡½æ•°
    print("\nä»·å€¼å‡½æ•°æ¯”è¾ƒï¼ˆéƒ¨åˆ†çŠ¶æ€ï¼‰ï¼š")
    print("Value Function Comparison (sample states):")
    
    sample_states = env.state_space[:5]
    for state in sample_states:
        v_pi = V_pi.get_value(state)
        v_vi = V_vi.get_value(state)
        v_mod = V_mod.get_value(state)
        
        print(f"  State {state.id}:")
        print(f"    Policy Iter: {v_pi:.3f}")
        print(f"    Value Iter:  {v_vi:.3f}")
        print(f"    Modified PI: {v_mod:.3f}")
        
        # æ£€æŸ¥æ˜¯å¦æ”¶æ•›åˆ°ç›¸åŒå€¼
        if abs(v_pi - v_vi) < 0.01 and abs(v_pi - v_mod) < 0.01:
            print(f"    âœ“ æ”¶æ•›ä¸€è‡´")
        else:
            print(f"    âš  å€¼ä¸ä¸€è‡´")
    
    print("\n" + "="*80)
    print("å¹¿ä¹‰ç­–ç•¥è¿­ä»£æ¼”ç¤ºå®Œæˆï¼")
    print("Generalized Policy Iteration Demo Complete!")
    print("\nå…³é”®è¦ç‚¹ Key Takeaways:")
    print("1. GPIæ˜¯å‡ ä¹æ‰€æœ‰RLæ–¹æ³•çš„æ ¸å¿ƒæ¨¡å¼")
    print("   GPI is the core pattern of almost all RL methods")
    print("2. è¯„ä¼°å’Œæ”¹è¿›ç›¸äº’ç«äº‰åˆç›¸äº’åˆä½œ")
    print("   Evaluation and improvement compete and cooperate")
    print("3. ä¸éœ€è¦å®Œç¾çš„è¯„ä¼°æˆ–æ”¹è¿›å°±èƒ½æ”¶æ•›")
    print("   Don't need perfect evaluation or improvement to converge")
    print("4. ä¸åŒçš„GPIå˜ä½“åœ¨è®¡ç®—æ•ˆç‡ä¸Šæœ‰ä¸åŒæƒè¡¡")
    print("   Different GPI variants have different computational tradeoffs")
    print("5. ç†è§£GPIæ˜¯ç†è§£æ•´ä¸ªRLçš„å…³é”®")
    print("   Understanding GPI is key to understanding all of RL")
    print("="*80)
    
    plt.show()
    
    return policy_pi, V_pi


if __name__ == "__main__":
    main()