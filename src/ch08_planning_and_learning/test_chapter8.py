#!/usr/bin/env python
"""
æµ‹è¯•ç¬¬8ç« æ‰€æœ‰è§„åˆ’ä¸å­¦ä¹ æ¨¡å—
Test all Chapter 8 Planning and Learning modules

ç¡®ä¿æ‰€æœ‰ç®—æ³•å®ç°æ­£ç¡®
Ensure all algorithm implementations are correct
"""

import sys
import traceback
import numpy as np
from pathlib import Path
import time

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))


def test_models_and_planning():
    """
    æµ‹è¯•æ¨¡å‹ä¸è§„åˆ’åŸºç¡€
    Test models and planning foundations
    """
    print("\n" + "="*60)
    print("æµ‹è¯•æ¨¡å‹ä¸è§„åˆ’...")
    print("Testing Models and Planning...")
    print("="*60)
    
    try:
        from src.ch08_planning_and_learning.models_and_planning import (
            DeterministicModel, StochasticModel, PlanningAgent
        )
        from src.ch02_mdp.gridworld import GridWorld
        
        # åˆ›å»ºç¯å¢ƒ
        env = GridWorld(rows=3, cols=3, start_pos=(0,0), goal_pos=(2,2))
        print("âœ“ åˆ›å»º3Ã—3ç½‘æ ¼ä¸–ç•Œ")
        
        # æµ‹è¯•ç¡®å®šæ€§æ¨¡å‹
        print("\næµ‹è¯•ç¡®å®šæ€§æ¨¡å‹...")
        det_model = DeterministicModel(env.state_space, env.action_space)
        
        state = env.state_space[0]
        action = env.action_space[0]
        next_state = env.state_space[1]
        reward = -1.0
        
        det_model.update(state, action, next_state, reward)
        assert det_model.is_known(state, action), "æ¨¡å‹åº”è¯¥çŸ¥é“æ­¤è½¬ç§»"
        
        sampled_next, sampled_reward = det_model.sample(state, action)
        assert sampled_next == next_state, "ç¡®å®šæ€§æ¨¡å‹åº”è¿”å›ç›¸åŒçš„ä¸‹ä¸€çŠ¶æ€"
        assert sampled_reward == reward, "ç¡®å®šæ€§æ¨¡å‹åº”è¿”å›ç›¸åŒçš„å¥–åŠ±"
        print("  âœ“ ç¡®å®šæ€§æ¨¡å‹æµ‹è¯•é€šè¿‡")
        
        # æµ‹è¯•éšæœºæ¨¡å‹
        print("\næµ‹è¯•éšæœºæ¨¡å‹...")
        stoch_model = StochasticModel(env.state_space, env.action_space)
        
        # æ·»åŠ å¤šä¸ªè½¬ç§»æ¨¡æ‹Ÿéšæœºæ€§
        for _ in range(10):
            stoch_model.update(state, action, next_state, -1.0)
        stoch_model.update(state, action, env.state_space[2], -2.0)
        
        prob = stoch_model.get_probability(state, action, next_state)
        assert 0 <= prob <= 1, "æ¦‚ç‡åº”åœ¨[0,1]èŒƒå›´"
        assert prob > 0.5, "ä¸»è¦è½¬ç§»åº”æœ‰è¾ƒé«˜æ¦‚ç‡"
        print("  âœ“ éšæœºæ¨¡å‹æµ‹è¯•é€šè¿‡")
        
        # æµ‹è¯•è§„åˆ’æ™ºèƒ½ä½“
        print("\næµ‹è¯•è§„åˆ’æ™ºèƒ½ä½“...")
        planner = PlanningAgent(det_model, env.state_space, env.action_space)
        
        # æ‰§è¡Œè§„åˆ’æ­¥éª¤
        initial_steps = planner.planning_steps
        planner.plan(10)
        assert planner.planning_steps == initial_steps + 10, "åº”æ‰§è¡Œ10æ­¥è§„åˆ’"
        print("  âœ“ è§„åˆ’æ™ºèƒ½ä½“æµ‹è¯•é€šè¿‡")
        
        print("\nâœ… æ¨¡å‹ä¸è§„åˆ’æµ‹è¯•å…¨éƒ¨é€šè¿‡ï¼")
        return True
        
    except Exception as e:
        print(f"\nâŒ æ¨¡å‹ä¸è§„åˆ’æµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return False


def test_dyna_q():
    """
    æµ‹è¯•Dyna-Qç®—æ³•
    Test Dyna-Q algorithm
    """
    print("\n" + "="*60)
    print("æµ‹è¯•Dyna-Qç®—æ³•...")
    print("Testing Dyna-Q Algorithm...")
    print("="*60)
    
    try:
        from src.ch08_planning_and_learning.dyna_q import (
            DynaQ, DynaQPlus, DynaQComparator
        )
        from src.ch02_mdp.gridworld import GridWorld
        
        # åˆ›å»ºç¯å¢ƒ
        env = GridWorld(rows=4, cols=4, start_pos=(0,0), goal_pos=(3,3))
        print("âœ“ åˆ›å»º4Ã—4ç½‘æ ¼ä¸–ç•Œ")
        
        # æµ‹è¯•åŸºæœ¬Dyna-Q
        print("\næµ‹è¯•Dyna-Q...")
        dyna_q = DynaQ(env, n_planning_steps=5, gamma=0.9, alpha=0.1, epsilon=0.1)
        
        # å­¦ä¹ å‡ ä¸ªå›åˆ
        for _ in range(10):
            ret, length = dyna_q.learn_episode()
            assert -100 < ret < 100, f"å›æŠ¥å¼‚å¸¸: {ret}"
            assert 0 < length < 1000, f"å›åˆé•¿åº¦å¼‚å¸¸: {length}"
        
        assert dyna_q.real_steps > 0, "åº”æœ‰çœŸå®æ­¥æ•°"
        assert dyna_q.planning_steps > 0, "åº”æœ‰è§„åˆ’æ­¥æ•°"
        assert len(dyna_q.observed_sa_pairs) > 0, "åº”æœ‰è§‚å¯Ÿçš„çŠ¶æ€-åŠ¨ä½œå¯¹"
        print(f"  âœ“ Dyna-Qæµ‹è¯•é€šè¿‡ (çœŸå®æ­¥æ•°={dyna_q.real_steps}, è§„åˆ’æ­¥æ•°={dyna_q.planning_steps})")
        
        # æµ‹è¯•Dyna-Q+
        print("\næµ‹è¯•Dyna-Q+...")
        dyna_q_plus = DynaQPlus(env, n_planning_steps=5, kappa=0.001)
        
        # æµ‹è¯•æ¢ç´¢å¥–åŠ±
        state = env.state_space[0]
        action = env.action_space[0]
        dyna_q_plus.current_time = 100
        dyna_q_plus.last_visit_time[(state, action)] = 0
        
        bonus = dyna_q_plus.get_exploration_bonus(state, action)
        assert bonus > 0, "åº”æœ‰æ¢ç´¢å¥–åŠ±"
        assert bonus == dyna_q_plus.kappa * np.sqrt(100), "æ¢ç´¢å¥–åŠ±è®¡ç®—é”™è¯¯"
        print(f"  âœ“ Dyna-Q+æµ‹è¯•é€šè¿‡ (æ¢ç´¢å¥–åŠ±={bonus:.4f})")
        
        # æµ‹è¯•æ¯”è¾ƒå™¨
        print("\næµ‹è¯•Dyna-Qæ¯”è¾ƒå™¨...")
        comparator = DynaQComparator(env)
        results = comparator.compare_planning_steps(
            n_values=[0, 5],
            n_episodes=10,
            n_runs=2,
            verbose=False
        )
        
        assert len(results) == 2, "åº”æœ‰2ä¸ªç»“æœ"
        assert 0 in results and 5 in results, "åº”åŒ…å«n=0å’Œn=5çš„ç»“æœ"
        print("  âœ“ Dyna-Qæ¯”è¾ƒå™¨æµ‹è¯•é€šè¿‡")
        
        print("\nâœ… Dyna-Qæµ‹è¯•å…¨éƒ¨é€šè¿‡ï¼")
        return True
        
    except Exception as e:
        print(f"\nâŒ Dyna-Qæµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return False


def test_prioritized_sweeping():
    """
    æµ‹è¯•ä¼˜å…ˆçº§æ‰«æ
    Test prioritized sweeping
    """
    print("\n" + "="*60)
    print("æµ‹è¯•ä¼˜å…ˆçº§æ‰«æ...")
    print("Testing Prioritized Sweeping...")
    print("="*60)
    
    try:
        from src.ch08_planning_and_learning.prioritized_sweeping import (
            PriorityQueue, PrioritizedSweeping, PrioritizedDynaQ
        )
        from src.ch02_mdp.gridworld import GridWorld
        
        # æµ‹è¯•ä¼˜å…ˆé˜Ÿåˆ—
        print("æµ‹è¯•ä¼˜å…ˆé˜Ÿåˆ—...")
        pqueue = PriorityQueue(threshold=0.01)
        
        from src.ch02_mdp.mdp_framework import State, Action
        state1 = State("s1", features={})
        state2 = State("s2", features={})
        action1 = Action("a1")
        action2 = Action("a2")
        
        pqueue.push(state1, action1, 0.5)
        pqueue.push(state2, action2, 0.8)
        
        assert pqueue.size() == 2, "é˜Ÿåˆ—åº”æœ‰2ä¸ªå…ƒç´ "
        
        # å¼¹å‡ºåº”è¯¥æ˜¯é«˜ä¼˜å…ˆçº§çš„
        s, a, p = pqueue.pop()
        assert s == state2 and a == action2, "åº”å…ˆå¼¹å‡ºé«˜ä¼˜å…ˆçº§é¡¹"
        assert abs(p - 0.8) < 0.001, "ä¼˜å…ˆçº§åº”ä¸º0.8"
        print("  âœ“ ä¼˜å…ˆé˜Ÿåˆ—æµ‹è¯•é€šè¿‡")
        
        # æµ‹è¯•ä¼˜å…ˆçº§æ‰«æ
        print("\næµ‹è¯•ä¼˜å…ˆçº§æ‰«æç®—æ³•...")
        env = GridWorld(rows=4, cols=4, start_pos=(0,0), goal_pos=(3,3))
        ps = PrioritizedSweeping(env, n_planning_steps=5, threshold=0.01)
        
        # å­¦ä¹ å‡ ä¸ªå›åˆ
        for _ in range(10):
            ret, length = ps.learn_episode()
            assert -100 < ret < 100, f"å›æŠ¥å¼‚å¸¸: {ret}"
            assert 0 < length < 1000, f"å›åˆé•¿åº¦å¼‚å¸¸: {length}"
        
        assert ps.real_steps > 0, "åº”æœ‰çœŸå®æ­¥æ•°"
        assert ps.planning_steps > 0, "åº”æœ‰è§„åˆ’æ­¥æ•°"
        assert len(ps.predecessors) > 0, "åº”æœ‰å‰é©±è®°å½•"
        print(f"  âœ“ ä¼˜å…ˆçº§æ‰«ææµ‹è¯•é€šè¿‡ (è§„åˆ’æ­¥æ•°={ps.planning_steps})")
        
        # æµ‹è¯•ä¼˜å…ˆçº§Dyna-Q
        print("\næµ‹è¯•ä¼˜å…ˆçº§Dyna-Q...")
        pdq = PrioritizedDynaQ(env, n_planning_steps=5, threshold=0.01)
        
        state = env.state_space[0]
        action = env.action_space[0]
        next_state = env.state_space[1]
        reward = -1.0
        
        pdq.learn_step(state, action, next_state, reward)
        assert pdq.real_steps == 1, "åº”æœ‰1ä¸ªçœŸå®æ­¥"
        assert (state, action) in pdq.observed_sa, "åº”è®°å½•è§‚å¯Ÿçš„(s,a)"
        print("  âœ“ ä¼˜å…ˆçº§Dyna-Qæµ‹è¯•é€šè¿‡")
        
        print("\nâœ… ä¼˜å…ˆçº§æ‰«ææµ‹è¯•å…¨éƒ¨é€šè¿‡ï¼")
        return True
        
    except Exception as e:
        print(f"\nâŒ ä¼˜å…ˆçº§æ‰«ææµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return False


def test_expected_vs_sample():
    """
    æµ‹è¯•æœŸæœ›æ›´æ–°vsæ ·æœ¬æ›´æ–°
    Test expected vs sample updates
    """
    print("\n" + "="*60)
    print("æµ‹è¯•æœŸæœ›æ›´æ–°vsæ ·æœ¬æ›´æ–°...")
    print("Testing Expected vs Sample Updates...")
    print("="*60)
    
    try:
        from src.ch08_planning_and_learning.expected_vs_sample import (
            ExpectedUpdate, SampleUpdate, UpdateComparator
        )
        from src.ch08_planning_and_learning.models_and_planning import StochasticModel
        from src.ch02_mdp.gridworld import GridWorld
        
        # åˆ›å»ºç¯å¢ƒå’Œæ¨¡å‹
        env = GridWorld(rows=3, cols=3, start_pos=(0,0), goal_pos=(2,2))
        model = StochasticModel(env.state_space, env.action_space)
        
        # æ„å»ºæ¨¡å‹
        print("æ„å»ºæ¨¡å‹...")
        for _ in range(50):
            state = env.reset()
            for _ in range(10):
                if state.is_terminal:
                    break
                action = np.random.choice(env.action_space)
                next_state, reward, done, _ = env.step(action)
                model.update(state, action, next_state, reward)
                state = next_state
        print("  âœ“ æ¨¡å‹æ„å»ºå®Œæˆ")
        
        # æµ‹è¯•æœŸæœ›æ›´æ–°
        print("\næµ‹è¯•æœŸæœ›æ›´æ–°...")
        exp_updater = ExpectedUpdate(env, model, gamma=0.95)
        
        state = env.state_space[0]
        action = env.action_space[0]
        
        if model.is_known(state, action):
            old_q = exp_updater.Q.get_value(state, action)
            exp_updater.expected_update_step(state, action)
            new_q = exp_updater.Q.get_value(state, action)
            assert new_q != old_q or old_q == 0, "Qå€¼åº”è¯¥æ›´æ–°"
        print("  âœ“ æœŸæœ›æ›´æ–°æµ‹è¯•é€šè¿‡")
        
        # æµ‹è¯•æ ·æœ¬æ›´æ–°
        print("\næµ‹è¯•æ ·æœ¬æ›´æ–°...")
        sample_updater = SampleUpdate(env, model, gamma=0.95, alpha=0.1)
        
        if model.is_known(state, action):
            old_q = sample_updater.Q.get_value(state, action)
            sample_updater.sample_update_step(state, action)
            new_q = sample_updater.Q.get_value(state, action)
            # æ ·æœ¬æ›´æ–°å¯èƒ½ä¸æ”¹å˜å€¼ï¼ˆå¦‚æœé‡‡æ ·çš„è½¬ç§»å¯¼è‡´ç›¸åŒçš„TDè¯¯å·®ï¼‰
            print(f"  æ ·æœ¬æ›´æ–°: {old_q:.3f} -> {new_q:.3f}")
        print("  âœ“ æ ·æœ¬æ›´æ–°æµ‹è¯•é€šè¿‡")
        
        # æµ‹è¯•æ¯”è¾ƒå™¨
        print("\næµ‹è¯•æ›´æ–°æ¯”è¾ƒå™¨...")
        comparator = UpdateComparator(env)
        comparison = comparator.compare_updates(
            model,
            expected_iterations=10,
            sample_iterations=100,
            n_runs=2
        )
        
        assert comparison.expected_iterations > 0, "æœŸæœ›æ›´æ–°åº”æœ‰è¿­ä»£"
        assert comparison.sample_iterations > 0, "æ ·æœ¬æ›´æ–°åº”æœ‰è¿­ä»£"
        assert comparison.value_difference >= 0, "ä»·å€¼å·®å¼‚åº”éè´Ÿ"
        print(f"  âœ“ æ›´æ–°æ¯”è¾ƒå™¨æµ‹è¯•é€šè¿‡ (ä»·å€¼å·®å¼‚={comparison.value_difference:.4f})")
        
        print("\nâœ… æœŸæœ›vsæ ·æœ¬æ›´æ–°æµ‹è¯•å…¨éƒ¨é€šè¿‡ï¼")
        return True
        
    except Exception as e:
        print(f"\nâŒ æœŸæœ›vsæ ·æœ¬æ›´æ–°æµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return False


def test_trajectory_sampling():
    """
    æµ‹è¯•è½¨è¿¹é‡‡æ ·
    Test trajectory sampling
    """
    print("\n" + "="*60)
    print("æµ‹è¯•è½¨è¿¹é‡‡æ ·...")
    print("Testing Trajectory Sampling...")
    print("="*60)
    
    try:
        from src.ch08_planning_and_learning.trajectory_sampling import (
            Trajectory, TrajectoryGenerator, TrajectorySampling,
            UniformSampling, OnPolicySampling, RealTimeDynamicProgramming
        )
        from src.ch08_planning_and_learning.models_and_planning import DeterministicModel
        from src.ch02_mdp.gridworld import GridWorld
        from src.ch02_mdp.policies_and_values import UniformRandomPolicy
        
        # åˆ›å»ºç¯å¢ƒ
        env = GridWorld(rows=3, cols=3, start_pos=(0,0), goal_pos=(2,2))
        model = DeterministicModel(env.state_space, env.action_space)
        
        # æ„å»ºæ¨¡å‹
        print("æ„å»ºæ¨¡å‹...")
        for _ in range(30):
            state = env.reset()
            for _ in range(10):
                if state.is_terminal:
                    break
                action = np.random.choice(env.action_space)
                next_state, reward, done, _ = env.step(action)
                model.update(state, action, next_state, reward)
                state = next_state
        print("  âœ“ æ¨¡å‹æ„å»ºå®Œæˆ")
        
        # æµ‹è¯•è½¨è¿¹
        print("\næµ‹è¯•è½¨è¿¹æ•°æ®ç»“æ„...")
        traj = Trajectory()
        traj.add_step(env.state_space[0], env.action_space[0], -1.0)
        traj.add_step(env.state_space[1], env.action_space[1], -1.0)
        
        assert traj.length == 2, "è½¨è¿¹é•¿åº¦åº”ä¸º2"
        assert traj.return_value == -2.0, "è½¨è¿¹å›æŠ¥åº”ä¸º-2.0"
        assert abs(traj.discounted_return(0.9) - (-1.0 - 0.9)) < 0.001, "æŠ˜æ‰£å›æŠ¥è®¡ç®—é”™è¯¯"
        print("  âœ“ è½¨è¿¹æµ‹è¯•é€šè¿‡")
        
        # æµ‹è¯•è½¨è¿¹ç”Ÿæˆå™¨
        print("\næµ‹è¯•è½¨è¿¹ç”Ÿæˆå™¨...")
        generator = TrajectoryGenerator(env, model)
        policy = UniformRandomPolicy(env.action_space)
        
        trajectory = generator.generate_trajectory(policy, max_steps=20)
        assert trajectory.length > 0, "åº”ç”Ÿæˆéç©ºè½¨è¿¹"
        assert len(trajectory.states) > 0, "åº”æœ‰çŠ¶æ€"
        print(f"  âœ“ è½¨è¿¹ç”Ÿæˆå™¨æµ‹è¯•é€šè¿‡ (é•¿åº¦={trajectory.length})")
        
        # æµ‹è¯•è½¨è¿¹é‡‡æ ·ç®—æ³•
        print("\næµ‹è¯•è½¨è¿¹é‡‡æ ·ç®—æ³•...")
        traj_sampler = TrajectorySampling(env, gamma=0.9, alpha=0.1)
        
        # ä»çœŸå®ç»éªŒå­¦ä¹ 
        state = env.state_space[0]
        action = env.action_space[0]
        next_state = env.state_space[1]
        reward = -1.0
        
        traj_sampler.learn_from_real_experience(state, action, next_state, reward)
        assert traj_sampler.update_count == 1, "åº”æœ‰1æ¬¡æ›´æ–°"
        
        # ä½¿ç”¨è½¨è¿¹è§„åˆ’
        traj_sampler.planning_with_trajectories(n_trajectories=5)
        assert traj_sampler.trajectory_count == 5, "åº”ç”Ÿæˆ5æ¡è½¨è¿¹"
        print(f"  âœ“ è½¨è¿¹é‡‡æ ·æµ‹è¯•é€šè¿‡ (æ›´æ–°æ•°={traj_sampler.update_count})")
        
        # æµ‹è¯•RTDP
        print("\næµ‹è¯•å®æ—¶åŠ¨æ€è§„åˆ’...")
        rtdp = RealTimeDynamicProgramming(env, gamma=0.9, alpha=0.1)
        
        rtdp.run_trial(max_steps=10)
        assert len(rtdp.visited_states) > 0, "åº”è®¿é—®ä¸€äº›çŠ¶æ€"
        assert rtdp.update_count > 0, "åº”æœ‰æ›´æ–°"
        print(f"  âœ“ RTDPæµ‹è¯•é€šè¿‡ (è®¿é—®çŠ¶æ€={len(rtdp.visited_states)})")
        
        print("\nâœ… è½¨è¿¹é‡‡æ ·æµ‹è¯•å…¨éƒ¨é€šè¿‡ï¼")
        return True
        
    except Exception as e:
        print(f"\nâŒ è½¨è¿¹é‡‡æ ·æµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return False


def test_mcts():
    """
    æµ‹è¯•è’™ç‰¹å¡æ´›æ ‘æœç´¢
    Test Monte Carlo Tree Search
    """
    print("\n" + "="*60)
    print("æµ‹è¯•è’™ç‰¹å¡æ´›æ ‘æœç´¢...")
    print("Testing Monte Carlo Tree Search...")
    print("="*60)
    
    try:
        from src.ch08_planning_and_learning.mcts import (
            MCTSNode, UCTSelection, MonteCarloTreeSearch
        )
        from src.ch02_mdp.gridworld import GridWorld
        from src.ch02_mdp.mdp_framework import State, Action
        
        # æµ‹è¯•MCTSèŠ‚ç‚¹
        print("æµ‹è¯•MCTSèŠ‚ç‚¹...")
        state = State("test", features={})
        node = MCTSNode(state)
        
        # æ›´æ–°èŠ‚ç‚¹
        node.update(1.0)
        assert node.visit_count == 1, "è®¿é—®æ¬¡æ•°åº”ä¸º1"
        assert node.total_value == 1.0, "æ€»ä»·å€¼åº”ä¸º1.0"
        assert node.average_value == 1.0, "å¹³å‡ä»·å€¼åº”ä¸º1.0"
        
        # æ·»åŠ å­èŠ‚ç‚¹
        action = Action("a1")
        child_state = State("child", features={})
        child = node.add_child(action, child_state)
        
        assert action in node.children, "åº”æœ‰å­èŠ‚ç‚¹"
        assert child.parent == node, "çˆ¶èŠ‚ç‚¹åº”æ­£ç¡®è®¾ç½®"
        print("  âœ“ MCTSèŠ‚ç‚¹æµ‹è¯•é€šè¿‡")
        
        # æµ‹è¯•UCTé€‰æ‹©
        print("\næµ‹è¯•UCTé€‰æ‹©...")
        uct = UCTSelection(c=1.41421356237)
        
        # æ›´æ–°å­èŠ‚ç‚¹ä½¿å…¶æœ‰ä¸åŒçš„ç»Ÿè®¡
        child.update(0.5)
        child.update(0.8)
        
        uct_values = uct.compute_uct_values(node)
        assert action in uct_values, "åº”è®¡ç®—UCTå€¼"
        print("  âœ“ UCTé€‰æ‹©æµ‹è¯•é€šè¿‡")
        
        # æµ‹è¯•MCTSç®—æ³•
        print("\næµ‹è¯•MCTSç®—æ³•...")
        env = GridWorld(rows=3, cols=3, start_pos=(0,0), goal_pos=(2,2))
        mcts = MonteCarloTreeSearch(env, c=1.41421356237, gamma=0.9)
        
        start_state = env.state_space[0]
        best_action = mcts.search(start_state, n_simulations=50, max_depth=10)
        
        assert best_action is not None, "åº”é€‰æ‹©ä¸€ä¸ªåŠ¨ä½œ"
        assert best_action in env.action_space, "åŠ¨ä½œåº”åœ¨åŠ¨ä½œç©ºé—´ä¸­"
        
        stats = mcts.get_tree_statistics()
        assert stats['tree_size'] > 1, "æ ‘åº”è¯¥å¢é•¿"
        assert stats['total_simulations'] == 50, "åº”æ‰§è¡Œ50æ¬¡æ¨¡æ‹Ÿ"
        print(f"  âœ“ MCTSæµ‹è¯•é€šè¿‡ (æ ‘å¤§å°={stats['tree_size']}, æœ€ä½³åŠ¨ä½œ={best_action.id})")
        
        print("\nâœ… MCTSæµ‹è¯•å…¨éƒ¨é€šè¿‡ï¼")
        return True
        
    except Exception as e:
        print(f"\nâŒ MCTSæµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return False


def main():
    """
    è¿è¡Œæ‰€æœ‰æµ‹è¯•
    Run all tests
    """
    print("\n" + "="*80)
    print("ç¬¬8ç« ï¼šè§„åˆ’ä¸å­¦ä¹  - æ¨¡å—æµ‹è¯•")
    print("Chapter 8: Planning and Learning - Module Tests")
    print("="*80)
    
    tests = [
        ("æ¨¡å‹ä¸è§„åˆ’", test_models_and_planning),
        ("Dyna-Qç®—æ³•", test_dyna_q),
        ("ä¼˜å…ˆçº§æ‰«æ", test_prioritized_sweeping),
        ("æœŸæœ›vsæ ·æœ¬æ›´æ–°", test_expected_vs_sample),
        ("è½¨è¿¹é‡‡æ ·", test_trajectory_sampling),
        ("è’™ç‰¹å¡æ´›æ ‘æœç´¢", test_mcts)
    ]
    
    results = []
    start_time = time.time()
    
    for name, test_func in tests:
        print(f"\nè¿è¡Œæµ‹è¯•: {name}")
        success = test_func()
        results.append((name, success))
        
        if not success:
            print(f"\nâš ï¸ æµ‹è¯•å¤±è´¥ï¼Œåœæ­¢åç»­æµ‹è¯•")
            break
    
    total_time = time.time() - start_time
    
    # æ€»ç»“
    print("\n" + "="*80)
    print("æµ‹è¯•æ€»ç»“ Test Summary")
    print("="*80)
    
    all_passed = True
    for name, success in results:
        status = "âœ… é€šè¿‡" if success else "âŒ å¤±è´¥"
        print(f"{name}: {status}")
        if not success:
            all_passed = False
    
    print(f"\næ€»æµ‹è¯•æ—¶é—´: {total_time:.2f}ç§’")
    
    if all_passed:
        print("\nğŸ‰ ç¬¬8ç« æ‰€æœ‰è§„åˆ’ä¸å­¦ä¹ æ¨¡å—æµ‹è¯•é€šè¿‡ï¼")
        print("ğŸ‰ All Chapter 8 Planning and Learning modules passed!")
        print("\nè§„åˆ’ä¸å­¦ä¹ å®ç°éªŒè¯å®Œæˆ:")
        print("âœ“ æ¨¡å‹ä¸è§„åˆ’åŸºç¡€")
        print("âœ“ Dyna-Qå’ŒDyna-Q+")
        print("âœ“ ä¼˜å…ˆçº§æ‰«æ")
        print("âœ“ æœŸæœ›vsæ ·æœ¬æ›´æ–°")
        print("âœ“ è½¨è¿¹é‡‡æ ·å’ŒRTDP")
        print("âœ“ è’™ç‰¹å¡æ´›æ ‘æœç´¢(MCTS)")
        print("\næ¨¡å‹å¢å¼ºäº†å­¦ä¹ æ•ˆç‡ï¼")
        print("Models enhance learning efficiency!")
        print("\nå¯ä»¥ç»§ç»­å­¦ä¹ ç¬¬9ç« æˆ–å¼€å§‹å®é™…é¡¹ç›®")
        print("Ready to proceed to Chapter 9 or start practical projects")
    else:
        print("\nâš ï¸ æœ‰äº›æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä»£ç ")
        print("âš ï¸ Some tests failed, please check the code")
    
    return all_passed


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)